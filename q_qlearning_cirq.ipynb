{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eQQMIX : Entangled Quantum QMIX Algorithm -- eQVDN\n",
    "\n",
    "Multi-agent learning by deriving $Q_{tot}$ from GHZ-entangled agent PQCs and joint measurements.\n",
    "\n",
    "Inspired by the tutorial: https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning#3_deep_q-learning_with_pqc_q-function_approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 16:55:56.341205: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('error', message=r\".*complex128.*\")\n",
    "# warnings.simplefilter('module')\n",
    "\n",
    "# from silence_tensorflow import silence_tensorflow\n",
    "# silence_tensorflow()\n",
    "\n",
    "# from eqmarl import *\n",
    "import cirq\n",
    "import sympy\n",
    "import tensorflow_quantum as tfq\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import functools as ft\n",
    "import collections as cl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Disable GPUs.\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# List any available GPUs.\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: PennyLane\n",
      "Version: 0.33.0\n",
      "Summary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\n",
      "Home-page: https://github.com/PennyLaneAI/pennylane\n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /usr/local/Caskroom/miniforge/base/envs/eqmarl/lib/python3.9/site-packages\n",
      "Requires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, rustworkx, scipy, semantic-version, toml, typing-extensions\n",
      "Required-by: PennyLane-Cirq, PennyLane-Lightning\n",
      "\n",
      "Platform info:           macOS-14.1.1-x86_64-i386-64bit\n",
      "Python version:          3.9.18\n",
      "Numpy version:           1.26.1\n",
      "Scipy version:           1.11.3\n",
      "Installed devices:\n",
      "- default.gaussian (PennyLane-0.33.0)\n",
      "- default.mixed (PennyLane-0.33.0)\n",
      "- default.qubit (PennyLane-0.33.0)\n",
      "- default.qubit.autograd (PennyLane-0.33.0)\n",
      "- default.qubit.jax (PennyLane-0.33.0)\n",
      "- default.qubit.legacy (PennyLane-0.33.0)\n",
      "- default.qubit.tf (PennyLane-0.33.0)\n",
      "- default.qubit.torch (PennyLane-0.33.0)\n",
      "- default.qutrit (PennyLane-0.33.0)\n",
      "- null.qubit (PennyLane-0.33.0)\n",
      "- cirq.mixedsimulator (PennyLane-Cirq-0.33.0)\n",
      "- cirq.pasqal (PennyLane-Cirq-0.33.0)\n",
      "- cirq.qsim (PennyLane-Cirq-0.33.0)\n",
      "- cirq.qsimh (PennyLane-Cirq-0.33.0)\n",
      "- cirq.simulator (PennyLane-Cirq-0.33.0)\n",
      "- lightning.qubit (PennyLane-Lightning-0.33.1)\n"
     ]
    }
   ],
   "source": [
    "# List information about PennyLane plugins.\n",
    "qml.about()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def catchtime() -> float:\n",
    "    start = perf_counter()\n",
    "    yield lambda: perf_counter() - start\n",
    "\n",
    "\n",
    "# with catchtime() as t:\n",
    "#     import time\n",
    "#     time.sleep(1)\n",
    "\n",
    "# print(f\"Execution time: {t():.4f} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow: auto; white-space: pre;\">                                                                                                                                                              ┌──────────────────────────────────┐                                                                                                                                                                                               ┌──────────────────────────────────┐                                                                                                                                                                                               ┌──────────────────────────────────┐                                                                                                                                                                                               ┌──────────────────────────────────┐                                                                                                                                                                                               ┌──────────────────────────────────┐\n",
       "0: ───Rx(0.318309886183791*pi*var0_0_0)───Ry(0.318309886183791*pi*var0_0_1)───Rz(0.318309886183791*pi*var0_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc0_0_0)───Rx(0.318309886183791*pi*var1_0_0)───Ry(0.318309886183791*pi*var1_0_1)───Rz(0.318309886183791*pi*var1_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc1_0_0)───Rx(0.318309886183791*pi*var2_0_0)───Ry(0.318309886183791*pi*var2_0_1)───Rz(0.318309886183791*pi*var2_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc2_0_0)───Rx(0.318309886183791*pi*var3_0_0)───Ry(0.318309886183791*pi*var3_0_1)───Rz(0.318309886183791*pi*var3_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc3_0_0)───Rx(0.318309886183791*pi*var4_0_0)───Ry(0.318309886183791*pi*var4_0_1)───Rz(0.318309886183791*pi*var4_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc4_0_0)───Rx(0.318309886183791*pi*var5_0_0)───Ry(0.318309886183791*pi*var5_0_1)───Rz(0.318309886183791*pi*var5_0_2)───\n",
       "                                                                                                                  │                                            │                                                                                                                                                                                     │                                            │                                                                                                                                                                                     │                                            │                                                                                                                                                                                     │                                            │                                                                                                                                                                                     │                                            │\n",
       "1: ───Rx(0.318309886183791*pi*var0_1_0)───Ry(0.318309886183791*pi*var0_1_1)───Rz(0.318309886183791*pi*var0_1_2)───@───@───Rx(0.318309886183791*pi*enc0_1_0)────┼Rx(0.318309886183791*pi*var1_1_0)────Ry(0.318309886183791*pi*var1_1_1)───Rz(0.318309886183791*pi*var1_1_2)───────────────────────────────────────────────────────────────────────────@───@───Rx(0.318309886183791*pi*enc1_1_0)────┼Rx(0.318309886183791*pi*var2_1_0)────Ry(0.318309886183791*pi*var2_1_1)───Rz(0.318309886183791*pi*var2_1_2)───────────────────────────────────────────────────────────────────────────@───@───Rx(0.318309886183791*pi*enc2_1_0)────┼Rx(0.318309886183791*pi*var3_1_0)────Ry(0.318309886183791*pi*var3_1_1)───Rz(0.318309886183791*pi*var3_1_2)───────────────────────────────────────────────────────────────────────────@───@───Rx(0.318309886183791*pi*enc3_1_0)────┼Rx(0.318309886183791*pi*var4_1_0)────Ry(0.318309886183791*pi*var4_1_1)───Rz(0.318309886183791*pi*var4_1_2)───────────────────────────────────────────────────────────────────────────@───@───Rx(0.318309886183791*pi*enc4_1_0)────┼Rx(0.318309886183791*pi*var5_1_0)────Ry(0.318309886183791*pi*var5_1_1)───Rz(0.318309886183791*pi*var5_1_2)───────────────────────────────────────────────────────────────────────────\n",
       "                                                                                                                      │                                        │                                                                                                                                                                                         │                                        │                                                                                                                                                                                         │                                        │                                                                                                                                                                                         │                                        │                                                                                                                                                                                         │                                        │\n",
       "2: ───Rx(0.318309886183791*pi*var0_2_0)───Ry(0.318309886183791*pi*var0_2_1)───Rz(0.318309886183791*pi*var0_2_2)───────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc0_2_0)────Rx(0.318309886183791*pi*var1_2_0)───Ry(0.318309886183791*pi*var1_2_1)───Rz(0.318309886183791*pi*var1_2_2)───────────────────────────────────────────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc1_2_0)────Rx(0.318309886183791*pi*var2_2_0)───Ry(0.318309886183791*pi*var2_2_1)───Rz(0.318309886183791*pi*var2_2_2)───────────────────────────────────────────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc2_2_0)────Rx(0.318309886183791*pi*var3_2_0)───Ry(0.318309886183791*pi*var3_2_1)───Rz(0.318309886183791*pi*var3_2_2)───────────────────────────────────────────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc3_2_0)────Rx(0.318309886183791*pi*var4_2_0)───Ry(0.318309886183791*pi*var4_2_1)───Rz(0.318309886183791*pi*var4_2_2)───────────────────────────────────────────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc4_2_0)────Rx(0.318309886183791*pi*var5_2_0)───Ry(0.318309886183791*pi*var5_2_1)───Rz(0.318309886183791*pi*var5_2_2)───────────────────────────────────────\n",
       "                                                                                                                          │                                    │                                                                                                                                                                                             │                                    │                                                                                                                                                                                             │                                    │                                                                                                                                                                                             │                                    │                                                                                                                                                                                             │                                    │\n",
       "3: ───Rx(0.318309886183791*pi*var0_3_0)───Ry(0.318309886183791*pi*var0_3_1)───Rz(0.318309886183791*pi*var0_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc0_3_0)───Rx(0.318309886183791*pi*var1_3_0)───Ry(0.318309886183791*pi*var1_3_1)───Rz(0.318309886183791*pi*var1_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc1_3_0)───Rx(0.318309886183791*pi*var2_3_0)───Ry(0.318309886183791*pi*var2_3_1)───Rz(0.318309886183791*pi*var2_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc2_3_0)───Rx(0.318309886183791*pi*var3_3_0)───Ry(0.318309886183791*pi*var3_3_1)───Rz(0.318309886183791*pi*var3_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc3_3_0)───Rx(0.318309886183791*pi*var4_3_0)───Ry(0.318309886183791*pi*var4_3_1)───Rz(0.318309886183791*pi*var4_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc4_3_0)───Rx(0.318309886183791*pi*var5_3_0)───Ry(0.318309886183791*pi*var5_3_1)───Rz(0.318309886183791*pi*var5_3_2)───\n",
       "                                                                                                                                                              └──────────────────────────────────┘                                                                                                                                                                                               └──────────────────────────────────┘                                                                                                                                                                                               └──────────────────────────────────┘                                                                                                                                                                                               └──────────────────────────────────┘                                                                                                                                                                                               └──────────────────────────────────┘</pre>"
      ],
      "text/plain": [
       "                                                                                                                                                              ┌──────────────────────────────────┐                                                                                                                                                                                               ┌──────────────────────────────────┐                                                                                                                                                                                               ┌──────────────────────────────────┐                                                                                                                                                                                               ┌──────────────────────────────────┐                                                                                                                                                                                               ┌──────────────────────────────────┐\n",
       "0: ───Rx(0.318309886183791*pi*var0_0_0)───Ry(0.318309886183791*pi*var0_0_1)───Rz(0.318309886183791*pi*var0_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc0_0_0)───Rx(0.318309886183791*pi*var1_0_0)───Ry(0.318309886183791*pi*var1_0_1)───Rz(0.318309886183791*pi*var1_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc1_0_0)───Rx(0.318309886183791*pi*var2_0_0)───Ry(0.318309886183791*pi*var2_0_1)───Rz(0.318309886183791*pi*var2_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc2_0_0)───Rx(0.318309886183791*pi*var3_0_0)───Ry(0.318309886183791*pi*var3_0_1)───Rz(0.318309886183791*pi*var3_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc3_0_0)───Rx(0.318309886183791*pi*var4_0_0)───Ry(0.318309886183791*pi*var4_0_1)───Rz(0.318309886183791*pi*var4_0_2)───@────────────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc4_0_0)───Rx(0.318309886183791*pi*var5_0_0)───Ry(0.318309886183791*pi*var5_0_1)───Rz(0.318309886183791*pi*var5_0_2)───\n",
       "                                                                                                                  │                                            │                                                                                                                                                                                     │                                            │                                                                                                                                                                                     │                                            │                                                                                                                                                                                     │                                            │                                                                                                                                                                                     │                                            │\n",
       "1: ───Rx(0.318309886183791*pi*var0_1_0)───Ry(0.318309886183791*pi*var0_1_1)───Rz(0.318309886183791*pi*var0_1_2)───@───@───Rx(0.318309886183791*pi*enc0_1_0)────┼Rx(0.318309886183791*pi*var1_1_0)────Ry(0.318309886183791*pi*var1_1_1)───Rz(0.318309886183791*pi*var1_1_2)───────────────────────────────────────────────────────────────────────────@───@───Rx(0.318309886183791*pi*enc1_1_0)────┼Rx(0.318309886183791*pi*var2_1_0)────Ry(0.318309886183791*pi*var2_1_1)───Rz(0.318309886183791*pi*var2_1_2)───────────────────────────────────────────────────────────────────────────@───@───Rx(0.318309886183791*pi*enc2_1_0)────┼Rx(0.318309886183791*pi*var3_1_0)────Ry(0.318309886183791*pi*var3_1_1)───Rz(0.318309886183791*pi*var3_1_2)───────────────────────────────────────────────────────────────────────────@───@───Rx(0.318309886183791*pi*enc3_1_0)────┼Rx(0.318309886183791*pi*var4_1_0)────Ry(0.318309886183791*pi*var4_1_1)───Rz(0.318309886183791*pi*var4_1_2)───────────────────────────────────────────────────────────────────────────@───@───Rx(0.318309886183791*pi*enc4_1_0)────┼Rx(0.318309886183791*pi*var5_1_0)────Ry(0.318309886183791*pi*var5_1_1)───Rz(0.318309886183791*pi*var5_1_2)───────────────────────────────────────────────────────────────────────────\n",
       "                                                                                                                      │                                        │                                                                                                                                                                                         │                                        │                                                                                                                                                                                         │                                        │                                                                                                                                                                                         │                                        │                                                                                                                                                                                         │                                        │\n",
       "2: ───Rx(0.318309886183791*pi*var0_2_0)───Ry(0.318309886183791*pi*var0_2_1)───Rz(0.318309886183791*pi*var0_2_2)───────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc0_2_0)────Rx(0.318309886183791*pi*var1_2_0)───Ry(0.318309886183791*pi*var1_2_1)───Rz(0.318309886183791*pi*var1_2_2)───────────────────────────────────────────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc1_2_0)────Rx(0.318309886183791*pi*var2_2_0)───Ry(0.318309886183791*pi*var2_2_1)───Rz(0.318309886183791*pi*var2_2_2)───────────────────────────────────────────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc2_2_0)────Rx(0.318309886183791*pi*var3_2_0)───Ry(0.318309886183791*pi*var3_2_1)───Rz(0.318309886183791*pi*var3_2_2)───────────────────────────────────────────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc3_2_0)────Rx(0.318309886183791*pi*var4_2_0)───Ry(0.318309886183791*pi*var4_2_1)───Rz(0.318309886183791*pi*var4_2_2)───────────────────────────────────────────@───@────────────────────────────────────┼Rx(0.318309886183791*pi*enc4_2_0)────Rx(0.318309886183791*pi*var5_2_0)───Ry(0.318309886183791*pi*var5_2_1)───Rz(0.318309886183791*pi*var5_2_2)───────────────────────────────────────\n",
       "                                                                                                                          │                                    │                                                                                                                                                                                             │                                    │                                                                                                                                                                                             │                                    │                                                                                                                                                                                             │                                    │                                                                                                                                                                                             │                                    │\n",
       "3: ───Rx(0.318309886183791*pi*var0_3_0)───Ry(0.318309886183791*pi*var0_3_1)───Rz(0.318309886183791*pi*var0_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc0_3_0)───Rx(0.318309886183791*pi*var1_3_0)───Ry(0.318309886183791*pi*var1_3_1)───Rz(0.318309886183791*pi*var1_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc1_3_0)───Rx(0.318309886183791*pi*var2_3_0)───Ry(0.318309886183791*pi*var2_3_1)───Rz(0.318309886183791*pi*var2_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc2_3_0)───Rx(0.318309886183791*pi*var3_3_0)───Ry(0.318309886183791*pi*var3_3_1)───Rz(0.318309886183791*pi*var3_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc3_3_0)───Rx(0.318309886183791*pi*var4_3_0)───Ry(0.318309886183791*pi*var4_3_1)───Rz(0.318309886183791*pi*var4_3_2)───────────@────────────────────────────────────@─────────────────────────────────────Rx(0.318309886183791*pi*enc4_3_0)───Rx(0.318309886183791*pi*var5_3_0)───Ry(0.318309886183791*pi*var5_3_1)───Rz(0.318309886183791*pi*var5_3_2)───\n",
       "                                                                                                                                                              └──────────────────────────────────┘                                                                                                                                                                                               └──────────────────────────────────┘                                                                                                                                                                                               └──────────────────────────────────┘                                                                                                                                                                                               └──────────────────────────────────┘                                                                                                                                                                                               └──────────────────────────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ParameterizedOperation(cirq.Gate):\n",
    "#     \"\"\"Performs a list of parameterized operations on each qubit.\n",
    "    \n",
    "#     Implements `shape()` to determine parameter shapes.\n",
    "#     \"\"\"\n",
    "#     operations: list[type[cirq.Operation]] = [] # Default is no operations, which will throw an error; users must override this or provide an operations list at runtime.\n",
    "    \n",
    "#     def __init__(self, \n",
    "#         weights: np.ndarray,\n",
    "#         # wires: list,\n",
    "#         id: str = None,\n",
    "#         operations: list[type[cirq.Operation]] = None,\n",
    "#         ):\n",
    "        \n",
    "#         # Ensure weights have proper shape.\n",
    "#         req_shape = self.shape(wires, operations)\n",
    "#         n_req_shape = len(req_shape)\n",
    "#         weights_shape = qml.math.shape(weights)\n",
    "#         n_weights_shape = len(weights_shape)\n",
    "#         assert n_weights_shape == n_req_shape or n_weights_shape == n_req_shape + 1, (\n",
    "#             f\"Weights tensor must be {n_req_shape}-dimensional with shape {req_shape}\"\n",
    "#             f\"or {n_req_shape+1}-dimensional if batching; got shape {weights_shape}\"\n",
    "#         )\n",
    "        \n",
    "#         # Validate operations.\n",
    "#         operations = operations or self.operations\n",
    "#         assert len(operations) > 0, 'at least one operation is required'\n",
    "        \n",
    "#         self._hyperparameters = {\"operations\": operations}\n",
    "#         super().__init__(weights, wires=wires, id=id)\n",
    "\n",
    "\n",
    "#     def call(self, weights, wires):\n",
    "#         # Decompose rotations into operations.\n",
    "#         op_list = []\n",
    "#         for i, wire in enumerate(wires):\n",
    "#             for j, op in enumerate(self.operations):\n",
    "#                 op_list.append(op(weights[..., i, j], wires=wire))\n",
    "\n",
    "#         return op_list\n",
    "\n",
    "#     # @staticmethod\n",
    "#     @classmethod\n",
    "#     def compute_decomposition(cls,\n",
    "#         weights: np.ndarray,\n",
    "#         wires: list,\n",
    "#         operations: list[cirq.Operation],\n",
    "#         ):\n",
    "\n",
    "#         # Decompose rotations into operations.\n",
    "#         op_list = []\n",
    "#         for i, wire in enumerate(wires):\n",
    "#             for j, op in enumerate(operations):\n",
    "#                 op_list.append(op(weights[..., i, j], wires=wire))\n",
    "\n",
    "#         return op_list\n",
    "\n",
    "#     @classmethod\n",
    "#     def shape(cls,\n",
    "#         wires: list,\n",
    "#         operations: list[cirq.Operation] = None,\n",
    "#         ):\n",
    "#         \"\"\"Returns tuple of (n_wires, n_operations).\n",
    "        \n",
    "#         If no operations are provided then defaults to class operations.\n",
    "        \n",
    "#         Note that the returned shape does not include a batch dimension.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Use default operations for class instance if none were provided.\n",
    "#         if operations is None: \n",
    "#             operations = cls.operations\n",
    "        \n",
    "#         if isinstance(wires, (int, str)):\n",
    "#             wires = [wires]\n",
    "        \n",
    "#         return (len(wires), len(operations),)\n",
    "\n",
    "\n",
    "\n",
    "class ParameterizedOperationGate(cirq.Gate):\n",
    "    \"\"\"\n",
    "    Applies a sequence of gates corresponding to a parameter matrix. By default gates are assumed to operate on single qubits. Every row of the parameter matrix is a separate set of qubits on which to apply the operation sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    operations: list[cirq.Gate] = []\n",
    "    \n",
    "    def __init__(self, theta: np.ndarray, name: str = None):\n",
    "        super().__init__()\n",
    "        self.theta = theta\n",
    "        self.name = name or self.__class__.__name__\n",
    "        \n",
    "    def _num_qubits_(self):\n",
    "        return self.theta.shape[0]\n",
    "\n",
    "    def _decompose_(self, qubits):\n",
    "        # for i, q in enumerate(qubits):\n",
    "        #     yield cirq.rx()\n",
    "        \n",
    "        # print(f\"{self.theta=}\")\n",
    "            \n",
    "        # Decompose rotations into operations.\n",
    "        for i, q in enumerate(qubits):\n",
    "            for j, op in enumerate(self.operations):\n",
    "                yield op(self.theta[..., i, j])(q)\n",
    "                \n",
    "    def _circuit_diagram_info_(self, args):\n",
    "        # return f\"R({self.theta})\"\n",
    "        # print(args)\n",
    "        return [f'{self.name}({self.theta[i]})' for i in range(self.theta.shape[0])]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_shape(cls,\n",
    "        n_qubits: int,\n",
    "        operations: list[cirq.Gate] = None,\n",
    "        ):\n",
    "        \"\"\"Returns tuple of (n_wires, n_operations).\n",
    "        \n",
    "        If no operations are provided then defaults to class operations.\n",
    "        \n",
    "        Note that the returned shape does not include a batch dimension.\n",
    "        \"\"\"\n",
    "\n",
    "        # Use default operations for class instance if none were provided.\n",
    "        if operations is None: \n",
    "            operations = cls.operations\n",
    "        \n",
    "        # if isinstance(qubits, (int, str)):\n",
    "        #     qubits = [qubits]\n",
    "        \n",
    "        return (n_qubits, len(operations),)\n",
    "\n",
    "\n",
    "class VariationalRotationLayer(ParameterizedOperationGate):\n",
    "    \"\"\"Parameterized variational rotation layer.\n",
    "    \n",
    "    Implements `shape()` to determine parameter shapes.\n",
    "    \"\"\"\n",
    "    operations = [cirq.rx, cirq.ry, cirq.rz] # Default is 3 rotation sequence RX, RY, RZ.\n",
    "\n",
    "\n",
    "class EncodingLayer(ParameterizedOperationGate):\n",
    "    \"\"\"Parameterized variational rotation layer.\n",
    "    \n",
    "    Implements `shape()` to determine parameter shapes.\n",
    "    \"\"\"\n",
    "    operations = [cirq.rx]\n",
    "\n",
    "\n",
    "def circular_entangling_layer(\n",
    "    qubits: list,\n",
    "    gate: cirq.Gate = cirq.CZ,\n",
    "    ) -> list[cirq.Operation]:\n",
    "    \"\"\"Entangles a list of qubits with their next-neighbor in circular fashion (i.e., ensures first and last qubit are also entangled).\"\"\"\n",
    "    ops = []\n",
    "    for q0, q1 in zip(qubits, qubits[1:]):\n",
    "        ops.append(gate(q0, q1))\n",
    "    if len(qubits) != 2:\n",
    "        ops.append(gate(qubits[0], qubits[-1])) # Entangle the first and last qubit.\n",
    "    return ops\n",
    "    # for w0, w1 in zip(wires, wires[1:]):\n",
    "    #     ops.append(gate(wires=[w0, w1]))\n",
    "    # if len(wires) != 2:\n",
    "    #     ops.append(gate(wires=[wires[0], wires[-1]])) # Entangle the first and last qubit.\n",
    "    # return ops\n",
    "\n",
    "\n",
    "\n",
    "def generate_variational_encoding_circuit(\n",
    "    qubits: list,\n",
    "    n_layers: int,\n",
    "    decompose: bool = False,\n",
    "    ) -> tuple[cirq.Circuit, tuple[np.ndarray,...]]:\n",
    "    \n",
    "    n_qubits = len(qubits)\n",
    "    \n",
    "    shape_var = VariationalRotationLayer.get_shape(n_qubits)\n",
    "    shape_enc = EncodingLayer.get_shape(n_qubits)\n",
    "    \n",
    "    ### Define weights for circuit.\n",
    "    #\n",
    "    ## Variational shape\n",
    "    theta_var = sympy.symbols(f'var(0:{n_layers+1})_' + '_'.join(f'(0:{x})' for x in shape_var))\n",
    "    theta_var = np.asarray(theta_var).reshape((n_layers+1, *shape_var))\n",
    "    ## Encoding shape\n",
    "    theta_enc = sympy.symbols(f'enc(0:{n_layers})_' + '_'.join(f'(0:{x})' for x in shape_enc))\n",
    "    theta_enc = np.asarray(theta_enc).reshape((n_layers, *shape_enc))\n",
    "    \n",
    "    # Build the circuit.\n",
    "    # circuit = cirq.Circuit()\n",
    "    ops = []\n",
    "    for l in range(n_layers):\n",
    "        # Variational layer.\n",
    "        ops.append(\n",
    "            VariationalRotationLayer(theta_var[l], name=f'v{l}')(*qubits)\n",
    "        )\n",
    "        \n",
    "        # Entangling layer.\n",
    "        ops.append(\n",
    "            circular_entangling_layer(qubits)\n",
    "        )\n",
    "        \n",
    "        # Encoding layer.\n",
    "        ops.append(\n",
    "            EncodingLayer(theta_enc[l], name=f'e{l}')(*qubits)\n",
    "        )\n",
    "    # Last variational layer at the end.\n",
    "    ops.append(\n",
    "        VariationalRotationLayer(theta_var[l+1], name=f'v{l+1}')(*qubits)\n",
    "    )\n",
    "    \n",
    "    # Decompose circuit into minimal gate representation.\n",
    "    # This is required when custom gates are implemented for use with TensorFlowQuantum\n",
    "    if decompose:\n",
    "        ops = [cirq.decompose(o) for o in cirq.ops.flatten_to_ops(ops)]\n",
    "\n",
    "    circuit = cirq.Circuit(ops)\n",
    "\n",
    "    return circuit, (theta_var, theta_enc)\n",
    "    \n",
    "    \n",
    "qubits = cirq.LineQubit.range(4)\n",
    "# qubits\n",
    "\n",
    "circuit, _ = generate_variational_encoding_circuit(qubits, 5, decompose=True)\n",
    "circuit\n",
    "\n",
    "# shape_var = VariationalRotationLayer.get_shape(qubits)\n",
    "# theta_var = np.random.random(size=shape_var)\n",
    "\n",
    "# shape_enc = EncodingLayer.get_shape(qubits)\n",
    "# theta_enc = np.random.random(size=shape_enc)\n",
    "\n",
    "# # VariationalRotationLayer(theta_var, name='vr')(*qubits)\n",
    "\n",
    "# circuit = cirq.Circuit()\n",
    "# circuit += VariationalRotationLayer(theta_var)(*qubits)\n",
    "# circuit += EncodingLayer(theta_enc)(*qubits)\n",
    "\n",
    "# print(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"9641.305898437502\" height=\"240.0\"><line x1=\"30.0\" x2=\"9611.305898437502\" y1=\"45.0\" y2=\"45.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"30.0\" x2=\"9611.305898437502\" y1=\"95.0\" y2=\"95.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"30.0\" x2=\"9611.305898437502\" y1=\"145.0\" y2=\"145.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"30.0\" x2=\"9611.305898437502\" y1=\"195.0\" y2=\"195.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"8237.717617187505\" x2=\"8534.309687500005\" y1=\"5.0\" y2=\"5.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"8237.717617187505\" x2=\"8534.309687500005\" y1=\"235.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"6487.537265625004\" x2=\"6784.129335937504\" y1=\"5.0\" y2=\"5.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"6487.537265625004\" x2=\"6784.129335937504\" y1=\"235.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"4737.356914062502\" x2=\"5033.9489843750025\" y1=\"5.0\" y2=\"5.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"4737.356914062502\" x2=\"5033.9489843750025\" y1=\"235.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"2987.1765625000007\" x2=\"3283.768632812501\" y1=\"5.0\" y2=\"5.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"2987.1765625000007\" x2=\"3283.768632812501\" y1=\"235.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"1236.9962109375\" x2=\"1533.5882812500001\" y1=\"5.0\" y2=\"5.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"1236.9962109375\" x2=\"1533.5882812500001\" y1=\"235.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"1\" /><line x1=\"880.4041406250001\" x2=\"880.4041406250001\" y1=\"45.0\" y2=\"95.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"940.4041406250001\" x2=\"940.4041406250001\" y1=\"95.0\" y2=\"145.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1103.70017578125\" x2=\"1103.70017578125\" y1=\"145.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1256.9962109375\" x2=\"1256.9962109375\" y1=\"45.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"2630.5844921875005\" x2=\"2630.5844921875005\" y1=\"45.0\" y2=\"95.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"2690.5844921875005\" x2=\"2690.5844921875005\" y1=\"95.0\" y2=\"145.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"2853.8805273437506\" x2=\"2853.8805273437506\" y1=\"145.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"3007.1765625000007\" x2=\"3007.1765625000007\" y1=\"45.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"4380.764843750002\" x2=\"4380.764843750002\" y1=\"45.0\" y2=\"95.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"4440.764843750002\" x2=\"4440.764843750002\" y1=\"95.0\" y2=\"145.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"4604.060878906252\" x2=\"4604.060878906252\" y1=\"145.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"4757.356914062502\" x2=\"4757.356914062502\" y1=\"45.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"6130.9451953125035\" x2=\"6130.9451953125035\" y1=\"45.0\" y2=\"95.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"6190.9451953125035\" x2=\"6190.9451953125035\" y1=\"95.0\" y2=\"145.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"6354.241230468753\" x2=\"6354.241230468753\" y1=\"145.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"6507.537265625004\" x2=\"6507.537265625004\" y1=\"45.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"7881.125546875005\" x2=\"7881.125546875005\" y1=\"45.0\" y2=\"95.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"7941.125546875005\" x2=\"7941.125546875005\" y1=\"95.0\" y2=\"145.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"8104.421582031255\" x2=\"8104.421582031255\" y1=\"145.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"8257.717617187505\" x2=\"8257.717617187505\" y1=\"45.0\" y2=\"195.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"8237.717617187505\" x2=\"8237.717617187505\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"8534.309687500005\" x2=\"8534.309687500005\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"8237.717617187505\" x2=\"8237.717617187505\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"8534.309687500005\" x2=\"8534.309687500005\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"6487.537265625004\" x2=\"6487.537265625004\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"6784.129335937504\" x2=\"6784.129335937504\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"6487.537265625004\" x2=\"6487.537265625004\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"6784.129335937504\" x2=\"6784.129335937504\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"4737.356914062502\" x2=\"4737.356914062502\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"5033.9489843750025\" x2=\"5033.9489843750025\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"4737.356914062502\" x2=\"4737.356914062502\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"5033.9489843750025\" x2=\"5033.9489843750025\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"2987.1765625000007\" x2=\"2987.1765625000007\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"3283.768632812501\" x2=\"3283.768632812501\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"2987.1765625000007\" x2=\"2987.1765625000007\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"3283.768632812501\" x2=\"3283.768632812501\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1236.9962109375\" x2=\"1236.9962109375\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1533.5882812500001\" x2=\"1533.5882812500001\" y1=\"5.0\" y2=\"15.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1236.9962109375\" x2=\"1236.9962109375\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"1533.5882812500001\" x2=\"1533.5882812500001\" y1=\"225.0\" y2=\"235.0\" stroke=\"black\" stroke-width=\"3\" /><rect x=\"10.0\" y=\"25.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"30.0\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">0: </text><rect x=\"10.0\" y=\"75.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"30.0\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">1: </text><rect x=\"10.0\" y=\"125.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"30.0\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">2: </text><rect x=\"10.0\" y=\"175.0\" width=\"40\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"30.0\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">3: </text><rect x=\"70.0\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"191.7340234375\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var0_0_0)</text><rect x=\"70.0\" y=\"75.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"191.7340234375\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var0_1_0)</text><rect x=\"70.0\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"191.7340234375\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var0_2_0)</text><rect x=\"70.0\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"191.7340234375\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var0_3_0)</text><rect x=\"333.468046875\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"455.20207031250004\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var0_0_1)</text><rect x=\"333.468046875\" y=\"75.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"455.20207031250004\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var0_1_1)</text><rect x=\"333.468046875\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"455.20207031250004\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var0_2_1)</text><rect x=\"333.468046875\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"455.20207031250004\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var0_3_1)</text><rect x=\"596.93609375\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"718.6701171875001\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var0_0_2)</text><rect x=\"596.93609375\" y=\"75.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"718.6701171875001\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var0_1_2)</text><rect x=\"596.93609375\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"718.6701171875001\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var0_2_2)</text><rect x=\"596.93609375\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"718.6701171875001\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var0_3_2)</text><circle cx=\"880.4041406250001\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"880.4041406250001\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"940.4041406250001\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"940.4041406250001\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"1103.70017578125\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"1103.70017578125\" cy=\"195.0\" r=\"10.0\" /><rect x=\"980.404140625\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1103.70017578125\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc0_1_0)</text><circle cx=\"1256.9962109375\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"1256.9962109375\" cy=\"195.0\" r=\"10.0\" /><rect x=\"1276.9962109375\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1400.2922460937502\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc0_2_0)</text><rect x=\"1276.9962109375\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1400.2922460937502\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var1_1_0)</text><rect x=\"1553.58828125\" y=\"25.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1676.88431640625\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc0_0_0)</text><rect x=\"1553.58828125\" y=\"175.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1676.88431640625\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc0_3_0)</text><rect x=\"1553.58828125\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1676.88431640625\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var1_1_1)</text><rect x=\"1553.58828125\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1676.88431640625\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var1_2_0)</text><rect x=\"1820.1803515625002\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1941.9143750000003\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var1_0_0)</text><rect x=\"1820.1803515625002\" y=\"75.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1941.9143750000003\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var1_1_2)</text><rect x=\"1820.1803515625002\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1941.9143750000003\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var1_2_1)</text><rect x=\"1820.1803515625002\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1941.9143750000003\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var1_3_0)</text><rect x=\"2083.6483984375\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2205.382421875\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var1_0_1)</text><rect x=\"2083.6483984375\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2205.382421875\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var1_2_2)</text><rect x=\"2083.6483984375\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2205.382421875\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var1_3_1)</text><rect x=\"2347.1164453125\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2468.8504687500003\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var1_0_2)</text><rect x=\"2347.1164453125\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2468.8504687500003\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var1_3_2)</text><circle cx=\"2630.5844921875005\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"2630.5844921875005\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"2690.5844921875005\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"2690.5844921875005\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"2853.8805273437506\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"2853.8805273437506\" cy=\"195.0\" r=\"10.0\" /><rect x=\"2730.5844921875005\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"2853.8805273437506\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc1_1_0)</text><circle cx=\"3007.1765625000007\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"3007.1765625000007\" cy=\"195.0\" r=\"10.0\" /><rect x=\"3027.1765625000007\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3150.472597656251\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc1_2_0)</text><rect x=\"3027.1765625000007\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3150.472597656251\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var2_1_0)</text><rect x=\"3303.768632812501\" y=\"25.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3427.064667968751\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc1_0_0)</text><rect x=\"3303.768632812501\" y=\"175.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3427.064667968751\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc1_3_0)</text><rect x=\"3303.768632812501\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3427.064667968751\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var2_1_1)</text><rect x=\"3303.768632812501\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3427.064667968751\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var2_2_0)</text><rect x=\"3570.3607031250012\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3692.0947265625014\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var2_0_0)</text><rect x=\"3570.3607031250012\" y=\"75.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3692.0947265625014\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var2_1_2)</text><rect x=\"3570.3607031250012\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3692.0947265625014\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var2_2_1)</text><rect x=\"3570.3607031250012\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3692.0947265625014\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var2_3_0)</text><rect x=\"3833.8287500000015\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3955.5627734375016\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var2_0_1)</text><rect x=\"3833.8287500000015\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3955.5627734375016\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var2_2_2)</text><rect x=\"3833.8287500000015\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"3955.5627734375016\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var2_3_1)</text><rect x=\"4097.296796875002\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"4219.030820312501\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var2_0_2)</text><rect x=\"4097.296796875002\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"4219.030820312501\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var2_3_2)</text><circle cx=\"4380.764843750002\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"4380.764843750002\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"4440.764843750002\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"4440.764843750002\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"4604.060878906252\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"4604.060878906252\" cy=\"195.0\" r=\"10.0\" /><rect x=\"4480.764843750002\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"4604.060878906252\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc2_1_0)</text><circle cx=\"4757.356914062502\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"4757.356914062502\" cy=\"195.0\" r=\"10.0\" /><rect x=\"4777.356914062502\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"4900.652949218752\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc2_2_0)</text><rect x=\"4777.356914062502\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"4900.652949218752\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var3_1_0)</text><rect x=\"5053.9489843750025\" y=\"25.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5177.245019531252\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc2_0_0)</text><rect x=\"5053.9489843750025\" y=\"175.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5177.245019531252\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc2_3_0)</text><rect x=\"5053.9489843750025\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5177.245019531252\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var3_1_1)</text><rect x=\"5053.9489843750025\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5177.245019531252\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var3_2_0)</text><rect x=\"5320.541054687503\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5442.275078125002\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var3_0_0)</text><rect x=\"5320.541054687503\" y=\"75.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5442.275078125002\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var3_1_2)</text><rect x=\"5320.541054687503\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5442.275078125002\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var3_2_1)</text><rect x=\"5320.541054687503\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5442.275078125002\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var3_3_0)</text><rect x=\"5584.009101562503\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5705.743125000003\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var3_0_1)</text><rect x=\"5584.009101562503\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5705.743125000003\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var3_2_2)</text><rect x=\"5584.009101562503\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5705.743125000003\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var3_3_1)</text><rect x=\"5847.477148437503\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5969.211171875003\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var3_0_2)</text><rect x=\"5847.477148437503\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"5969.211171875003\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var3_3_2)</text><circle cx=\"6130.9451953125035\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"6130.9451953125035\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"6190.9451953125035\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"6190.9451953125035\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"6354.241230468753\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"6354.241230468753\" cy=\"195.0\" r=\"10.0\" /><rect x=\"6230.9451953125035\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"6354.241230468753\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc3_1_0)</text><circle cx=\"6507.537265625004\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"6507.537265625004\" cy=\"195.0\" r=\"10.0\" /><rect x=\"6527.537265625004\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"6650.8333007812535\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc3_2_0)</text><rect x=\"6527.537265625004\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"6650.8333007812535\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var4_1_0)</text><rect x=\"6804.129335937504\" y=\"25.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"6927.425371093754\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc3_0_0)</text><rect x=\"6804.129335937504\" y=\"175.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"6927.425371093754\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc3_3_0)</text><rect x=\"6804.129335937504\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"6927.425371093754\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var4_1_1)</text><rect x=\"6804.129335937504\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"6927.425371093754\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var4_2_0)</text><rect x=\"7070.721406250004\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"7192.455429687504\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var4_0_0)</text><rect x=\"7070.721406250004\" y=\"75.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"7192.455429687504\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var4_1_2)</text><rect x=\"7070.721406250004\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"7192.455429687504\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var4_2_1)</text><rect x=\"7070.721406250004\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"7192.455429687504\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var4_3_0)</text><rect x=\"7334.189453125005\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"7455.923476562504\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var4_0_1)</text><rect x=\"7334.189453125005\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"7455.923476562504\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var4_2_2)</text><rect x=\"7334.189453125005\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"7455.923476562504\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var4_3_1)</text><rect x=\"7597.657500000005\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"7719.3915234375045\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var4_0_2)</text><rect x=\"7597.657500000005\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"7719.3915234375045\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var4_3_2)</text><circle cx=\"7881.125546875005\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"7881.125546875005\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"7941.125546875005\" cy=\"95.0\" r=\"10.0\" /><circle cx=\"7941.125546875005\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"8104.421582031255\" cy=\"145.0\" r=\"10.0\" /><circle cx=\"8104.421582031255\" cy=\"195.0\" r=\"10.0\" /><rect x=\"7981.125546875005\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8104.421582031255\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc4_1_0)</text><circle cx=\"8257.717617187505\" cy=\"45.0\" r=\"10.0\" /><circle cx=\"8257.717617187505\" cy=\"195.0\" r=\"10.0\" /><rect x=\"8277.717617187505\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8401.013652343756\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc4_2_0)</text><rect x=\"8277.717617187505\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8401.013652343756\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var5_1_0)</text><rect x=\"8554.309687500005\" y=\"25.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8677.605722656255\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc4_0_0)</text><rect x=\"8554.309687500005\" y=\"175.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8677.605722656255\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*enc4_3_0)</text><rect x=\"8554.309687500005\" y=\"75.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8677.605722656255\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var5_1_1)</text><rect x=\"8554.309687500005\" y=\"125.0\" width=\"246.59207031250003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8677.605722656255\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var5_2_0)</text><rect x=\"8820.901757812504\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8942.635781250005\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var5_0_0)</text><rect x=\"8820.901757812504\" y=\"75.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8942.635781250005\" y=\"95.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var5_1_2)</text><rect x=\"8820.901757812504\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8942.635781250005\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var5_2_1)</text><rect x=\"8820.901757812504\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"8942.635781250005\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rx(0.318309886183791*pi*var5_3_0)</text><rect x=\"9084.369804687503\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"9206.103828125004\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var5_0_1)</text><rect x=\"9084.369804687503\" y=\"125.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"9206.103828125004\" y=\"145.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var5_2_2)</text><rect x=\"9084.369804687503\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"9206.103828125004\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Ry(0.318309886183791*pi*var5_3_1)</text><rect x=\"9347.837851562503\" y=\"25.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"9469.571875000003\" y=\"45.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var5_0_2)</text><rect x=\"9347.837851562503\" y=\"175.0\" width=\"243.46804687500003\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"9469.571875000003\" y=\"195.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial\">Rz(0.318309886183791*pi*var5_3_2)</text></svg>"
      ],
      "text/plain": [
       "<cirq.contrib.svg.svg.SVGCircuit at 0x158ae11f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cirq.contrib.svg import SVGCircuit\n",
    "SVGCircuit(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 2), dtype=float32, numpy=\n",
       "array([[ 0.01745525,  0.10810581],\n",
       "       [ 0.3009823 , -0.11701147],\n",
       "       [ 0.14866196, -0.1907006 ],\n",
       "       [ 0.20163324,  0.08790958],\n",
       "       [-0.12392092, -0.16998743],\n",
       "       [ 0.14271474,  0.16371481],\n",
       "       [ 0.10464965, -0.05828256],\n",
       "       [ 0.20110786, -0.17766742],\n",
       "       [ 0.23662487,  0.22041366],\n",
       "       [ 0.2873416 ,  0.1641842 ],\n",
       "       [ 0.16659059,  0.01791796],\n",
       "       [ 0.29541832,  0.26349494],\n",
       "       [-0.05352817, -0.22294943],\n",
       "       [ 0.14184454, -0.16593295],\n",
       "       [-0.07891774, -0.03741387],\n",
       "       [ 0.11522146, -0.23346977]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HybridVariationalEncodingPQC(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, \n",
    "        qubits: list,\n",
    "        n_layers: int,\n",
    "        observables: list,\n",
    "        name: str = None,\n",
    "        squash_activation: str = 'linear',\n",
    "        ):\n",
    "        name = name or self.__class__.__name__\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = len(qubits)\n",
    "        self.squash_activation = squash_activation\n",
    "        \n",
    "        # Build circuit.\n",
    "        circuit, (symbols_var, symbols_enc) = self.generate_circuit(qubits, n_layers, decompose=True)\n",
    "        \n",
    "        # Define trainable variables for TensorFlow layer.\n",
    "        self.w_var = tf.Variable(\n",
    "            initial_value=tf.random_uniform_initializer(minval=0.0, maxval=np.pi)(shape=symbols_var.shape, dtype='float32'),\n",
    "            trainable=True,\n",
    "            name='w_var',\n",
    "        )\n",
    "        self.w_enc = tf.Variable(\n",
    "            initial_value=tf.ones(shape=symbols_enc.shape, dtype='float32'),\n",
    "            trainable=True,\n",
    "            name='w_enc',\n",
    "        )\n",
    "        \n",
    "        # Explicit symbol ordering.\n",
    "        self.symbols = [str(s) for s in np.concatenate((symbols_var.flatten(), symbols_enc.flatten()))]\n",
    "        self.symbols_idx = tf.constant([self.symbols.index(s) for s in sorted(self.symbols)]) # Cross-ref symbol with its index in the explicit ordering.\n",
    "        \n",
    "        # Empty circuit for batching.\n",
    "        self.empty_circuit_tensor = tfq.convert_to_tensor([cirq.Circuit()])\n",
    "        \n",
    "        # The variational+encoding circuit for computation.\n",
    "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.gather(tf.shape(inputs), 0)\n",
    "        \n",
    "        # Since input is batched, we must batch the TFQ circuits.\n",
    "        batched_circuits = tf.repeat(self.empty_circuit_tensor, repeats=batch_size)\n",
    "        \n",
    "        # Batch the variational weight angles.\n",
    "        angles_var = tf.reshape(tf.tile(self.w_var, multiples=[batch_size, 1, 1]), shape=(-1, *self.w_var.shape))\n",
    "        \n",
    "        # Multiply input vectors by the encoding weights.\n",
    "        # Preserves batching.\n",
    "        # Einsum dimension labels:\n",
    "        #   b = batch\n",
    "        #   l = layer\n",
    "        #   q = qubit\n",
    "        #   f = feature\n",
    "        angles_enc = tf.einsum(\"lqf,bq->blqf\", self.w_enc, inputs)\n",
    "        \n",
    "        # Squash the encoding input angles using the provided activation function.\n",
    "        angles_enc = keras.layers.Activation(self.squash_activation)(angles_enc)\n",
    "        \n",
    "        \n",
    "        # Combine all angles into a single batched tensor.\n",
    "        # This is necessary because TensorFlowQuantum requires parameters to be in 1D list format. Since the circuits are also batched, this turns into 2D with shape (batch_size, num_symbols).\n",
    "        #\n",
    "        # Since all angles are different shapes, compress each down to batched 2D and then concatenate along the feature dimension.\n",
    "        joined_angles = tf.concat([\n",
    "            tf.reshape(angles_var, (batch_size, -1)),\n",
    "            tf.reshape(angles_enc, (batch_size, -1)),\n",
    "        ], axis=1)\n",
    "        #\n",
    "        # Now reorder angles based on explicit symbol ordering.\n",
    "        joined_angles = tf.gather(joined_angles, self.symbols_idx, axis=1)\n",
    "        \n",
    "        # Run batched angles.\n",
    "        # Result will be 2D with shape (batch_size, num_observables).\n",
    "        out = self.computation_layer([batched_circuits, joined_angles])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_circuit(*args, **kwargs):\n",
    "        return generate_variational_encoding_circuit(*args, **kwargs)\n",
    "    \n",
    "    \n",
    "observables = [\n",
    "    cirq.Z(qubits[0]) * cirq.Z(qubits[1]),\n",
    "    cirq.Z(qubits[2]) * cirq.Z(qubits[3]),\n",
    "]\n",
    "    \n",
    "layer = HybridVariationalEncodingPQC(qubits, 5, observables, squash_activation='tanh')\n",
    "layer.w_var.shape, layer.w_enc.shape, layer.symbols, layer.symbols_idx\n",
    "\n",
    "\n",
    "x = np.random.random(size=(16,4,))\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescaling(keras.layers.Layer):\n",
    "    \"\"\"Learnable rescaling from range [-1, 1] to range [0, 1].\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=tf.ones(shape=(1,input_dim)),\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            name='obs-weights',\n",
    "            )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs = tf.math.abs(inputs)\n",
    "        # inputs = tf.cast(inputs, dtype='float32')\n",
    "        return tf.math.multiply(\n",
    "            (1+inputs)/2., # Rescale from [-1, 1] to range [0, 1].\n",
    "            tf.repeat(self.w, repeats=tf.shape(inputs)[0], axis=0),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_Qlearning(qubits, n_layers, observables, is_target):\n",
    "    \n",
    "    n_wires = len(qubits)\n",
    "    \n",
    "    # qubits = cirq.LineQubit.range(n_wires)\n",
    "    \n",
    "    qlayer = HybridVariationalEncodingPQC(qubits, n_layers, observables, squash_activation='tanh')\n",
    "\n",
    "    model = keras.Sequential([\n",
    "            keras.Input(shape=(n_wires,), dtype=tf.dtypes.float32, name='input'), # Shape of model input, which should match the observation vector shape.\n",
    "            qlayer,\n",
    "            # CustomQuantumLayer(n_wires, n_layers, observables),\n",
    "            # keras.layers.Lambda(lambda x: tf.math.abs(x)), # Convert complex to float via abs.\n",
    "            # keras.layers.Activation('tanh'), # Ensure outputs of PQC are in range [-1, 1].\n",
    "            keras.Sequential([Rescaling(len(observables))], name=is_target*'Target'+'Q-values'),\n",
    "        ],\n",
    "        )\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2 # because `env.action_space.n == 2`\n",
    "n_wires = 4 # because `env.observation_space.shape == (4,)`\n",
    "n_layers = 5\n",
    "\n",
    "qubits = cirq.LineQubit.range(n_wires)\n",
    "\n",
    "observables = [\n",
    "    cirq.Z(qubits[0]) * cirq.Z(qubits[1]),\n",
    "    cirq.Z(qubits[2]) * cirq.Z(qubits[3]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting weights\n"
     ]
    }
   ],
   "source": [
    "model = generate_model_Qlearning(\n",
    "    qubits=qubits,\n",
    "    n_layers=n_layers,\n",
    "    observables=observables,\n",
    "    is_target=False,\n",
    "    )\n",
    "\n",
    "model_target = generate_model_Qlearning(\n",
    "    qubits=qubits,\n",
    "    n_layers=n_layers,\n",
    "    observables=observables,\n",
    "    is_target=True,\n",
    "    )\n",
    "\n",
    "print('setting weights')\n",
    "model_target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70, to_file=\"model_classic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " HybridVariationalEncodingPQ  (None, 2)                92        \n",
      " C (HybridVariationalEncodin                                     \n",
      " gPQC)                                                           \n",
      "                                                                 \n",
      " Q-values (Sequential)       (None, 2)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94\n",
      "Trainable params: 94\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 2), dtype=float32, numpy=\n",
       "array([[0.38282248, 0.65529966],\n",
       "       [0.369394  , 0.63457334],\n",
       "       [0.3453862 , 0.5056309 ],\n",
       "       [0.4466657 , 0.613704  ],\n",
       "       [0.3789647 , 0.629547  ],\n",
       "       [0.43408078, 0.5745201 ],\n",
       "       [0.38882577, 0.5137159 ],\n",
       "       [0.4089002 , 0.6004227 ],\n",
       "       [0.3801592 , 0.5282665 ],\n",
       "       [0.37647727, 0.5363251 ],\n",
       "       [0.36829612, 0.63520384],\n",
       "       [0.45573935, 0.53261024],\n",
       "       [0.3809028 , 0.5918867 ],\n",
       "       [0.40693483, 0.59963745],\n",
       "       [0.37490648, 0.55447453],\n",
       "       [0.38772053, 0.64839405]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random(size=(16,4,))\n",
    "model([x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "n_episodes = 2000\n",
    "\n",
    "# Define replay memory\n",
    "max_memory_length = 10000 # Maximum replay length\n",
    "replay_memory = cl.deque(maxlen=max_memory_length)\n",
    "\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
    "decay_epsilon = 0.99 # Decay rate of epsilon greedy parameter\n",
    "batch_size = 16\n",
    "steps_per_update = 10 # Train the model every x steps\n",
    "steps_per_target_update = 30 # Update the target model every x steps\n",
    "\n",
    "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
    "\n",
    "# Assign the model parameters to each optimizer\n",
    "w_in, w_var, w_out = 1, 0, 2\n",
    "\n",
    "# optimizer_w_tups = list(zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q-learning update function.\n",
    "\n",
    "@tf.function\n",
    "def Q_learning_update(states, actions, rewards, next_states, done, model, model_target, gamma, n_actions):\n",
    "    \n",
    "    states = tf.convert_to_tensor(states)\n",
    "    actions = tf.convert_to_tensor(actions)\n",
    "    rewards = tf.convert_to_tensor(rewards)\n",
    "    next_states = tf.convert_to_tensor(next_states)\n",
    "    done = tf.convert_to_tensor(done)\n",
    "\n",
    "    # Compute target Q-values and masks on sampled actions.\n",
    "    # with catchtime() as t:\n",
    "    future_rewards = model_target([next_states])\n",
    "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1) * (1. - done))\n",
    "    # print(f\"{target_q_values=}\")\n",
    "    # print(f\"[target_q_values] Execution time: {t():.4f} secs\")\n",
    "    \n",
    "    masks = tf.one_hot(actions, n_actions)\n",
    "    # print(f\"{actions=}\")\n",
    "    # print(f\"{masks=}\")\n",
    "    \n",
    "    \n",
    "    # with catchtime() as t:\n",
    "    # Train the model using target Q-values.\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        # with catchtime() as t1:\n",
    "        q_values = model([states])\n",
    "        # print(f\"[tape:eval] Execution time: {t1():.4f} secs\")\n",
    "        # print(f\"{q_values=}\")\n",
    "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "        # print(f\"{q_values_masked=}\")\n",
    "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
    "    # print(f\"[tape] Execution time: {t():.4f} secs\")\n",
    "        \n",
    "    # with catchtime() as t:\n",
    "    # Backprop.\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # for optimizer, w in optimizer_w_tups:\n",
    "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
    "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])\n",
    "    # print(f\"[backprop] Execution time: {t():.4f} secs\")\n",
    "        \n",
    "    # # print(f\"{loss=}\")\n",
    "    # # print(f\"{grads=}\")\n",
    "    # # print(f\"{masks=}\")\n",
    "    # print(f\"{target_q_values=}\")\n",
    "    # print(f\"{q_values=}\")\n",
    "    # print(f\"{loss=}\")\n",
    "    # print(f\"{grads=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_env(state, model, epsilon, n_actions, env):\n",
    "    # state_shape = state.shape\n",
    "    # state = np.asarray(state).reshape((1, *state_shape)) # Reshape to (1, features) so that batch_size=1.\n",
    "    # state = tf.convert_to_tensor([state]) # Reshape to (1, features) so that batch_size=1.\n",
    "    state = np.asarray(state).reshape((1, 4)) # Reshape to (1, features) so that batch_size=1.\n",
    "    \n",
    "    # Select action.\n",
    "    coin = np.random.random()\n",
    "    if coin > epsilon: # Epsilon-greedy\n",
    "        q_vals = model([state])\n",
    "        action = int(tf.argmax(q_vals[0]).numpy())\n",
    "    else: # Random action.\n",
    "        action = np.random.choice(n_actions)\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = np.asarray(next_state).reshape(state.shape)\n",
    "    \n",
    "    return dict(\n",
    "        state=state,\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        next_state=next_state,\n",
    "        done=done,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/2000, average last 10 rewards 23.9\n",
      "Episode 20/2000, average last 10 rewards 17.9\n",
      "Episode 30/2000, average last 10 rewards 19.9\n",
      "Episode 40/2000, average last 10 rewards 19.0\n",
      "Episode 50/2000, average last 10 rewards 19.7\n",
      "Episode 60/2000, average last 10 rewards 16.5\n",
      "Episode 70/2000, average last 10 rewards 21.0\n",
      "Episode 80/2000, average last 10 rewards 14.8\n",
      "Episode 90/2000, average last 10 rewards 17.0\n",
      "Episode 100/2000, average last 10 rewards 16.4\n",
      "Episode 110/2000, average last 10 rewards 18.6\n",
      "Episode 120/2000, average last 10 rewards 22.7\n",
      "Episode 130/2000, average last 10 rewards 24.1\n",
      "Episode 140/2000, average last 10 rewards 19.0\n",
      "Episode 150/2000, average last 10 rewards 21.2\n",
      "Episode 160/2000, average last 10 rewards 20.2\n",
      "Episode 170/2000, average last 10 rewards 20.3\n",
      "Episode 180/2000, average last 10 rewards 17.9\n",
      "Episode 190/2000, average last 10 rewards 20.9\n",
      "Episode 200/2000, average last 10 rewards 21.1\n",
      "Episode 210/2000, average last 10 rewards 16.3\n",
      "Episode 220/2000, average last 10 rewards 20.5\n",
      "Episode 230/2000, average last 10 rewards 21.6\n",
      "Episode 240/2000, average last 10 rewards 21.2\n",
      "Episode 250/2000, average last 10 rewards 20.0\n",
      "Episode 260/2000, average last 10 rewards 21.3\n",
      "Episode 270/2000, average last 10 rewards 22.7\n",
      "Episode 280/2000, average last 10 rewards 21.9\n",
      "Episode 290/2000, average last 10 rewards 21.9\n",
      "Episode 300/2000, average last 10 rewards 21.6\n",
      "Episode 310/2000, average last 10 rewards 19.5\n",
      "Episode 320/2000, average last 10 rewards 21.6\n",
      "Episode 330/2000, average last 10 rewards 21.4\n",
      "Episode 340/2000, average last 10 rewards 20.6\n",
      "Episode 350/2000, average last 10 rewards 17.6\n",
      "Episode 360/2000, average last 10 rewards 20.2\n",
      "Episode 370/2000, average last 10 rewards 20.1\n",
      "Episode 380/2000, average last 10 rewards 20.3\n",
      "Episode 390/2000, average last 10 rewards 19.2\n",
      "Episode 400/2000, average last 10 rewards 19.3\n",
      "Episode 410/2000, average last 10 rewards 17.3\n",
      "Episode 420/2000, average last 10 rewards 20.1\n",
      "Episode 430/2000, average last 10 rewards 19.9\n",
      "Episode 440/2000, average last 10 rewards 25.1\n",
      "Episode 450/2000, average last 10 rewards 20.5\n",
      "Episode 460/2000, average last 10 rewards 20.2\n",
      "Episode 470/2000, average last 10 rewards 16.5\n",
      "Episode 480/2000, average last 10 rewards 22.4\n",
      "Episode 490/2000, average last 10 rewards 24.9\n",
      "Episode 500/2000, average last 10 rewards 18.4\n",
      "Episode 510/2000, average last 10 rewards 19.2\n",
      "Episode 520/2000, average last 10 rewards 23.5\n",
      "Episode 530/2000, average last 10 rewards 19.6\n",
      "Episode 540/2000, average last 10 rewards 21.6\n",
      "Episode 550/2000, average last 10 rewards 18.8\n",
      "Episode 560/2000, average last 10 rewards 23.2\n",
      "Episode 570/2000, average last 10 rewards 22.5\n",
      "Episode 580/2000, average last 10 rewards 19.4\n",
      "Episode 590/2000, average last 10 rewards 19.4\n",
      "Episode 600/2000, average last 10 rewards 19.8\n",
      "Episode 610/2000, average last 10 rewards 18.3\n",
      "Episode 620/2000, average last 10 rewards 19.1\n",
      "Episode 630/2000, average last 10 rewards 19.1\n",
      "Episode 640/2000, average last 10 rewards 19.4\n",
      "Episode 650/2000, average last 10 rewards 20.2\n",
      "Episode 660/2000, average last 10 rewards 21.3\n",
      "Episode 670/2000, average last 10 rewards 22.0\n",
      "Episode 680/2000, average last 10 rewards 25.0\n",
      "Episode 690/2000, average last 10 rewards 34.7\n",
      "Episode 700/2000, average last 10 rewards 62.4\n",
      "Episode 710/2000, average last 10 rewards 75.7\n",
      "Episode 720/2000, average last 10 rewards 105.8\n",
      "Episode 730/2000, average last 10 rewards 104.3\n",
      "Episode 740/2000, average last 10 rewards 70.1\n",
      "Episode 750/2000, average last 10 rewards 59.7\n",
      "Episode 760/2000, average last 10 rewards 58.0\n",
      "Episode 770/2000, average last 10 rewards 63.1\n",
      "Episode 780/2000, average last 10 rewards 61.5\n",
      "Episode 790/2000, average last 10 rewards 56.5\n",
      "Episode 800/2000, average last 10 rewards 55.2\n",
      "Episode 810/2000, average last 10 rewards 49.1\n",
      "Episode 820/2000, average last 10 rewards 74.4\n",
      "Episode 830/2000, average last 10 rewards 92.4\n",
      "Episode 840/2000, average last 10 rewards 157.0\n",
      "Episode 850/2000, average last 10 rewards 131.1\n",
      "Episode 860/2000, average last 10 rewards 154.7\n",
      "Episode 870/2000, average last 10 rewards 117.7\n",
      "Episode 880/2000, average last 10 rewards 154.6\n",
      "Episode 890/2000, average last 10 rewards 95.9\n",
      "Episode 900/2000, average last 10 rewards 124.0\n",
      "Episode 910/2000, average last 10 rewards 138.8\n",
      "Episode 920/2000, average last 10 rewards 159.6\n",
      "Episode 930/2000, average last 10 rewards 128.2\n",
      "Episode 940/2000, average last 10 rewards 189.2\n",
      "Episode 950/2000, average last 10 rewards 263.3\n",
      "Episode 960/2000, average last 10 rewards 311.2\n",
      "Episode 970/2000, average last 10 rewards 1031.6\n"
     ]
    }
   ],
   "source": [
    "### Main training loop\n",
    "\n",
    "# with tf.device(\"/gpu:0\"):\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "episode_reward_history = []\n",
    "step_count = 0\n",
    "for episode in range(n_episodes):\n",
    "    episode_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        # Interact with environment.\n",
    "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
    "        \n",
    "        # Preserve interaction in the replay memory.\n",
    "        replay_memory.append(interaction)\n",
    "        \n",
    "        state = interaction['next_state']\n",
    "        episode_reward += interaction['reward']\n",
    "        step_count += 1\n",
    "        \n",
    "        if step_count % steps_per_update == 0:\n",
    "            training_batch = np.random.choice(replay_memory, size=batch_size) # Randomly select interactions from replay memory and train on them.\n",
    "            \n",
    "            Q_learning_update(\n",
    "                states=np.asarray([x['state'] for x in training_batch]).squeeze(),\n",
    "                actions=np.asarray([x['action'] for x in training_batch]).squeeze(),\n",
    "                rewards=np.asarray([x['reward'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "                next_states=np.asarray([x['next_state'] for x in training_batch]).squeeze(),\n",
    "                done=np.asarray([x['done'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "                model=model,\n",
    "                model_target=model_target,\n",
    "                gamma=gamma,\n",
    "                n_actions=n_actions,\n",
    "                # optimizer_w_tups=optimizer_w_tups,\n",
    "            )\n",
    "            # \n",
    "            # \n",
    "            # \n",
    "            # Q_learning_update_new(np.asarray([x['state'] for x in training_batch]).squeeze(),\n",
    "            #                   np.asarray([x['action'] for x in training_batch]).squeeze(),\n",
    "            #                   np.asarray([x['reward'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "            #                   np.asarray([x['next_state'] for x in training_batch]).squeeze(),\n",
    "            #                   np.asarray([x['done'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "            #                   model, gamma, n_actions)\n",
    "        \n",
    "        if step_count % steps_per_target_update == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "        \n",
    "        if interaction['done']:\n",
    "            break\n",
    "        \n",
    "    # Decay epsilon.\n",
    "    epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    \n",
    "    if (episode+1) % 10 == 0:\n",
    "        avg_rewards = np.mean(episode_reward_history[-10:])\n",
    "        print(\"Episode {}/{}, average last 10 rewards {}\".format(episode+1, n_episodes, avg_rewards))\n",
    "        if avg_rewards >= 500.0:\n",
    "            break\n",
    "    \n",
    "    # if episode > 50: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('q_qlearning_cirq', save_format='tf')\n",
    "model.save_weights('q_qlearning_cirq__weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15934ed30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('q_qlearning_cirq__weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x159e11070>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_var.shape=(6, 4, 3)\n",
      "weights_enc.shape=(5, 4)\n"
     ]
    }
   ],
   "source": [
    "def one_qubit_rotation(qubit, symbols):\n",
    "    \"\"\"\n",
    "    Returns Cirq gates that apply a rotation of the bloch sphere about the X,\n",
    "    Y and Z axis, specified by the values in `symbols`.\n",
    "    \"\"\"\n",
    "    return [cirq.rx(symbols[0])(qubit),\n",
    "            cirq.ry(symbols[1])(qubit),\n",
    "            cirq.rz(symbols[2])(qubit)]\n",
    "\n",
    "def entangling_layer(qubits):\n",
    "    \"\"\"\n",
    "    Returns a layer of CZ entangling gates on `qubits` (arranged in a circular topology).\n",
    "    \"\"\"\n",
    "    cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
    "    cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
    "    return cz_ops\n",
    "\n",
    "def generate_circuit_cirq(wires, n_layers):\n",
    "    \n",
    "    if isinstance(wires, int):\n",
    "            wires = list(range(wires))\n",
    "    elif isinstance(wires, (list, tuple)):\n",
    "        wires = wires\n",
    "    else:\n",
    "        raise ValueError(f\"Wires must either be an integer, list, or tuple; got {wires}\")\n",
    "    \n",
    "    n_wires = len(wires)\n",
    "    qubits = cirq.LineQubit.range(n_wires)\n",
    "    \n",
    "    \n",
    "    ### Weights\n",
    "    weights_var = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_wires})')\n",
    "    weights_var = np.asarray(weights_var).reshape((n_layers + 1, n_wires, 3))\n",
    "    #\n",
    "    weights_enc = sympy.symbols(f'x(0:{n_layers})'+f'_(0:{n_wires})')\n",
    "    weights_enc = np.asarray(weights_enc).reshape((n_layers, n_wires))\n",
    "    \n",
    "    print(f\"{weights_var.shape=}\")\n",
    "    print(f\"{weights_enc.shape=}\")\n",
    "    \n",
    "    circuit = cirq.Circuit()\n",
    "    for l in range(n_layers):\n",
    "        # Variational layer\n",
    "        circuit += cirq.Circuit(one_qubit_rotation(q, weights_var[l, i]) for i, q in enumerate(qubits))\n",
    "        circuit += entangling_layer(qubits)\n",
    "        # Encoding layer\n",
    "        circuit += cirq.Circuit(cirq.rx(weights_enc[l, i])(q) for i, q in enumerate(qubits))\n",
    "    \n",
    "    # Last varitional layer\n",
    "    circuit += cirq.Circuit(one_qubit_rotation(q, weights_var[n_layers, i]) for i, q in enumerate(qubits))\n",
    "    \n",
    "    return circuit, (weights_var, weights_enc)\n",
    "\n",
    "\n",
    "_, (cirq_weights_var, cirq_weights_enc) = generate_circuit_cirq(4, 5)\n",
    "\n",
    "# class CirqQuantumLayer(keras.layers.Layer):\n",
    "#     def __init__(self,\n",
    "#         wires: int | list[int],\n",
    "#         n_layers: int,\n",
    "#         observables: list | Callable[[list], list] | None = None,\n",
    "#         initial_state: list | Callable[[list], None] = None,\n",
    "#         ):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         if isinstance(wires, int):\n",
    "#             wires = list(range(wires))\n",
    "#         elif isinstance(wires, (list, tuple)):\n",
    "#             wires = wires\n",
    "#         else:\n",
    "#             raise ValueError(f\"Wires must either be an integer, list, or tuple; got {wires}\")\n",
    "#         self.n_wires = len(wires)\n",
    "#         self.wires = wires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = qml.device('default.qubit')\n",
    "# @qml.qnode(device=device)\n",
    "# def quantum_circuit(wires, n_layers, observables, weights_var, weights_enc):\n",
    "#     VariationalEncodingPQC(\n",
    "#         weights_var=weights_var,\n",
    "#         weights_enc=weights_enc,\n",
    "#         n_layers=n_layers,\n",
    "#         wires=wires,\n",
    "#         )\n",
    "#     if observables is not None:\n",
    "#         res = [qml.expval(o) for o in observables]\n",
    "#         return res\n",
    "\n",
    "# class CustomQuantumLayer(keras.layers.Layer):\n",
    "#     def __init__(self,\n",
    "#         wires: int | list[int],\n",
    "#         n_layers: int,\n",
    "#         observables: list | Callable[[list], list] | None = None,\n",
    "#         initial_state: list | Callable[[list], None] = None,\n",
    "#         ):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         if isinstance(wires, int):\n",
    "#             wires = list(range(wires))\n",
    "#         elif isinstance(wires, (list, tuple)):\n",
    "#             wires = wires\n",
    "#         else:\n",
    "#             raise ValueError(f\"Wires must either be an integer, list, or tuple; got {wires}\")\n",
    "#         self.n_wires = len(wires)\n",
    "#         self.wires = wires\n",
    "        \n",
    "#         self.set_observables(observables=observables)\n",
    "        \n",
    "#         self.n_layers = n_layers\n",
    "#         self.initial_state = initial_state\n",
    "        \n",
    "        \n",
    "#         ### Create trainable variables for this layer.\n",
    "#         shape_weights_var, shape_weights_enc = VariationalEncodingPQC.shape(\n",
    "#             n_layers=n_layers,\n",
    "#             wires=wires,\n",
    "#             )\n",
    "#         print(f'got shape: {(shape_weights_var, shape_weights_enc)=}')\n",
    "        \n",
    "#         self.weights_var = tf.Variable(\n",
    "#             initial_value=tf.random_uniform_initializer(minval=0., maxval=np.pi,)(shape=shape_weights_var, dtype='float32'),\n",
    "#             trainable=True,\n",
    "#             name='weights_var',\n",
    "#             )\n",
    "#         self.weights_enc = tf.Variable(\n",
    "#             initial_value=tf.ones(shape=shape_weights_enc, dtype='float32'),\n",
    "#             trainable=True,\n",
    "#             name='weights_enc',\n",
    "#             )\n",
    "#         print('init vars')\n",
    "        \n",
    "#         ### Build quantum circuit.\n",
    "#         # self.q_device = qml.device('default.qubit', wires=wires)\n",
    "#         # self.q_qnode = qml.QNode(self.quantum_circuit, device=self.q_device)\n",
    "#         self.q_qnode = quantum_circuit\n",
    "#         print('build circuit')\n",
    "        \n",
    "        \n",
    "#     def call(self, inputs):\n",
    "        \n",
    "#         inputs = tf.reshape(inputs, (-1, self.n_wires)) # Ensure shape is 2D with (batch, d_qubits)\n",
    "        \n",
    "#         print('pre-einsum')\n",
    "#         weights_enc = tf.einsum(\"lqf,bq->blqf\", self.weights_enc, inputs) # For each agent, encode each `input` state feature `q` on the `q-th` qubit and repeat encoding on same qubit for every layer `l`. Number of input features must match number of qubits.\n",
    "#         print('post-einsum')\n",
    "        \n",
    "#         weights_var = self.weights_var\n",
    "        \n",
    "#         print('pre-eval')\n",
    "#         # res = self.q_qnode(weights_var, weights_enc)\n",
    "#         res = self.q_qnode(self.wires, self.n_layers, self.observables, weights_var, weights_enc)\n",
    "#         print('post-eval')\n",
    "#         res = tf.convert_to_tensor(res) # Ensure tensor.\n",
    "#         res = tf.transpose(res) # Place batch dimension first.\n",
    "#         # new_shape = tf.concat([batch_dim, tf.shape(res)[1:]], axis=0)\n",
    "#         # res = tf.reshape(res, new_shape)\n",
    "#         return res\n",
    "\n",
    "#     def quantum_circuit(self, weights_var, weights_enc):\n",
    "#         VariationalEncodingPQC(\n",
    "#             weights_var=weights_var,\n",
    "#             weights_enc=weights_enc,\n",
    "#             n_layers=self.n_layers,\n",
    "#             wires=self.wires,\n",
    "#             )\n",
    "#         return self.measure()\n",
    "    \n",
    "#     def measure(self) -> list | None:\n",
    "#         \"\"\"Returns list of expectations across all observables.\"\"\"\n",
    "#         if self.observables is not None:\n",
    "#             res = [qml.expval(o) for o in self.observables]\n",
    "#             return res\n",
    "        \n",
    "        \n",
    "#     def set_observables(self, observables: list | Callable[[list], list] | None = None):\n",
    "#         \"\"\"Allows setting observables after circuit has been created.\"\"\"\n",
    "#         if observables is None:\n",
    "#             self.observables = None\n",
    "#         elif hasattr(observables, '__iter__') and not isinstance(observables, str):\n",
    "#             self.observables = np.asarray(observables).tolist() # Ensure type is a Python list.\n",
    "#         elif isinstance(observables, Callable):\n",
    "#             self.observables = np.asarray(observables(self.wires)).tolist() # Ensure type is a Python list.\n",
    "#         else:\n",
    "#             raise ValueError(f\"observables must either be a list or function; got `{observables}`\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescaling(keras.layers.Layer):\n",
    "    \"\"\"Learnable rescaling from range [-1, 1] to range [0, 1].\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=tf.ones(shape=(1,input_dim)),\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            name='obs-weights',\n",
    "            )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs = tf.math.abs(inputs)\n",
    "        # inputs = tf.cast(inputs, dtype='float32')\n",
    "        return tf.math.multiply(\n",
    "            (1+inputs)/2., # Rescale from [-1, 1] to range [0, 1].\n",
    "            tf.repeat(self.w, repeats=tf.shape(inputs)[0], axis=0),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circuit(qubits, n_layers):\n",
    "    \"\"\"Prepares a data re-uploading circuit on `qubits` with `n_layers` layers.\"\"\"\n",
    "    # Number of qubits\n",
    "    n_qubits = len(qubits)\n",
    "    \n",
    "    # Sympy symbols for variational angles\n",
    "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_qubits})')\n",
    "    params = np.asarray(params).reshape((n_layers + 1, n_qubits, 3))\n",
    "    \n",
    "    # Sympy symbols for encoding angles\n",
    "    inputs = sympy.symbols(f'x(0:{n_layers})'+f'_(0:{n_qubits})')\n",
    "    inputs = np.asarray(inputs).reshape((n_layers, n_qubits))\n",
    "    \n",
    "    # Define circuit\n",
    "    circuit = cirq.Circuit()\n",
    "    for l in range(n_layers):\n",
    "        # Variational layer\n",
    "        circuit += cirq.Circuit(one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits))\n",
    "        circuit += entangling_layer(qubits)\n",
    "        # Encoding layer\n",
    "        circuit += cirq.Circuit(cirq.rx(inputs[l, i])(q) for i, q in enumerate(qubits))\n",
    "\n",
    "    # Last varitional layer\n",
    "    circuit += cirq.Circuit(one_qubit_rotation(q, params[n_layers, i]) for i,q in enumerate(qubits))\n",
    "    \n",
    "    return circuit, list(params.flat), list(inputs.flat)\n",
    "\n",
    "\n",
    "class ReUploadingPQC(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Performs the transformation (s_1, ..., s_d) -> (theta_1, ..., theta_N, lmbd[1][1]s_1, ..., lmbd[1][M]s_1,\n",
    "        ......., lmbd[d][1]s_d, ..., lmbd[d][M]s_d) for d=input_dim, N=theta_dim and M=n_layers.\n",
    "    An activation function from tf.keras.activations, specified by `activation` ('linear' by default) is\n",
    "        then applied to all lmbd[i][j]s_i.\n",
    "    All angles are finally permuted to follow the alphabetical order of their symbol names, as processed\n",
    "        by the ControlledPQC.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\"):\n",
    "        super(ReUploadingPQC, self).__init__(name=name)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = len(qubits)\n",
    "\n",
    "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
    "\n",
    "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
    "        self.theta = tf.Variable(\n",
    "            initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"),\n",
    "            trainable=True, name=\"thetas\"\n",
    "        )\n",
    "        \n",
    "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
    "        self.lmbd = tf.Variable(\n",
    "            initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\"\n",
    "        )\n",
    "        \n",
    "        # Define explicit symbol order.\n",
    "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
    "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
    "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs[0] = encoding data for the state.\n",
    "        print(f\"{inputs=}\")\n",
    "        # print(f\"{inputs.shape=}\")\n",
    "        # print(f\"{inputs[0].shape=}\")\n",
    "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
    "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
    "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
    "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
    "        # print(f\"{tiled_up_inputs.shape=}\")\n",
    "        print(f\"{self.lmbd=}\")\n",
    "        print(f\"{tiled_up_inputs=}\")\n",
    "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
    "        print(f\"{scaled_inputs.shape=}\")\n",
    "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
    "\n",
    "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
    "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
    "        \n",
    "        return self.computation_layer([tiled_up_circuits, joined_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_Qlearning(n_wires, n_layers, observables, is_target):\n",
    "    \n",
    "    # circuit = AgentCircuit(\n",
    "    #     wires=n_wires,\n",
    "    #     n_layers=n_layers,\n",
    "    #     observables=observables,\n",
    "    # )\n",
    "    # # circuit = MARLCircuit(\n",
    "    # #     n_agents=1,\n",
    "    # #     d_qubits=4,\n",
    "    # #     n_layers=n_layers,\n",
    "    # #     observables=observables,\n",
    "    # #     # initial_state=lambda wires: entangle_agents_phi_plus(wires, d_qubits, n_agents),\n",
    "    # # )\n",
    "    # # qnode = circuit.qnode(device='lightning.gpu', interface='tf') # Set interface to TensorFlow.\n",
    "    # # tf_device_kwargs = dict(\n",
    "    # #     name='default.qubit.tf',\n",
    "    # #     tensorflow_device='cuda',\n",
    "    # # )\n",
    "    # # qnode = circuit.qnode(device=tf_device_kwargs, interface='tf', diff_method=\"backprop\") # Set interface to TensorFlow.\n",
    "    # # qnode = circuit.qnode(device='default.qubit.tf', interface='tf', diff_method=\"backprop\") # Set interface to TensorFlow.\n",
    "    # # qnode = circuit.qnode(device='lightning.qubit', interface='tf') # Set interface to TensorFlow.\n",
    "    # # qnode = circuit.qnode(device='default.qubit', diff_method=\"adjoint\") # Set interface to TensorFlow.\n",
    "    # # qnode = circuit.qnode(device='default.qubit', interface='tf') # Set interface to TensorFlow.\n",
    "    # # qnode = circuit.qnode(device='cirq.simulator') # Set interface to TensorFlow.\n",
    "    # qnode = circuit.qnode(device='default.qubit.tf', interface='tf') # Set interface to TensorFlow.\n",
    "    # # qnode = circuit.qnode(device='lightning.qubit', interface=None) # Set interface to TensorFlow.\n",
    "    # qlayer = circuit.get_keras_layer(name='ReUploadingPQC', qnode=qnode)\n",
    "    \n",
    "    \n",
    "    # n_qubits = 4 # Dimension of the state vectors in CartPole\n",
    "    # n_layers = 5 # Number of layers in the PQC\n",
    "    # n_actions = 2 # Number of actions in CartPole\n",
    "    \n",
    "    \n",
    "    \n",
    "    #############\n",
    "\n",
    "    qubits = cirq.GridQubit.rect(1, n_wires)\n",
    "    ops = [cirq.Z(q) for q in qubits]\n",
    "    observables = [ops[0]*ops[1], ops[2]*ops[3]] # Z_0*Z_1 for action 0 and Z_2*Z_3 for action 1\n",
    "    # qlayer = ReUploadingPQC(qubits, n_layers, observables, activation='tanh')\n",
    "\n",
    "    # model = keras.Sequential([\n",
    "    #         keras.Input(shape=(n_wires,), dtype=tf.dtypes.float32, name='input'), # Shape of model input, which should match the observation vector shape.\n",
    "    #         qlayer,\n",
    "    #         # CustomQuantumLayer(n_wires, n_layers, observables),\n",
    "    #         # keras.layers.Lambda(lambda x: tf.math.abs(x)), # Convert complex to float via abs.\n",
    "    #         keras.layers.Activation('tanh'), # Ensure outputs of PQC are in range [-1, 1].\n",
    "    #         keras.Sequential([Rescaling(len(observables))], name=is_target*'Target'+'Q-values'),\n",
    "    #     ],\n",
    "    #     )\n",
    "    # return model\n",
    "\n",
    "\n",
    "    ##############\n",
    "    \n",
    "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
    "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables, activation='tanh')([input_tensor])\n",
    "    process = tf.keras.Sequential([Rescaling(len(observables))], name=is_target*\"Target\"+\"Q-values\")\n",
    "    Q_values = process(re_uploading_pqc)\n",
    "    model = tf.keras.Model(inputs=[input_tensor], outputs=Q_values)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'qubit',\n",
       " 'tensor_observables': True,\n",
       " 'supports_broadcasting': False,\n",
       " 'supports_finite_shots': True,\n",
       " 'supports_tensor_observables': True,\n",
       " 'returns_probs': True,\n",
       " 'returns_state': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qml.device('cirq.simulator', wires=4).capabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_model_Qlearning_MARL(n_agents, d_qubits, n_layers, observables, is_target):\n",
    "\n",
    "#     circuit = MARLCircuit(\n",
    "#         n_agents=n_agents,\n",
    "#         d_qubits=d_qubits,\n",
    "#         n_layers=n_layers,\n",
    "#         observables=observables,\n",
    "#         initial_state=lambda wires: entangle_agents_phi_plus(wires, d_qubits, n_agents),\n",
    "#     )\n",
    "#     qnode = circuit.qnode(interface='tf') # Set interface to TensorFlow.\n",
    "#     qlayer = circuit.get_keras_layer(name='MARL-ReUploadingPQCs', qnode=qnode)\n",
    "    \n",
    "#     model = keras.Sequential([\n",
    "#             # Input shape must be 1D into the quantum layer, so use (n_agents, d_qubits,) followed by a reshape layer.\n",
    "#             keras.Input(shape=(n_agents, d_qubits,), dtype=tf.dtypes.float32, name='input'), # Shape of model input, which should match the combined observation vector shape for all agents.\n",
    "#             keras.layers.Reshape(target_shape=(-1,)), # Ensure input to the quantum layer is 1D for total number of wires.\n",
    "#             qlayer,\n",
    "#             # keras.layers.Lambda(lambda x: tf.math.abs(x)), # Convert complex to float via abs.\n",
    "#             keras.layers.Activation('tanh'), # Ensure outputs of PQC are in range [-1, 1].\n",
    "#             keras.Sequential([Rescaling(len(observables))], name=is_target*'Target'+'Q-values'),\n",
    "#         ],\n",
    "#         )\n",
    "#     return model, circuit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v1')\n",
    "n_actions = 2 # because `env.action_space.n == 2`\n",
    "n_wires = 4 # because `env.observation_space.shape == (4,)`\n",
    "# input_shape = (4,) # Shape of model input, which should match the observation vector shape `env.observation_space.shape == (4,)`\n",
    "\n",
    "n_layers = 5 # Number of PQC layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observables = np.array([\n",
    "#     qml.PauliZ(wires=0) @ qml.PauliZ(wires=1), # Z0 Z1 (action 0 -- \"left\")\n",
    "#     qml.PauliZ(wires=2) @ qml.PauliZ(wires=3), # Z2 Z3 (action 1 -- \"right\")\n",
    "#     ])\n",
    "# # observables = lambda wires: [\n",
    "# #     qml.PauliZ(wires=0) @ qml.PauliZ(wires=1), # Z0 Z1 (action 0 -- \"left\")\n",
    "# #     qml.PauliZ(wires=2) @ qml.PauliZ(wires=3), # Z2 Z3 (action 1 -- \"right\")\n",
    "# #     ]\n",
    "\n",
    "# circuit = AgentCircuit(\n",
    "#     wires=n_wires,\n",
    "#     n_layers=n_layers,\n",
    "#     observables=observables,\n",
    "# )\n",
    "\n",
    "# # print(qml.draw(qnode, wire_order=list(range(n_wires)))())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = np.random.random(size=(16,4,))\n",
    "# CustomQuantumLayer(wires=4, n_layers=5, observables=observables)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs=[<tf.Tensor 'Placeholder:0' shape=(None, 4) dtype=float32>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32>\n",
      "tiled_up_inputs=<tf.Tensor 're-uploading_PQC/Tile_1:0' shape=(None, 20) dtype=float32>\n",
      "scaled_inputs.shape=TensorShape([None, 20])\n",
      "inputs=[<tf.Tensor 'Placeholder:0' shape=(None, 4) dtype=float32>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32>\n",
      "tiled_up_inputs=<tf.Tensor 're-uploading_PQC/Tile_1:0' shape=(None, 20) dtype=float32>\n",
      "scaled_inputs.shape=TensorShape([None, 20])\n",
      "setting weights\n"
     ]
    }
   ],
   "source": [
    "model = generate_model_Qlearning(\n",
    "    n_wires=n_wires,\n",
    "    n_layers=n_layers,\n",
    "    observables=None,\n",
    "    is_target=False,\n",
    "    )\n",
    "\n",
    "model_target = generate_model_Qlearning(\n",
    "    n_wires=n_wires,\n",
    "    n_layers=n_layers,\n",
    "    observables=None,\n",
    "    is_target=True,\n",
    "    )\n",
    "\n",
    "print('setting weights')\n",
    "model_target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = model.layers[0].weights\n",
    "# inputs = tf.convert_to_tensor(tf.constant(np.random.random(size=(16, 4,)), dtype='float32'))\n",
    "# # inputs = [tf.convert_to_tensor(tf.constant(np.random.random(size=(4,)), dtype='float32')) for i in range(16)]\n",
    "\n",
    "# # print(qml.draw(circuit, wire_order=list(range(circuit.n_wires)))(*weights, inputs))\n",
    "# circuit.qnode()(*weights, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reduce_all(tf.reshape(tf.tile(weights[0], multiples=[16, 1, 1]), (16, 6, 4, 3))[0] == tf.reshape(tf.tile(weights[0], multiples=[16, 1, 1]), (16, 6, 4, 3))[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observables_MARL = np.array([\n",
    "#     qml.PauliZ(wires=0) @ qml.PauliZ(wires=1) @ qml.PauliZ(wires=4) @ qml.PauliZ(wires=5), # Z0 Z1 Z4 Z5 (action 0 -- \"left\")\n",
    "#     # -(qml.PauliZ(wires=0) @ qml.PauliZ(wires=1) @ qml.PauliZ(wires=4) @ qml.PauliZ(wires=5)), # - Z0 Z1 Z4 Z5 (action 0 -- \"left\")\n",
    "#     qml.PauliZ(wires=2) @ qml.PauliZ(wires=3) @ qml.PauliZ(wires=6) @ qml.PauliZ(wires=7), # Z2 Z3 Z6 Z7 (action 1 -- \"right\")\n",
    "#     # -(qml.PauliZ(wires=2) @ qml.PauliZ(wires=3) @ qml.PauliZ(wires=6) @ qml.PauliZ(wires=7)), # - Z2 Z3 Z6 Z7 (action 1 -- \"right\")\n",
    "#     ])\n",
    "\n",
    "# model_MARL, circuit_MARL = generate_model_Qlearning_MARL(\n",
    "#     n_agents=2,\n",
    "#     d_qubits=4,\n",
    "#     n_layers=n_layers,\n",
    "#     observables=observables_MARL,\n",
    "#     is_target=False,\n",
    "#     )\n",
    "\n",
    "# # model_MARL.layers[0].weights[1].shape\n",
    "\n",
    "# # # Get shapes for weights of quantum layer.\n",
    "# # [w.shape for w in model.layers[0].weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = np.random.random(size=(3, 2, 4,))\n",
    "# model_MARL([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_layer('ReUploadingPQC').weights[0].shape, model.get_layer('ReUploadingPQC').weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_MARL.get_layer('MARL-ReUploadingPQCs').weights[0].shape, model_MARL.get_layer('MARL-ReUploadingPQCs').weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example override of MARL weights with weights from a local agent model.\n",
    "# aidx = 0 # Agent index to update.\n",
    "# updated_agents_weights_var = tf.stack([])\n",
    "# updated_agents_weights_var = model_MARL.get_layer('MARL-ReUploadingPQCs').weights[0]\n",
    "# # updated_agents_weights_var.scatter_update(model.get_layer('ReUploadingPQC').weights[0][:])\n",
    "# # updated_agents_weights_var = tf.tensor_scatter_nd_update(updated_agents_weights_var, [[aidx, 6, 4, 3]], model.get_layer('ReUploadingPQC').weights[0])\n",
    "# # updated_agents_weights_var[aidx].assign(model.get_layer('ReUploadingPQC').weights[0])\n",
    "# # updated_agents_weights_enc = model_MARL.get_layer('MARL-ReUploadingPQCs').weights[1]\n",
    "# # updated_agents_weights_enc[aidx].assign(model.get_layer('ReUploadingPQC').weights[1])\n",
    "# # updated_agents_weights_enc\n",
    "# updated_agents_weights_var\n",
    "# # model_MARL.get_layer('MARL-ReUploadingPQCs').set_weights([updated_agents_weights_var, updated_agents_weights_enc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reduce_all(model_MARL.get_layer('MARL-ReUploadingPQCs').get_weights()[aidx][0] == model.get_layer('ReUploadingPQC').weights[0])\n",
    "# tf.reduce_all(model_MARL.get_layer('MARL-ReUploadingPQCs').get_weights()[aidx][1] == model.get_layer('ReUploadingPQC').weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_MARL.get_layer('MARL-ReUploadingPQCs').get_weights()[aidx][1].shape, model.get_layer('ReUploadingPQC').weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model_MARL, show_shapes=True, dpi=70, to_file=\"model_MARL.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(qml.draw(circuit, wire_order=list(range(circuit.n_wires)))(*model.layers[0].weights))\n",
    "# qml.draw_mpl(circuit_MARL, wire_order=list(range(circuit.n_wires)))(*model_MARL.layers[1].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAADgCAYAAADSZNceAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3deVQUV/o38C/N7sIiIGq7QMRojAvGbRhN0AQhOhlRcRDjMriAMVEjJvoalRFwCwE14RcBMVEhRhNwdFCUOEZjDBiCOSKoYNhkEZV9R7pZ7vsH6Rra7oZubCwans85nmNVdd1+6pY+FFX3PqXFGGMghBDCh2IB3xEQQkhPRkmYEEJ4pMN3AJ1BJBKhuLiY7zAIIWpkaWkJXV1dvsNQu26ZhH/66ScsXLgQAwYM4DuUdlVUVKB3797d8h+XOpWWlsLU1BQCQff45a25uRnl5eUwMzPjOxSNUFBQgLi4OEycOJHvUNSuWyZhAHBwcMB//vMfvsNo14wZM/Dpp5/iL3/5C9+hdGk2Nja4du0aBg8ezHcoapGXlwcHBwekp6fzHYpGmDZtGt8hdJrucVlBCCEaipIwIYTwiJIwIYTwiJIwIYTwiJIwIYTwqNuOjiA9W1JSEvT09PDqq6/yHcpzSUpKglgsBgBMnjyZG6JXXl4OADAyMgIAaGtrS+3HGENzczMAQCAQQEtL60WFzMnOzsbgwYOhp6eHnJwcWFlZcdsePnyIgoICAICVlRUsLS1feHxdBV0Jk27pwoUL+PHHHzv1OxobGzu1fQBwdnbGvXv3UFhYCEmZl/j4eERFRUFPTw8nT56EUCiEi4sLWpeBycvLw6pVqzB79mykpqZ2epzPyszMxLhx4/D48WMAgI6ODgIDA9HQ0AAAqK2tRWFhIXbt2oWoqKgXHl9XQlfCpFvasWNHp7YfGxuLAQMGYMKECZ36PQAwa9YsDBkyBACQm5uLsLAwhIeHAwCWLVuGqqoqrF+/Hn5+fti5cycAYNiwYXB3d0d2dvYL/22gqakJn3/+OXcFDwCDBw+Gg4MDAgMD8cknn2DkyJEYOXJkp/+g1AR0JUy6padPn+LBgwfc8sOHD1FdXY2qqircvXtX6rOPHj2CWCzGkydPUFhYCAAoKipCUVERmpqaIBKJUFRUxN0CyM7OhqenJ8rKylBdXY2CggJuW2fbtm0b5s2bJ7VOR0cH3t7e8PPzw9mzZ7n1+vr60NPT45Zra2tx8+ZN1NTUSO3fVt8UFBTg3r17KsV48OBBrF+/XmZ2o62tLa5cuYK0tDSV2uvuKAmTbmnixIk4cuQIAODUqVOwsrLCV199hc2bN8PJyQkHDhwAAISGhmLo0KE4evQo5s+fD2trawQGBiIlJQWWlpbIzs5GfX09tm/fjrlz5wIA7t69i+rqasTFxSElJQUeHh7YvXt3px9TUVERoqKi4ODgILPNxcUF3t7eWL58Oe7cuSOzPSIiAocOHUJ2djbs7OwQExMDoO2+2b9/Py5duoSrV69i6tSpqKioaDfGxMREmJqaYuTIkXK3T5kyBV9//bUqh93tURIm3ZKdnR3398WLF6Nfv36wsbHB4cOH4ePjg3PnzgEAPD090dTUBGtra/z666/Yv38//P39MXPmTPTq1QsAYGxsDEdHR669uXPnwtDQEHPnzsW0adNw8uRJ7Nmzp9OP6datW9DX10ffvn3lbt+5cyccHR3h7OyM0tJSbn1mZib8/f2xZcsWLFq0CB988AE2bNgAQHHf3L59G3FxcZg0aRLs7e1RXV2NyMjINuOrra1FZGQkVq1apfAzAwcORFJSUgeOvvuiJEy6JUNDQ6llXV1dDBo0CAAwZMgQ7qpOIBBAIBBg9OjRAIAFCxagtLQU8fHxSo8oMDExgYGBgRqjly8vLw/9+/dXuF1LSwsRERHo27cvXF1duQeHly9flhp94OjoiAcPHuDmzZsA5PfNjRs3oK+vD7FYDLFYjIiICKkfRPJs374dNjY2OHPmDM6cOYOmpibExsZyoyAAwMLCAikpKR3ug+6IHsyRbqmtBNrWNktLS5iZmUndS+0qhEIhqqqq2vxM7969ER0djcmTJ2Pr1q1Yu3YtDAwMkJycDMYYtLS0YG1tDUNDQ7k/OCR9Y25ujqSkJEyaNInbVlRU1OZ3m5mZITExkVtubm7G1atXMXHiRAiFQgCAWCxGnz59lD7mnoCuhEm31NzczI2TBVqe2EuGcLUeQysheViVlpYGQ0ND2NrawszMDFlZWQCA9PR0qaf9urq6qKmpQVVVFXJzc1FWVtbZhwRbW1uUlZWhqalJar3kalXCysoKp0+f5q50nZ2dUVVVxV2BpqWlQSgUYsyYMQDk980777yDgoIC7Nq1C9XV1UhMTERKSgoYY0hISJA7PM/b2xtHjx7l/ujq6iIgIACTJ0/mPvPo0SPY2Niot2M0HCVh0i0lJCQgPj4eubm5uHDhAgoLC3HmzBnk5+fj/PnzyMnJQXx8PPf5gIAAREdHw9fXF+Hh4TAwMICbmxs8PT2xZs0aaGtrIycnBz/99BOAlhKkXl5eSEpKgqenJ/bt29fpxyQUCjFixAip8pe3bt1CVFQUwsLCkJ+fz623t7dHUFAQAKBfv34IDg6Gh4cHjh07hgMHDiAmJgZaWloK+yYpKQl+fn7w9fXFsGHDcOLECTg4OCA1NRV2dnbw8fHp0DFkZGRgyZIlz9UP3Q7rhmJjY5mzszPfYSjF3t6e/frrr3yH0eUNHz6c5efnd0rbAoGAZWVlsczMTCYWi6W2PXz4kDU2NrKKigpWV1cnta2+vp4xxlhlZSUTiUQqfWdubi4bMWJEu58bMmQIy8vL45avXbvG/P39lf6empoa7u/19fUq92FlZSUrKyuTWpebm8v27NmjUjuMMVZbW8tsbW3Z06dPuXXr169n//d//9fuvn/961/Z77//rvJ3aoAiuhImPRpjDIwxCAQCDB8+XOYNJ0KhENra2jA2NpZ52Kevrw+gZerwi7qHbG9vj7q6Oqmr3rb07t2b+7u+vr7KRfGNjIxgamrKLYtEIty4cQNLly5VqR2gZTjg8ePHX8hDTE3So5NwUlKSygPR+dDU1MT96WzNzc1S38daTYXtbpqbmxEeHg4LCwtERkbi6dOnfIckY9asWfjyyy+xY8cObsqvj48PUlNTUVlZ+cLjaWhogKurK4YOHarSfnFxcZgzZw7Gjx8PALhy5Qq2bduGqqoqqZoSPVGPHh1x4cIF9O3bt9OmdTY2NkJH5/m7eP78+Th//jxCQ0OxdOlSqasbdZHEevv2baxfvx6JiYkICQnB3//+925bXEUgEMDd3R3u7u58h6KQookNTk5OLziSFh0d2TB9+nSp5bfeegtvvfWWOkLSeD06CXdmfQF11hZwdnZGXFwc1qxZo4bIZLWO9bXXXoOzszMePXqE1atXd8r3EUL+p0ffjmhdX6Azaws8Lx0dHe7+o4SieOXF2la88mI1MDCQ+b5nZWVl4Y8//uBuV5SUlKCoqAglJSUAWn5tLSoq4iZFyKtBUFBQgKqqKmRnZ6O2traDvUOIZuuxV8J37tzBokWLMG/ePIwdOxbLli1DQEAA7t+/j5iYGHz00UfYtGkTQkNDsW7dOnz55ZcIDw9HcnIy/Pz8YGtri1mzZiE9PR39+/fH9u3bcf/+ffzyyy9StQUMDAzU/qbYU6dOyY23V69ecmP9+OOPkZKSIjfezZs3qxRrY2Mj5syZg23btuH7779HSUkJoqKicP78eaxcuRJHjhzB6tWrIRaL8e6778LHxwe//fYbTE1NUVtbi5UrV+LSpUv44YcfsGzZMvj6+uLbb7/FwoUL4evr2+b3Xr16Febm5mrtS74UFxejrq4OFy9e5DsUjfBs0aHupMcm4bFjx3L1BRYvXowPP/wQNjY28PLywpEjR/Dtt99i06ZN8PT0xNq1a7naAiEhIfjXv/6FJ0+eyNQWuH//PgDp2gKdUepQUbxXr16VG6uXlxccHBzkxqtqrFlZWbCwsMCMGTNgYmKCmTNnAgBWrFiB8+fPc/VjdXV1MXr0aPTp0wdxcXFcgg0JCUFkZCQ8PT3h5eUFKysr3L17V2byxLMaGhpw6tQpmREKmqqurg61tbX46quv+A5FIyhTPEhT9dgkDEjXF1CltsAHH3ygUm2BziAv3rZifeONN9QS78iRIxEcHIyQkBAUFxdLjSjYtm0bZs+ejU2bNuHcuXNwcXGRqkEAtFTzklzN6ujo4OWXX4aWlpbMmyGeZWhoiCNHjqg8xKqrysvLg4ODA86cOcN3KBpB3b9NdiU9OgkrSkqaVltAUbzqjjUrKwu6urpYuXIlIiIiIBaLsXfvXm77pEmTMGHCBBw+fBgZGRkICQlBYWGhyjUICOlJevSDudb1BTqztsDzEolEMnP124pXXqwAFMb7bKwikUjqWICW+gSnTp3CmTNnoKenh0GDBuHJkydoaGhAfX0997lt27Zh9+7d3NW4ohoEkrhfxCuCCOnKemwSzsrK4uoLBAcHd2ptgef1/fffo7S0FOHh4aipqWm3FoK8WAEojLd1rImJifj3v/+N3NxcLF++HBs2bMDy5csxbtw4mJubw8nJCbdu3cLbb7+N27dvQ09PDwEBAVysM2bMwJgxY7Bs2TIAQK9eveTWILhw4QKKi4tx/PjxF1L8hpAui7cZ051I3bUjnre2QFvUXTuirVjbileZWCVEIhH3+WeP+enTp2zLli0y+8irQaCKzqwdwQdla0eQFt25dkSPviesDNaqtoC86ZWSOqnGxsYy29oba6tu7cUKKI5XlVhb32OWPNxMT0/HvXv3cPPmTXh4eMjsI3k1OyFEWo+9HaEMTagtIMF3rPHx8di5cyemTJlC9WLVKCkpCb/99ht+++03qfv+5eXlKC8vV1hThDHGe/2P7Oxs7tlCTk6O1LaHDx9yx9V6UlFPREm4DZLaAoWFhdiyZUuXHqPKd6wrVqxASkqKzJuANYE6Hg521gNGZ2dn3Lt3D4WFhVwyjY+PR1RUFPT09HDy5EkIhUK4uLhIJdu8vDysWrUKs2fPRmpqaqfE1pbMzEyMGzeOGzeuo6ODwMBArghRbW0tCgsLsWvXLkRFRb3w+LoSSsKkR4uNjZX7duIX3UZbZs2ahblz50JbWxu5ubkICwuDp6cnevfujWXLlsHb2xtnz56Fn58ft8+wYcPg7u4ONze3TitQpUhTUxM+//xzqRE2gwcPhoODAwIDAwG0jDefO3cuXnrppRcaW1dESZh0K7W1tbh586bUNFdV6maoo05IQUEBt13dtm3bJvPbho6ODry9veHn54ezZ89y6/X19aXu38vrG6Dtuinyan605+DBg1i/fj0EAun0YmtriytXriAtLU2l9ro7SsKk24iIiMChQ4eQnZ0NOzs7xMTEAABSUlJgaWmJ7Oxs1NfXY/v27Zg7d65UjY+UlBSEhoZi6NChOHr0KObPnw9ra2sEBgYq3B+ATBsA4OHhgd27d6v9+IqKihAVFQUHBweZbS4uLvD29sby5cvlXpUr6ptTp07BysoKX331FTZv3gwnJyccOHAAALB//35cunQJV69exdSpU5WaOpyYmAhTU1OMHDlS7vYpU6YoLM/ZY/E3MqPz0OuNup/2hqhlZGSw0aNHc8shISHM2tqaW+7VqxdLT09njDEWGRnJpk+fzhhjbMCAAezWrVuMMcaampoYAPbDDz8wxhgLDg5m5ubmrLGxUeH+z7bBGGPl5eVSr/CRpyOvN4qNjWV9+vSR+UxoaChLTk5mzc3NbMGCBcza2pqVlJSwGzdusG+++abdvrGwsGDnzp1jjDEWFhbG7O3tWVJSEps3bx5LTk5mycnJ7JVXXmGHDx9uM9aamhr20Ucfccv6+vosJydH6jNBQUHszTff5Jbp9Ub0eiPSTVy+fFmq+LyjoyMePHjAvXFYmboZ8mpvlJaWqlwnxMTEpFNe4ZOXl4f+/fsr3K6lpYWIiAj07dsXrq6u3MPC9vpGXh2S1jU/xGIxIiIi4Ojo2GZ827dvh42NDc6cOYMzZ86gqakJsbGxKCgo4D5jYWHB/cZAWtA4YdItGBgYIDk5GYwxaGlpwdraGoaGhs+VDLtanRChUNjuNPjevXsjOjoakydPxtatW7F27VqV+kbyw8bc3Fzlmh9mZmZITEzklpubm3H16lVMnDiRG58uFos7/HaO7oquhEm34OzsjKqqKu4qKy0tDUKhEGPGjAGgfN0M4PnrhOTm5nbKVGxbW1uUlZXJjAuWXK1KWFlZ4fTp09yVbnt9I68OiaKaH4wxJCQkyB2S5+3tjaNHj3J/dHV1ERAQgMmTJ3OfefToEY0jfwYlYdIt9OvXD8HBwfDw8MCxY8dw4MABxMTEcFd2ytTNkHjeOiGenp7Yt2+f2o9RKBRixIgRSE9P59bdunULUVFRCAsLk3oDs729PYKCgtrtG0V1SJKSkuTW/EhNTYWdnR18fHw6dAwZGRlYsmTJc/VDt8PrLelOQg/muh9la0fU19cr/JwydTPUUSeksrKSiUSiNuPsyIM5xhi7du0a8/f3b3c/iZqaGqkYVa2/Ia/mR25uLtuzZ49K7TDGWG1tLbO1tZV6aEkP5ujBHOlm9PX1FRZ+FwqF0NbWhrGxsdSMQkndDNaq9sbw4cOhq6ur1P6t2wBa6mR01n1ke3t71NXVSV31tqX1m7nb6htFjIyMYGpqyi2LRCLcuHEDS5cuVakdAAgNDcXx48c75aGlJqMkTAj4r72hyKxZs/Dll19ix44d3JRfHx8fpKamorKy8oXH09DQAFdXVwwdOlSl/eLi4jBnzhyMHz8eAHDlyhVs27YNVVVVCotN9RQ0OoIQ/K/2hru7O9+hSFE0scHJyekFR9KioyMbpk+fLrX81ltv4a233lJHSBqProQJIYRHlIQJIYRH3fZ2REZGBj799FO+w2hXXl4eIiIicO3aNb5D6dIqKipw6NAhucXzNVFFRQXKy8s14t9oV9B61l13o8UYTxWfO1FmZiZOnDjBdxhKYX/OYiJtk9dPzc3NOHfunEbWMAbo3KtqzZo1GDhwIN9hqFtxt0zCpGcQiUSwtLRUqroXIV1UMd0TJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHlESJoQQHunwHQAhqqirq4NIJALQ8rZlxhjKy8u57QYGBjA0NOQrPEJURlfCRKMcPXoU/fv3h5WVFV5++WWIxWJYWVnBysoKFhYWOH/+PN8hEqISLcYY4zsIQpRVXFyMYcOG4enTpzLbevfujaKiIvTq1YuHyAjpkGK6EiYaxcLCAuPHj5dZr6WlBScnJ0rARONQEiYa57333kPfvn2l1hkZGcHDw4OniAjpOLodQTROVVUVBgwYIHVLom/fvigtLYWuri6PkRGiMrodQTSPkZERXn/9dW5ZIBDAxcWFEjDRSJSEiUby9PSEkZERgJar4FWrVvEcESEdQ7cjiEaqr6+Hubk5amtrYWZmhuLiYmhpafEdFiGqotsRRDMZGBhgzpw5EAgEWLp0KSVgorEoCRONtXr1ajQ3N8Pd3Z3vUAjpMLodQTRWU1MTZs2ahatXr/IdCiEdVSyVhGNiYvD777/zGRBRE8ZYj/gVvbKyEsbGxnyHoVbd8dx1x2PqKHt7e8ycOVOyWCxVwCcmJgalpaWYOHHii4+MqJW/vz/ef/99mUkN3Y2BgQHfIajdl19+iX/84x+wtLTkOxS1EIlECAwMxI4dO/gOhXfXr19HU1NT6yQsW0Xtb3/7G91j6wZCQ0Oxfv16DBgwgO9QiIpOnz6NVatWYcyYMXyHohZVVVU4dOgQtm7dyncovNPW1kZVVZXUOnowRwghPKIkTAghPKIkTAghPKIkTAghPKIkTAghPKIkTIiG2717N7744otO/Y7GxsZObV8iPj4eBw8exDfffIMBAwbAxcUFreeT5ebmwt3dHY6Ojrh3794LielZmZmZ6NOnD3Jzc/Hw4UMEBgaioaGhw+3Riz4J0XCdPf42NjYWAwYMwIQJEzr1e3JzcxEWFobw8HAALUPb1q9fDz8/P+zcuRMAMGzYMLi7uyM7Oxuvvvpqp8YjT1NTEz7//HOIxWIAwODBg+Hg4IDAwEB88sknHWqTroQJ0XBPnz7FgwcPuOWHDx+iuroaVVVVuHv3rtRnHz16BLFYjCdPnqCwsBAAUFRUhKKiIjQ1NUEkEqGoqIh7g3V2djY8PT1RVlaG6upqFBQUSL3dWp22bduGefPmccs6Ojrw9vaGn58fzp49y63X19eHnp6e1L61tbW4efMmampquHVt9QMAFBQUqHw1ffDgQaxfvx4Cwf9Sp62tLa5cuYK0tDSV2pKgJEyIhps4cSKOHDkCADh16hSsrKzw1VdfYfPmzXBycsKBAwcAtEzgGTp0KI4ePYr58+fD2toagYGBSElJgaWlJbKzs1FfX4/t27dj7ty5AIC7d++iuroacXFxSElJgYeHB3bv3q32YygqKkJUVBQcHByk1ru4uMDb2xvLly/HnTt35O4bERGBQ4cOITs7G3Z2doiJiWmzHwBg//79uHTpEq5evYqpU6eioqKi3RgTExNhamqKkSNHymybMmUKvv76axWPugUlYUI0nJ2dHff3xYsXo1+/frCxscHhw4fh4+ODc+fOAWgphN/U1ARra2v8+uuv2L9/P/z9/TFz5kzuBanGxsZwdHTk2ps7dy4MDQ0xd+5cTJs2DSdPnsSePXvUfgy3bt2Cvr6+3Gn2O3fuhKOjI5ydnVFaWiq1LTMzE/7+/tiyZQsWLVqEDz74ABs2bGizH27fvo24uDhMmjQJ9vb2qK6uRmRkZJvx1dbWIjIyUuHLAwYOHIikpKQOHTslYUI0nKGhodSyrq4uBg0aBAAYMmQId5UnEAggEAgwevRoAMCCBQtQWlqK+Ph4pYvrmJiYdEq9jry8PPTv31/uNi0tLURERKBv375wdXWVekh4+fJlqRobjo6OePDgAW7evKmwH27cuAF9fX2IxWKIxWJERERI/eCRZ/v27bCxscGZM2dw5swZNDU1ITY2FgUFBQBa3gKekpLSoWOnB3OEaLi2Emhb2ywtLWFmZiZzf5UPQqFQpqZCa71790Z0dDQmT56MrVu3Yu3atQBaCjglJydzVdqsra1haGgo84OidT+Ym5sjKSkJkyZN4tYVFRW1GZ+ZmRkSExO55ebmZly9ehUTJ06EUCiEWCxGnz59VDpmCboSJkTDNTc3o7m5mVtuamrihnUxxqS2AeAeXqWlpcHQ0BC2trYwMzNDVlYWACA9PZ17+g+0XFnX1NSgqqoKubm5KCsrU/sx2NraoqysDE1NTdw6yZWqhJWVFU6fPo2bN29y65ydnVFVVcVdhaalpUEoFGLMmDEK++Gdd95BQUEBdu3aherqaiQmJiIlJQWMMSQkJMgdjuft7Y2jR49yf3R1dREQEIDJkycDaHngaWNj06FjpyRMiIZLSEhAfHw8cnNzceHCBRQWFuLMmTPIz8/H+fPnkZOTg/j4eO7zAQEBiI6Ohq+vL8LDw2FgYAA3Nzd4enpizZo10NbWRk5ODn766ScAwIwZM+Dl5YWkpCR4enpi3759aj8GoVCIESNGID09HUDLPeKoqCiEhYUhPz+f+5y9vT2CgoK45X79+iE4OBgeHh44duwYDhw4gJiYGFy8eFFhP/Tq1Qt+fn7w9fXFsGHDcOLECTg4OCA1NRV2dnbw8fFROf6MjAwsWbKkYwfPWlmzZg07duwYI5pv2LBh7PHjx3yHQTpg4sSJ7M6dO53StkAgYFlZWSwzM5OJxWKpbQ8fPmSNjY2soqKC1dXVSW2rr69njDFWWVnJRCKRSt9ZWVnJLCws2v3ctWvXmL+/v1Jt1tTUyMSXn5+vclxlZWVS63Jzc9mePXtUaqe2tpbZ2tqyp0+ftvvZzz77jO3YsaP1qiK6Eiakh2CMgTEGgUCA4cOHQ1dXV2q7UCiEtrY2jI2NZR726evrAwCMjIw67R6yvb096urqpK58Fendu7dMfIMHD1bp+4yMjGBqasoti0Qi3LhxA0uXLlWpndDQUBw/frzDDywpCRPSAzQ3NyM8PBwWFhaIjIzE06dP+Q5JLh8fH6SmpqKysvKFf3dDQwNcXV0xdOhQpfeJi4vDnDlzMH78+A5/b5dOwvfu3cOYMWPUMi2zvLwc7u7uGDduHLeus+bc//bbb5g0aRLMzc2xceNGuLm5YdGiRdwMJQA4fvw43n33XYSHh+PTTz+Fo6Mjdw+utbNnz2L16tU4ePAgwsPD4eXlhVmzZuH69evPFWNiYiImT54MCwsLbN26FYsXL8a0adOk7h0q8t///heDBw/GwIEDER0dza2XzOu3s7Nrc8xkZ57X5z2nmnDuOkIgEMDd3R2FhYXYsmWLzJVuV+Lk5MTLewP79OkjNRNOGdOnT8eoUaOe74tb35zoiveEV69ezbZv366Wti5evMjGjh2rlrbas2PHDjZ58mTGGGNNTU3M3t6evfHGG4wxxgICAtjrr7/Ompubuc9nZWUxc3Nz9uOPP3Lrzp07x0aMGMFKS0ul2vbw8GD/+c9/2vx+Ze4J79y5k02cOJFbXrduHTMyMlLq3pabmxubN2+ezPrr16+zQ4cOtbt/Vz6vfJ+7zrwnzAdl7wn3BGq5J1xQUICqqipkZ2ejtrZWan1b87Dbmp8ubz67hLz7T/LmiQNAVlYW/vjjD6mqSwBQVlaG5ORkaGtrS61vPee+vXnmIpEIKSkpMsN9FJHcQwNarkLefPNNJCQkIDs7G97e3ti6davU2MWXXnoJ8+fPx5o1a9Dc3AyxWIzFixdj27Zt6Nevn1Tbe/fulWq/o569hzVt2jRUVVUhJydHar28c2toaCg3BgMDA269KudV0TkFVDuvqtRRUHRONeHcke5D5SRsZWWFL7/8En//+9/x2WefAVBuHrai+emK5rMrIpQZ4lgAACAASURBVG+eeGNjIxwdHZGfn4/PP/8crq6u3OfDw8Ph7e2NmpoafPrpp9z6O3fucHPu25tnfvz4cQQFBaGurg6vvPIK5s2bx02BVNa1a9fwzjvv4LfffkN9fT3Gjh0r85lx48YhKyuLm/FTW1sr93Pm5uZ4++23Vfr+9tTV1SE0NBTTpk2T+vWqI3PsAcV1CuSRd04BqHxeW59ToO06Cqqc065+7ohmU3nGnLm5OaysrHD37l00Nzdz87B9fX0BACEhIYiMjISnp6fUfg4ODjLz0+/fvw9PT0+sXbuWm88eEhKCf/3rX/Dy8pK5cpXME5dclZWXl2PDhg2IjY2FhYUFZsyYARMTE+510rm5udiwYQMKCgrQp08feHh4cGMcx44dy825X7x4MT788EPY2NjAy8sLR44cwbfffotNmzahsbERO3bsQFxcHKysrDBlyhRYWFhwBU7aUlJSgiNHjiA+Ph7jxo3D+vXruTnq8qZoSkrzZWdnc4PPX375ZSXOSscVFxdj06ZNCAkJwe7du7Fx40Zum7LnVh5lz6uic/rOO+8gKytLpfPa+pwCis/rhg0b2j2nfJ672tpabNy4EUZGRh3av6tpbGxETU0NFixYwHcovMvIyJDJHSonYR0dHbz88svQ0tKCtra21DxsoOWqxtzcHD///DP30EYgECAwMFDuFEp589k/+OADxMfH44033pD6rLx54mvXrkVVVRWCg4MREhKC4uJi7slvdHQ0JkyYwE0nNDMzk2qv9cMJRfPMtbS0UFtbi9TUVFhZWcHMzEzhHPdnmZubw8PDAx4eHty6V155BQDw+PFjWFlZSX1eMm1z8ODBePjwIQAgPz+f65vOYGFhgf379+POnTuIj4/HRx99xG1TdG6Bln8H8m7NNDY2wtDQUOnzquic3rx5E5MnT1b5vCpTR0GZc8rnudPT04OLiwuGDRum8r5dUV1dHa5fv47Vq1fzHQrv/v3vf8vkweeuHaFoHnafPn24cXvKFgcB2p7PrmieeFFRET755BNERERALBZj7969AFrGRWZkZCj8LkVxtV6vra2NY8eOwd/fH5WVlTA0NMR7772n9PE8a8qUKdDR0UFycrLMf+R79+5h1KhRGDlyJPT19aGjo4Pff/9d7n/kyspKtT1BlhRIGTduHMLCwrgr3bbm2JuYmMjt25KSErnJQ9F5bWvuf15eHlauXKnSeVWmjkJHz+mLOne6urp4/fXXMWbMmHZj0gRVVVXQ09PDnDlz+A6Fd/fu3ZOpkaHyPWHGmNTcakXzsCdOnIhNmzZh06ZN8PLyAoA256fLm88OSM+LVzRPPD09HXp6ehg0aBCePHmChoYG1NfXY+HChXj8+DHi4uIAtIwDbP2drdtua7795cuXER0dDTc3N+zbtw8mJibt9pNYLEZ9fb3M+kGDBiEwMBD+/v5S31FbW4uwsDB88cUXEAgEeOmll7Bx40b8v//3/5CbmyvVxpUrV9Qyf18kEnExDhw4EMeOHYOXlxdu3LgBQPG5lWy7efOmVBxNTU2Ijo6WeuNBe+e1rbn/Z86cUfm8KltHoa1zqgnnjnQjrcdKKDNETU9Pj61Zs0Zq6M3+/fuZtrY2MzU1ZevXr1e475YtW9iQIUOYp6cn27dvH+vfvz+7evUqEwgEbMWKFew///kPW7RoEbt69SpjjLHs7Gz22muvsenTp7PMzEzGGGNhYWFs8uTJ7OjRo2zVqlXs/v37LDU1lVlaWjInJycWEhLCDAwMmJ+fH2OMsU2bNjFdXV22cOFCtmjRImZiYsIuXLjAMjMzubYPHTrEALBPPvmE5eXlsffff5/17duXxcXFMcZahgwZGhoyS0tL9tJLL7GlS5fKTHdsLS4ujtna2jJ9fX128uRJmemhjDF28OBBtmTJEhYVFcWOHz/OXF1d2Q8//CDzuc8++4yNGTOGrVq1in3xxRfs4MGDLCkpqc1zxFj7Q9Ti4+PZhAkTmJ6eHvv222+5GDds2MBMTU2Zv78/q6ysbPPcfvHFF2zWrFnM39+f7d27l61Zs4brM8aY0udV3jlljKl8XoOCgrh2c3JyWExMjMLzquicdoVzR0PUui95Q9TUNk5Y3jxseeTNT29rPrs88uaJi0Qibn77s/PeCwsLWVlZGauqqmLV1dXKHhJjjLGKigq2b98+9vTpU/bo0SOWnJzMwsLCWHh4uErtyFNaWsrefvttBoCdPHmyzc/m5OSwyspKpdtWZ+2I9s5tXl6ezHhYxlQ7r4rm/nfGeVXHOe3Mc0dJuPuSl4TVVk9Y2Se5QqEQALh7YqzVfPZn77MpIm+eeOt7jc8+nFH2QZo8wcHBSEtLQ0lJCQYPHgwDAwNcvnwZK1eu7HCbEv369cOFCxcQFBSEHTt2YODAgZgxY4bcz/L5kKa9cztkyBCZdaqeV0Vz/zvjvKrjnGrKuSNdH6/TljVhPvvq1asxdOhQLFy4EFOnTsV7772HadOmYdeuXdw979Z/VJ2KKxAIsHHjRqSlpaGsrAxRUVG8zJtXp65+XuWd07/97W9SxVyU0R3PHeFB6+virjhtmXQMlbLUXD35dkRcXBw7cOAAi4iIYJaWlmzBggVSU8RzcnLYP//5TzZr1ix29+7dzgpZroCAAPbyyy8zAwMD9o9//IM9ffqU5efns4CAAKVuozJGpSwJ6dFiY2M7/DJKdbahSG5uLsLCwuDl5YVly5bB29sbZ8+ehZ+fH/eZYcOGwd3dHW5ublKjcDrbkydPYGJigvv37yMlJQXXr1/HiRMnMHjwYDg4OLQ5y7c9lIQJ0VDy6m0oqtGSnZ0NT09PlJWVobq6GoD82h5t1XiR10ZBQQG3/Xlt27YN8+bN45Z1dHTg7e0NPz8/nD17lluvr6+vVO2R9urBtFfvpjXGGFavXg0tLS2MGDECbm5u3DvnbG1tceXKFaSlpal0vBKUhAnRQIrqbSiq0XL37l1UV1cjLi4OKSkpCmt7KNofgEwbAODh4YHdu3c/9/EUFRUhKioKDg4OUutdXFzg7e2N5cuX486dO0r3RXv1YFStiTJw4ECpZZFIJPWG5ilTpuDrr79W9bBbtL45QfeEuw+6J6y52rsnnJGRwUaPHs0th4SEMGtra265V69eLD09nTHGWGRkJJs+fTpjjLEBAwawW7duMcZaSnQC4MY3BwcHM3Nzc9bY2Khw/2fbYIyx8vLydkufKnNPODY2lvXp00dqXWhoKEtOTmbNzc1swYIFzNrampWUlLAbN26wb775pt2+sLCwYOfOnWOMtcwvsLe3Z4wxlpSUxObNm8eSk5NZcnIye+WVV9jhw4fbjK+1iooKtmLFCql1QUFB7M0332x3X7onTEg3IK/ehqR6G6BcmQB5tT1KS0sRHx+vUpkBExOTDr/Wp7W8vDyFQw4l0+r79u0LV1dXqRm7bfWFonowrWuiiMViRERESF3Vtuezzz6Dv7+/1DoLCwvutwNVURImRMO0rrcBQKreRke1VbPlRRAKhTI1FVrr3bs3oqOjkZKSgq1bt3Lrle2L1j9YWtdEkfyRVHhsz5EjR+Di4gILCwup9WKxmCsopSpKwoRomLbqbQCKa7To6uqipqZGKtnJq+3RVo2XZ9vIzc1VSy0MW1tblJWVoampiVsnuVKVsLKywunTp7kr/vb6QlHdEEU1URhjSEhIkLrSbu3YsWMYNWoUXnvtNYjFYly4cAElJSUAWh5y2tjYdOjYKQkTomH69euH4OBgeHh44NixYzhw4ABiYmK4qz03Nzd4enpizZo10NbWRk5ODn766SfMmDEDXl5eUkPMAgICEB0dDV9fX4SHh8PAwEDh/gBk2vD09ORqdD8PoVCIESNGID09HQBw69YtREVFISwsTOrty/b29ggKCmq3Ly5evIjCwkKcOXMG+fn5OH/+PHJychAfH49evXrBz88Pvr6+GDZsGE6cOAEHBwekpqbCzs4OPj4+MvEFBwdj5cqVeOONN6ClpQV9fX34+vpypV0zMjKwZMmSjh186zvE9GCu+6AHc5pL2ckaiuptMCa/RotkH4m2anso2v/ZNiorK5lIJGozTmUna1y7do35+/u3+znGGKupqZGJSVFftBXXszVRcnNz2Z49e1Rqp7a2ltna2ir1bkZ6MEdIN6Ko3gbQcmWpra0NY2NjqZobkvfbsVa1PYYPHw5dXV2l9m/dBtBSV0Rd95Ht7e1RV1cndeWrSO/evWViUtQXihgZGUlNVReJRLhx4waWLl2qUjuhoaE4fvx4h+/JUxImpIfpyrU9fHx8kJqayksNjoaGBri6umLo0KFK7xMXF4c5c+Zg/PjxHf5etVVRI4RoBoFAAHd3d7i7u/MdilxOTk68fG9HRjdMnz79ub+XroQJIYRHlIQJIYRHlIQJIYRHWoz9OZoZwHvvvYezZ8+ib9++fMZE1KC0tBSmpqYQCLr3z9nm5uZud4zl5eXo27cvdHS6xyMbxhhKS0u5MbU9WUVFBdauXYtdu3ZJVhVLJeGKigqpUnCEdGVisRi2trZITU3lOxRClGZkZNT6lWHSSZgQTSISiWBpadluGUJCurDi7vV7HCGEaBhKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiNKwoQQwiMdvgMgRBU//fQTEhMTAQCNjY0QiUTw9/fnts+cORNTpkzhKzxCVEavvCcaJTY2Fs7OzmhoaJDZpqOjg4SEBEycOJGHyAjpkGJKwkSjNDY2wszMDFVVVTLbBgwYgMePH/MQFSEdVkz3hIlG0dHRgYuLCwQC6X+6enp6WLVqFU9REdJxlISJxlm5ciX69u0rtU5PTw/Lly/nKSJCOo5uRxCNwxhD//79UVJSwq0bMWIE0tPTeYyKkA6h2xFE82hpaWHp0qXQ1dUFABgaGsLDw4PnqAjpGErCRCP985//hIGBAYCWpLx48WKeIyKkYygJE41ka2sLExMTAMCoUaMwePBgniMipGMoCRONtXLlSmhpacHT05PvUAjpMHowRzRWVlYWXnnlFTx+/BhmZmZ8h0NIR9CDOaK5hg8fjn/961+UgIlG6xZXwuvWrUN0dDTfYfCivr4eurq60NbW5juULqc79k1tbS169+7NdxhdwooVK+Dn58d3GM+ruFsU8CkpKcGePXswe/ZsvkN54d59912sXbsWr7/+Ot+hdDkuLi7YsmULpk6dyncoajN48GD88ccffIfBuxMnTiA7O5vvMNSiWyRhADAyMoKFhQXfYbxwenp6MDY27pHH3h49PT2YmJh0q77R0tLqVsfTUX369OE7BLWhe8KEEMIjSsKEEMIjSsKEEMIjSsKEEMIjSsKE/CkpKQn37t3jOwy1KS8vR0lJCZqammS2McbQ1NSEpqYm8DFKVSQSoaysTGpdTk7OC4+jK6AkTMifLly4gB9//LFTv6OxsbFT25eIj4/H8ePHERsbC6FQCBcXF6lkm5eXh1WrVmH27NlITU19ITFJBAYGYty4cRAKhXB1dUV9fT2AloL9gYGBcl9d1Z1REibkTzt27MCHH37Yae3Hxsbizp07nda+RG5uLsLCwuDl5YVly5bB29sbZ8+elZrYMGzYMLi7u8PNzQ2vvvpqp8ck8eTJE5iYmOD+/ftISUnB9evXceLECQAtY6AdHBwQGBj4wuLpCigJE/Knp0+f4sGDB9zyw4cPUV1djaqqKty9e1fqs48ePYJYLMaTJ09QWFgIACgqKkJRURGampogEolQVFSE8vJyAEB2djY8PT1RVlaG6upqFBQUcNvUbdu2bZg3bx63rKOjA29vb/j5+eHs2bPcen19fejp6UntW1tbi5s3b6KmpoZb11Y/AEBBQYHSt3EYY1i9ejW0tLQwYsQIuLm5cW/PBlqq4125cgVpaWlKH6+moyRMyJ8mTpyII0eOAABOnToFKysrfPXVV9i8eTOcnJxw4MABAEBoaCiGDh2Ko0ePYv78+bC2tkZgYCBSUlJgaWmJ7Oxs1NfXY/v27Zg7dy4A4O7du6iurkZcXBxSUlLg4eGB3bt3q/0YioqKEBUVBQcHB6n1Li4u8Pb2xvLlyxVejUdERODQoUPIzs6GnZ0dYmJi2uwHANi/fz8uXbqEq1evYurUqaioqGgzvoEDB0oti0QiODo6Sq2bMmUKvv76a1UOW6NREibkT3Z2dtzfFy9ejH79+sHGxgaHDx+Gj48Pzp07BwDw9PREU1MTrK2t8euvv2L//v3w9/fHzJkz0atXLwCAsbGxVHKZO3cuDA0NMXfuXEybNg0nT57Enj171H4Mt27dgr6+vsw7+ABg586dcHR0hLOzM0pLS6W2ZWZmwt/fH1u2bMGiRYvwwQcfYMOGDW32w+3btxEXF4dJkybB3t4e1dXViIyMVDrWyspKiEQiLFy4UGr9wIEDkZSU1IGj10yUhAn5k6GhodSyrq4uBg0aBAAYMmQId5UnEAggEAgwevRoAMCCBQtQWlqK+Ph4aGlpKfVdJiYm3JtB1CkvLw/9+/eXu01LSwsRERHo27cvXF1dpR4SXr58GZaWltyyo6MjHjx4gJs3byrshxs3bkBfXx9isRhisRgREREyV7Vt+eyzz+Dv7y+z3sLCAikpKUq3o+koCRPyp7YSaFvbLC0tYWZmJnN/lQ9CoRBVVVUKt/fu3RvR0dFISUnB1q1bufUGBgZITk7mRlBYW1vD0NBQ5gdF634wNzdHUlISJk2axP2R/CbQniNHjsDFxUVuHQyxWNytakO0h5IwIX9qbm5Gc3Mzt9x6DC1jTGobAO7hVVpaGgwNDWFrawszMzNkZWUBANLT0yEWi7nP6+rqoqamBlVVVcjNzZUZJ6sOtra2KCsrkxobLLlSlbCyssLp06dx8+ZNbp2zszOqqqq4K9C0tDQIhUKMGTNGYT+88847KCgowK5du1BdXY3ExESkpKSAMYaEhASFw/GOHTuGUaNG4bXXXoNYLMaFCxek3pz96NEj2NjYqK9TujhKwoT8KSEhAfHx8cjNzcWFCxdQWFiIM2fOID8/H+fPn0dOTg7i4+O5zwcEBCA6Ohq+vr4IDw+HgYEB3Nzc4OnpiTVr1kBbWxs5OTn46aefAAAzZsyAl5cXkpKS4OnpiX379qn9GIRCIUaMGIH09HQALfeIo6KiEBYWhvz8fO5z9vb2CAoK4pb79euH4OBgeHh44NixYzhw4ABiYmJw8eJFhf3Qq1cv+Pn5wdfXF8OGDcOJEyfg4OCA1NRU2NnZwcfHRya+4OBgrFy5Em+88Qa0tLSgr68PX19fmJubc5/JyMjAkiVL1N43XRbrBhYtWsTOnj3Ldxi8mDNnDvvxxx/5DqNLeuutt9gvv/zSKW0LBAKWlZXFMjMzmVgsltr28OFD1tjYyCoqKlhdXZ3Utvr6esYYY5WVlUwkEqn8vfr6+u1+5tq1a8zf31+p9mpqamTiy8/PVymmyspKVlZWJrUuNzeX7dmzR6V2GGOstraW2drasqdPn7b5ubCwMLZu3TqV2++CiuhKmBAVMcbAGINAIMDw4cOhq6srtV0oFEJbWxvGxsYyD/v09fUBtNS/7qx7yPb29qirq5O68lXk2bd06Ovrq/zmaiMjI5iamnLLIpEIN27cwNKlS1VqB2gZ/nf8+PFOeWjZVfXIJPzgwQMUFBR0Stv379/Hr7/+2iltP6/MzEw0NDSgubkZ1dXVfIcjQ9naDXz2cXNzM8LDw2FhYYHIyEg8ffqUlzja4+Pjg9TUVFRWVr7w725oaICrqyuGDh2q0n5xcXGYM2cOxo8f30mRdU09KgkfP34c7777Lq5fv45vvvkGjo6O3P06dbh37x4WLlyICxcuqK1NdYiJicH777+Pa9eu4aOPPsLkyZNx+/ZtvsPiSB7gKFO7ge8+FggEcHd3R2FhIbZs2SJzpduVODk5wdjY+IV/b58+fSAQqJ5apk+fjlGjRnVCRF1bt3m9UXsCAwNx7tw5/Pzzz9wwG1dXV0ydOhXfffcd3nrrref+jldffVVqwH9XsXXrVsTHx3P/IXfu3MlzRP8TGxuLAQMGYMKECdixY0e7n++qfUxIR/WYK2Fvb29s3bpVapzjSy+9hPnz52PNmjUyw48AoLS0FEVFRSguLgbQMtSnqKhI6le8rKws/PHHH9wQntb3+dqqJSAhb949Ywy3bt1CYWGh1NCdjkpNTcWlS5e45RUrVkjFqWjuf3V1NRISElBaWorc3FyljkleW4pqDzxbT+HZ2g2AbP8C6BLjcQlRlx5zJVxfX4+xY8fKrB83bhyOHDmCBw8eYPjw4VLbEhISMH/+fOzduxcff/wxRCIRVq9ejZ07d2L8+PGYM2cOtm3bhu+//x4lJSWIioqS2j8lJQWzZs1Ceno6+vfvj+3bt+P+/fv45ZdfALTMuzc1NUVtbS1WrlyJS5cuoXfv3ti8eTOWLFmCzZs3w9HRsUMPOFpbtWoV3Nzc8N///hd79+6FlZUVrKysFMZgYmKCkydP4ueff8b8+fOxbNkyPH78GElJSW0ek7y2YmNjsWzZMgQEBOD+/fuIiYnBRx99hE2bNknVU8jIyEBQUBDmzZuHvXv3orGxsd3+bQ9jDNXV1Z1WKIcv3e14OqKuro7vENSmxyRhAHKnc0rK+GVnZ8sk4b/97W9YvHgxdyWsp6eHv/zlL5g4cSL++OMPWFhYYMaMGTAxMcHMmTNl2nZwcJCpJXD//n0A/5t37+vrCwAICQlBZGQkRo0ahfv372Ps2LEICgqSGlDfUaGhoXj11Vfh7e2NqKgo7N69G+vXr1cYw5tvvolPPvkEOTk50NLSwtOnT7nPKDomRW15enriww8/hI2NDby8vHDkyBF8++232LRpk1Q9hQkTJkgda1ZWVrv9256qqiosXbq0W105NzQ0cNOle7K6ujq4urryHYZa9Kgk/PjxY+4KUEIyxXPw4MH4+eefER0dDaDlAUxgYCDWrVuH2bNnw9fXF2fPnuWKjYwcORLBwcEICQlBcXGxwqfkiqa7tp53D7RUsDI3N4e5uTlKSkowZswY7N69G25ubs993Nra2ti4cSMWLlyIjz/+GBs2bIC2tjYAyI3h/Pnz+Otf/8rF/uxUVHnHpOh4AMU1GJ7V+iGXsv3bFmNjY0RHR2P69Okq79tVGRgY4PHjx3yHwbsjR450m/oSPSYJ6+joIDk5WSYJ37t3D6NGjcLIkSNRV1fHjZGUJJrJkydj+PDh+O6775CXl4fFixcDaCmUsnLlSkREREAsFmPv3r0qxdN63r1EUVERDAwMkJCQAH9/f6xcuRJ5eXnYsmXLcxx5y8Ov2bNnY/Dgwfjuu++gr6+P7777DuvWrZMbQ0NDA7Kzs9VyPM9Stj7D8/YvIZqixzyYCwwMhL+/v9QDuNraWoSFheGLL76AQCDAxIkTsWnTJmzatAleXl7c59atW4edO3dK3VM+c+YM9PT0MGjQIDx58gQNDQ2or6+XqT+gqJaAonn3Fy9exO3bt7F9+3aEh4cjLi7uuY89NDRU6sHW8OHD8fbbbyuMwdnZGb///jsSEhIAgHso19YxKWoLaLsGQ+t6Cq37TlH/ArI1HgjRZD0mCX/44YdwdXXF8uXLcfr0aYSHh2PlypU4fPhwu+X3XF1dYW5uzhXoBlrGYN66dQtvv/02bt++DT09Pbz//vtITEzEL7/8wiUpRbUEFM27Z4zhvffew/fff487d+7g448/fu5jf/DgAd59910cPnwYu3btQkFBAdauXaswhuHDh2Px4sWws7PDm2++KfMgSN4x/fbbb3Lbaq8Gg6SewunTp6VqN8jr34CAADx48ECmjwnRaHxNmFYnVWpHlJaWsrfffpsBYCdPnlT6O8rLy2XWiUQirhbAszUCWmurlsCz8+5FIhFrbGxk2dnZrKGhod24lKkdUVNTw5qamlhaWhqrrKyU2S5v7j9jjOXn5zOxWMx++OEHNn78eKWOSVFbbZH04bOU7V9FOrN2BF+UqR3RE3Sn2hE95p6wRL9+/XDhwgUEBQVhx44dGDhwIGbMmNHufiYmJjLrWj91b2vmlFAoBAC5s5eMjIzktmltbd1uTMqS1AdQNBvp2RgkJPfHJa9Gb03RMSlqqy2SegrPUrZ/CdFkPeZ2RGsCgQAbN25EWloaysrKEBUVxcsce01QXl6Oy5cvo66uDteuXeM7HKKC8vJylJSUyPwABVruzUt+uLJWzwteFJFIJFNPOScn54XH0RX0yCQsoaenhwULFuAf//gHL3PsNYGpqSkOHjyIrKwspX5j6CkUFSx/0W0oEh8fj+PHjyM2NhZCoRAuLi5SyTYvLw+rVq3C7NmzkZqa2mlxyBMYGIhx48ZBKBTC1dWVe+Cqo6ODwMBANDQ0vNB4+NajkzAhHREbG6vwjcUvsg1FcnNzERYWBi8vLyxbtgze3t44e/Ys/Pz8uM8MGzYM7u7ucHNz4yYsvQhPnjyBiYkJ7t+/j5SUFFy/fh0nTpwA0HL7y8HBAYGBgS8snq6AkjDp8Wpra3Hz5k3udUWA4hoZz9a7AFpexyMWi/HkyRMUFha2uT8gWzMDaKm5oa7pyNu2bcO8efO4ZR0dHXh7e8PPzw9nz57l1uvr68vMJpTXF4pqf0goqj0iD2MMq1evhpaWFkaMGAE3NzckJiZy221tbXHlyhWkpaUpfbyajpIw6dEiIiJw6NAhZGdnw87ODjExMQBa6n5YWloiOzsb9fX12L59O+bOnStV7yIlJQWhoaEYOnQojh49ivnz58Pa2hqBgYEK9wcg0wYAeHh4YPfu3c99PEVFRYiKioKDg4PUehcXF3h7e2P58uUKr8Dl9cWpU6dgZWWFr776lXeqtAAABMtJREFUCps3b4aTkxMOHDjA7bN//35cunQJV69exdSpUxXOhpQYOHCg1LJIJJIZIjplyhR8/fXXqhy2ZuN3dIZ60OuN6PVG8igzRG306NHc30NCQpi1tTW33KtXL5aens4YYywyMpJNnz6dMcbYgAED2K1btxhjjDU1NTEA7IcffmCMMRYcHMzMzc1ZY2Ojwv2fbYOxliGQ7b3Sh7H2h6jFxsayPn36SK0LDQ1lycnJrLm5mS1YsIBZW1uzkpISduPGDfbNN98wxhjLyMhQ2BcWFhbs3LlzjLGWoWH29vaMMcaSkpLYvHnzWHJyMktOTmavvPIKO3z4cLvHIFFRUcFWrFghsz4oKIi9+eabbe7bnYao0ZUw6dEsLS25vzs6OuLBgwdcIaG2plhLCAQCCAQCrqjOggULUFpaivj4eKX2lzAxMVHLK33y8vLkFqoCWo4nIiICffv2haurq9SDwcuXLyvsC0W1P1rXCxGLxYiIiGh34lNrn332Gfz9/WXWW1hYdJu6EMqgJEx6tOTkZG7UgLW1NQwNDZ8rGVpaWsLMzIy3ym1CoZArSiVP7969ER0djZSUFGzdupVbb2BgoFRftP7B0rpeiOTPs8WeFDly5AhcXFxgYWEhs00sFqNPnz5KtdMdUBImPVpVVRV31ZWWlgahUIgxY8YAUFz3o3W9CwnJg6y0tDQYGhrC1tZW4f7y2sjNzZUZN9sRtra2KCsrkxobLLlSlbCyssLp06elSoc6Ozsr7AtFtT8U1QthjCEhIUHhELxjx45h1KhReO211yAWi3HhwgWplxc8evQINjY2z90XmoKSMOnRgoOD4eHhgWPHjuHAgQOIiYnhrvYU1f2Q1LtISkri2gkICEB0dDR8fX0RHh4OAwMDhfsDkGnD09MT+/bte+7jEQqFGDFiBNLT0wEAt27dQlRUFMLCwqTevmxvb4+goCBuuV+/fnL74uLFiwprfyiqPZKamgo7Ozv4+PjI7e+VK1fijTfegJaWFvT19eHr68uVPQWAjIwMLFmy5Ln7QmPwektaTejBHD2Yk0fZ2hH19fUsPz9f7jZFNTJa17sQCAQsKyuLZWZmMrFYrNT+z7ZRWVnJRCJRu7EqUzvi2rVrzN/fv93PMdZSV+TZmBT1hSLy6oXk5uayPXv2qNQOY4zV1tYyW1vbdh9S0oM5QroRfX19rk7Gs4RCIbS1tWFsbCxVv0JS74IxBsYYBAIBhg8fDl1dXaX2b90G0FJzQ133ke3t7VFXVyd15auIpK5I65gU9YUiRkZGMDU15ZZFIhFu3LjRoddyhYaG4vjx42p5SKkpKAkT0kHNzc0IDw+HhYUFIiMjO/T2j87i4+OD1NRUXmqiNDQ0wNXVFUOHDlVpv7i4OMyZMwfjx4/vpMi6ph5XRY0QdREIBHB3d4e7uzvfocjl5OTEy/d2dGRDd3oNlSroSpgQQnhESZgQQnjUbW5H3L17V27h9e6utLQUycnJ3NuTyf+Ul5cjKSmpU0tGvmjNzc1U1xnghuB1B1qM8VDRWc0+/fTTHvsPs6amBgYGBtDR6TY/T9WmO/ZNRUVFj7zYkMfZ2Rlr167lO4znVdwtkjAhhGioYronTAghPKIkTAghPNIBEMZ3EIQQ0kNV/3+PLL8TINNPegAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70, to_file=\"model_classic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_target, show_shapes=True, dpi=70, to_file=\"model_classic_target.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 4)]               0         \n",
      "                                                                 \n",
      " re-uploading_PQC (ReUploadi  (None, 2)                92        \n",
      " ngPQC)                                                          \n",
      "                                                                 \n",
      " Q-values (Sequential)       (None, 2)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94\n",
      "Trainable params: 94\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Visualize the circuit.\n",
    "# weight_shapes = circuit.weight_shapes\n",
    "# weights = {\n",
    "#     k: np.random.uniform(low=0., high=np.pi, size=s)\n",
    "#     for k, s in weight_shapes.items()\n",
    "# }\n",
    "# # shape_var, shape_enc = circuit.shape\n",
    "# # weights_var = np.random.uniform(low=0., high=np.pi, size=shape_var)\n",
    "# # weights_enc = np.random.uniform(low=0., high=np.pi, size=shape_enc)\n",
    "# print(qml.draw(circuit, wire_order=circuit.wires)(**weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_env(state, model, epsilon, n_actions, env):\n",
    "    # state_shape = state.shape\n",
    "    # state = np.asarray(state).reshape((1, *state_shape)) # Reshape to (1, features) so that batch_size=1.\n",
    "    # state = tf.convert_to_tensor([state]) # Reshape to (1, features) so that batch_size=1.\n",
    "    state = np.asarray(state).reshape((1, 4)) # Reshape to (1, features) so that batch_size=1.\n",
    "    # state = np.asarray(state).reshape((4,)) # Reshape to (1, features) so that batch_size=1.\n",
    "    \n",
    "    # Select action.\n",
    "    coin = np.random.random()\n",
    "    if coin > epsilon: # Epsilon-greedy\n",
    "        q_vals = model([state])\n",
    "        action = int(tf.argmax(q_vals[0]).numpy())\n",
    "    else: # Random action.\n",
    "        action = np.random.choice(n_actions)\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = np.asarray(next_state).reshape(state.shape)\n",
    "    \n",
    "    return dict(\n",
    "        state=state,\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        next_state=next_state,\n",
    "        done=done,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "n_episodes = 2000\n",
    "\n",
    "# Define replay memory\n",
    "max_memory_length = 10000 # Maximum replay length\n",
    "replay_memory = cl.deque(maxlen=max_memory_length)\n",
    "\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
    "decay_epsilon = 0.99 # Decay rate of epsilon greedy parameter\n",
    "batch_size = 2\n",
    "steps_per_update = 10 # Train the model every x steps\n",
    "steps_per_target_update = 30 # Update the target model every x steps\n",
    "\n",
    "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
    "\n",
    "# Assign the model parameters to each optimizer\n",
    "w_in, w_var, w_out = 1, 0, 2\n",
    "\n",
    "# optimizer_w_tups = list(zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]))\n",
    "optimizer_w_tups = zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q-learning update function.\n",
    "\n",
    "# @tf.function\n",
    "def Q_learning_update(\n",
    "    states,\n",
    "    actions, \n",
    "    rewards, \n",
    "    next_states, \n",
    "    done, \n",
    "    model, \n",
    "    model_target, \n",
    "    gamma, \n",
    "    n_actions, \n",
    "    optimizer_w_tups,\n",
    "    ):\n",
    "    \n",
    "    states = tf.convert_to_tensor(states)\n",
    "    actions = tf.convert_to_tensor(actions)\n",
    "    rewards = tf.convert_to_tensor(rewards)\n",
    "    next_states = tf.convert_to_tensor(next_states)\n",
    "    done = tf.convert_to_tensor(done)\n",
    "    \n",
    "    print(f\"{states=}\")\n",
    "\n",
    "    # Compute target Q-values and masks on sampled actions.\n",
    "    # with catchtime() as t:\n",
    "    future_rewards = model_target([next_states])\n",
    "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1) * (1. - done))\n",
    "    # print(f\"{target_q_values=}\")\n",
    "    # print(f\"[target_q_values] Execution time: {t():.4f} secs\")\n",
    "    \n",
    "    masks = tf.one_hot(actions, n_actions)\n",
    "    # print(f\"{actions=}\")\n",
    "    # print(f\"{masks=}\")\n",
    "    \n",
    "    \n",
    "    # with catchtime() as t:\n",
    "    # Train the model using target Q-values.\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        # with catchtime() as t1:\n",
    "        q_values = model([states])\n",
    "        # print(f\"[tape:eval] Execution time: {t1():.4f} secs\")\n",
    "        # print(f\"{q_values=}\")\n",
    "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "        # print(f\"{q_values_masked=}\")\n",
    "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
    "    # print(f\"[tape] Execution time: {t():.4f} secs\")\n",
    "        \n",
    "    # with catchtime() as t:\n",
    "    # Backprop.\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "    # optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "    # optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
    "    # optimizer_w_tups = zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out])\n",
    "    for optimizer, w in optimizer_w_tups:\n",
    "    # for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
    "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])\n",
    "    # print(f\"[backprop] Execution time: {t():.4f} secs\")\n",
    "        \n",
    "    # # print(f\"{loss=}\")\n",
    "    # # print(f\"{grads=}\")\n",
    "    # # print(f\"{masks=}\")\n",
    "    # print(f\"{target_q_values=}\")\n",
    "    # print(f\"{q_values=}\")\n",
    "    # print(f\"{loss=}\")\n",
    "    # print(f\"{grads=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### NEW Q-LEARNING UPDATE\n",
    "# @tf.function\n",
    "def Q_learning_update_new(states, actions, rewards, next_states, done, model, gamma, n_actions, model_target):\n",
    "    states = tf.convert_to_tensor(states)\n",
    "    actions = tf.convert_to_tensor(actions)\n",
    "    rewards = tf.convert_to_tensor(rewards)\n",
    "    next_states = tf.convert_to_tensor(next_states)\n",
    "    done = tf.convert_to_tensor(done)\n",
    "\n",
    "    # Compute their target q_values and the masks on sampled actions\n",
    "    future_rewards = model_target([next_states])\n",
    "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1) * (1.0 - done))\n",
    "    masks = tf.one_hot(actions, n_actions)\n",
    "\n",
    "    # Train the model on the states and target Q-values\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        q_values = model([states])\n",
    "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
    "\n",
    "    # Backpropagation\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
    "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])\n",
    "    \n",
    "    \n",
    "    # print(f\"{masks=}\")\n",
    "    # print(f\"{target_q_values=}\")\n",
    "    # print(f\"{q_values=}\")\n",
    "    # print(f\"{loss=}\")\n",
    "    # print(f\"{grads=}\")\n",
    "###### NEW Q-LEARNING UPDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'thetas:0' shape=(1, 72) dtype=float32, numpy=\n",
      "array([[2.2372582 , 2.5880358 , 3.06306   , 3.085846  , 1.0635768 ,\n",
      "        3.1228194 , 1.1418483 , 3.0050817 , 1.905281  , 2.785892  ,\n",
      "        0.5004826 , 0.6447723 , 1.9543257 , 2.0800443 , 2.447851  ,\n",
      "        2.3334134 , 2.1009912 , 0.3908801 , 2.5953097 , 0.71879137,\n",
      "        0.7986804 , 3.040161  , 1.6241587 , 0.07939886, 2.0816183 ,\n",
      "        0.84860075, 0.03480743, 0.69438773, 1.1510717 , 2.4947326 ,\n",
      "        0.08011679, 1.1260244 , 1.8323342 , 2.179437  , 1.9240333 ,\n",
      "        0.7027988 , 1.6664402 , 3.0567665 , 0.784517  , 2.8431237 ,\n",
      "        0.28822324, 0.69219875, 1.0673001 , 0.8905047 , 2.25244   ,\n",
      "        0.18145877, 1.030313  , 3.0538795 , 2.6694224 , 2.385821  ,\n",
      "        1.135508  , 2.664714  , 0.28354713, 2.111842  , 0.9070527 ,\n",
      "        2.223006  , 1.1406634 , 2.441232  , 2.972549  , 0.73719424,\n",
      "        0.07695109, 0.9957925 , 2.2279787 , 1.7951983 , 0.353165  ,\n",
      "        1.5548764 , 1.2224962 , 0.70328975, 0.6142481 , 1.939367  ,\n",
      "        0.27050233, 1.5495498 ]], dtype=float32)>\n",
      "<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32)>\n",
      "<tf.Variable 'obs-weights:0' shape=(1, 2) dtype=float32, numpy=array([[1., 1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "original_trainable_variables = model.trainable_variables # Preserve\n",
    "for v in original_trainable_variables: print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 20])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random(size=(16, 4,))\n",
    "tf.tile(x, multiples=[1, 5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03672619,  0.00953293, -0.01919124, -0.02750128],\n",
      "       [ 0.03263681,  0.20446923, -0.01286928, -0.31609806]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03691685,  0.20492478, -0.01974127, -0.32617694],\n",
      "       [ 0.03672619,  0.00953293, -0.01919124, -0.02750128]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.03691685,  0.20492478, -0.01974127, -0.32617694,  0.03691685,\n",
      "         0.20492478, -0.01974127, -0.32617694,  0.03691685,  0.20492478,\n",
      "        -0.01974127, -0.32617694,  0.03691685,  0.20492478, -0.01974127,\n",
      "        -0.32617694,  0.03691685,  0.20492478, -0.01974127, -0.32617694],\n",
      "       [ 0.03672619,  0.00953293, -0.01919124, -0.02750128,  0.03672619,\n",
      "         0.00953293, -0.01919124, -0.02750128,  0.03672619,  0.00953293,\n",
      "        -0.01919124, -0.02750128,  0.03672619,  0.00953293, -0.01919124,\n",
      "        -0.02750128,  0.03672619,  0.00953293, -0.01919124, -0.02750128]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03672619,  0.00953293, -0.01919124, -0.02750128],\n",
      "       [ 0.03263681,  0.20446923, -0.01286928, -0.31609806]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.03672619,  0.00953293, -0.01919124, -0.02750128,  0.03672619,\n",
      "         0.00953293, -0.01919124, -0.02750128,  0.03672619,  0.00953293,\n",
      "        -0.01919124, -0.02750128,  0.03672619,  0.00953293, -0.01919124,\n",
      "        -0.02750128,  0.03672619,  0.00953293, -0.01919124, -0.02750128],\n",
      "       [ 0.03263681,  0.20446923, -0.01286928, -0.31609806,  0.03263681,\n",
      "         0.20446923, -0.01286928, -0.31609806,  0.03263681,  0.20446923,\n",
      "        -0.01286928, -0.31609806,  0.03263681,  0.20446923, -0.01286928,\n",
      "        -0.31609806,  0.03263681,  0.20446923, -0.01286928, -0.31609806]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03691685,  0.20492478, -0.01974127, -0.32617694],\n",
      "       [ 0.10096238,  0.01624257, -0.12344771, -0.17675042]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.04101535,  0.40032214, -0.02626481, -0.62501943],\n",
      "       [ 0.10128722,  0.2128951 , -0.12698272, -0.5056859 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.04101535,  0.40032214, -0.02626481, -0.62501943,  0.04101535,\n",
      "         0.40032214, -0.02626481, -0.62501943,  0.04101535,  0.40032214,\n",
      "        -0.02626481, -0.62501943,  0.04101535,  0.40032214, -0.02626481,\n",
      "        -0.62501943,  0.04101535,  0.40032214, -0.02626481, -0.62501943],\n",
      "       [ 0.10128722,  0.2128951 , -0.12698272, -0.5056859 ,  0.10128722,\n",
      "         0.2128951 , -0.12698272, -0.5056859 ,  0.10128722,  0.2128951 ,\n",
      "        -0.12698272, -0.5056859 ,  0.10128722,  0.2128951 , -0.12698272,\n",
      "        -0.5056859 ,  0.10128722,  0.2128951 , -0.12698272, -0.5056859 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03691685,  0.20492478, -0.01974127, -0.32617694],\n",
      "       [ 0.10096238,  0.01624257, -0.12344771, -0.17675042]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.03691685,  0.20492478, -0.01974127, -0.32617694,  0.03691685,\n",
      "         0.20492478, -0.01974127, -0.32617694,  0.03691685,  0.20492478,\n",
      "        -0.01974127, -0.32617694,  0.03691685,  0.20492478, -0.01974127,\n",
      "        -0.32617694,  0.03691685,  0.20492478, -0.01974127, -0.32617694],\n",
      "       [ 0.10096238,  0.01624257, -0.12344771, -0.17675042,  0.10096238,\n",
      "         0.01624257, -0.12344771, -0.17675042,  0.10096238,  0.01624257,\n",
      "        -0.12344771, -0.17675042,  0.10096238,  0.01624257, -0.12344771,\n",
      "        -0.17675042,  0.10096238,  0.01624257, -0.12344771, -0.17675042]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03722323, -0.7403858 , -0.11757976,  0.47266403],\n",
      "       [ 0.08453389, -0.16523424, -0.13668841, -0.18540974]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.02241552, -0.54381657, -0.10812648,  0.14535719],\n",
      "       [ 0.0812292 , -0.35816267, -0.14039661,  0.06122226]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.02241552, -0.54381657, -0.10812648,  0.14535719,  0.02241552,\n",
      "        -0.54381657, -0.10812648,  0.14535719,  0.02241552, -0.54381657,\n",
      "        -0.10812648,  0.14535719,  0.02241552, -0.54381657, -0.10812648,\n",
      "         0.14535719,  0.02241552, -0.54381657, -0.10812648,  0.14535719],\n",
      "       [ 0.0812292 , -0.35816267, -0.14039661,  0.06122226,  0.0812292 ,\n",
      "        -0.35816267, -0.14039661,  0.06122226,  0.0812292 , -0.35816267,\n",
      "        -0.14039661,  0.06122226,  0.0812292 , -0.35816267, -0.14039661,\n",
      "         0.06122226,  0.0812292 , -0.35816267, -0.14039661,  0.06122226]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03722323, -0.7403858 , -0.11757976,  0.47266403],\n",
      "       [ 0.08453389, -0.16523424, -0.13668841, -0.18540974]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.03722323, -0.7403858 , -0.11757976,  0.47266403,  0.03722323,\n",
      "        -0.7403858 , -0.11757976,  0.47266403,  0.03722323, -0.7403858 ,\n",
      "        -0.11757976,  0.47266403,  0.03722323, -0.7403858 , -0.11757976,\n",
      "         0.47266403,  0.03722323, -0.7403858 , -0.11757976,  0.47266403],\n",
      "       [ 0.08453389, -0.16523424, -0.13668841, -0.18540974,  0.08453389,\n",
      "        -0.16523424, -0.13668841, -0.18540974,  0.08453389, -0.16523424,\n",
      "        -0.13668841, -0.18540974,  0.08453389, -0.16523424, -0.13668841,\n",
      "        -0.18540974,  0.08453389, -0.16523424, -0.13668841, -0.18540974]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03532046, -0.33324277, -0.16598035, -0.49550858],\n",
      "       [ 0.0917749 , -0.36205098, -0.1396457 ,  0.14786401]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.04198532, -0.525683  , -0.17589054, -0.25938982],\n",
      "       [ 0.08453389, -0.16523424, -0.13668841, -0.18540974]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.04198532, -0.525683  , -0.17589054, -0.25938982, -0.04198532,\n",
      "        -0.525683  , -0.17589054, -0.25938982, -0.04198532, -0.525683  ,\n",
      "        -0.17589054, -0.25938982, -0.04198532, -0.525683  , -0.17589054,\n",
      "        -0.25938982, -0.04198532, -0.525683  , -0.17589054, -0.25938982],\n",
      "       [ 0.08453389, -0.16523424, -0.13668841, -0.18540974,  0.08453389,\n",
      "        -0.16523424, -0.13668841, -0.18540974,  0.08453389, -0.16523424,\n",
      "        -0.13668841, -0.18540974,  0.08453389, -0.16523424, -0.13668841,\n",
      "        -0.18540974,  0.08453389, -0.16523424, -0.13668841, -0.18540974]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03532046, -0.33324277, -0.16598035, -0.49550858],\n",
      "       [ 0.0917749 , -0.36205098, -0.1396457 ,  0.14786401]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03532046, -0.33324277, -0.16598035, -0.49550858, -0.03532046,\n",
      "        -0.33324277, -0.16598035, -0.49550858, -0.03532046, -0.33324277,\n",
      "        -0.16598035, -0.49550858, -0.03532046, -0.33324277, -0.16598035,\n",
      "        -0.49550858, -0.03532046, -0.33324277, -0.16598035, -0.49550858],\n",
      "       [ 0.0917749 , -0.36205098, -0.1396457 ,  0.14786401,  0.0917749 ,\n",
      "        -0.36205098, -0.1396457 ,  0.14786401,  0.0917749 , -0.36205098,\n",
      "        -0.1396457 ,  0.14786401,  0.0917749 , -0.36205098, -0.1396457 ,\n",
      "         0.14786401,  0.0917749 , -0.36205098, -0.1396457 ,  0.14786401]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.04058871,  0.04801724, -0.01572341, -0.03296316],\n",
      "       [ 0.06885947,  0.2450221 , -0.0553556 , -0.36766863]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.04154906,  0.2433611 , -0.01638267, -0.3305652 ],\n",
      "       [ 0.07375991,  0.44088516, -0.06270897, -0.6772794 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.04154906,  0.2433611 , -0.01638267, -0.3305652 ,  0.04154906,\n",
      "         0.2433611 , -0.01638267, -0.3305652 ,  0.04154906,  0.2433611 ,\n",
      "        -0.01638267, -0.3305652 ,  0.04154906,  0.2433611 , -0.01638267,\n",
      "        -0.3305652 ,  0.04154906,  0.2433611 , -0.01638267, -0.3305652 ],\n",
      "       [ 0.07375991,  0.44088516, -0.06270897, -0.6772794 ,  0.07375991,\n",
      "         0.44088516, -0.06270897, -0.6772794 ,  0.07375991,  0.44088516,\n",
      "        -0.06270897, -0.6772794 ,  0.07375991,  0.44088516, -0.06270897,\n",
      "        -0.6772794 ,  0.07375991,  0.44088516, -0.06270897, -0.6772794 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.04058871,  0.04801724, -0.01572341, -0.03296316],\n",
      "       [ 0.06885947,  0.2450221 , -0.0553556 , -0.36766863]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.04058871,  0.04801724, -0.01572341, -0.03296316,  0.04058871,\n",
      "         0.04801724, -0.01572341, -0.03296316,  0.04058871,  0.04801724,\n",
      "        -0.01572341, -0.03296316,  0.04058871,  0.04801724, -0.01572341,\n",
      "        -0.03296316,  0.04058871,  0.04801724, -0.01572341, -0.03296316],\n",
      "       [ 0.06885947,  0.2450221 , -0.0553556 , -0.36766863,  0.06885947,\n",
      "         0.2450221 , -0.0553556 , -0.36766863,  0.06885947,  0.2450221 ,\n",
      "        -0.0553556 , -0.36766863,  0.06885947,  0.2450221 , -0.0553556 ,\n",
      "        -0.36766863,  0.06885947,  0.2450221 , -0.0553556 , -0.36766863]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.01485835, -0.43928373,  0.00817567,  0.56781924],\n",
      "       [ 0.01485835, -0.43928373,  0.00817567,  0.56781924]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.00607267, -0.6345194 ,  0.01953206,  0.86306655],\n",
      "       [ 0.00607267, -0.6345194 ,  0.01953206,  0.86306655]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.00607267, -0.6345194 ,  0.01953206,  0.86306655,  0.00607267,\n",
      "        -0.6345194 ,  0.01953206,  0.86306655,  0.00607267, -0.6345194 ,\n",
      "         0.01953206,  0.86306655,  0.00607267, -0.6345194 ,  0.01953206,\n",
      "         0.86306655,  0.00607267, -0.6345194 ,  0.01953206,  0.86306655],\n",
      "       [ 0.00607267, -0.6345194 ,  0.01953206,  0.86306655,  0.00607267,\n",
      "        -0.6345194 ,  0.01953206,  0.86306655,  0.00607267, -0.6345194 ,\n",
      "         0.01953206,  0.86306655,  0.00607267, -0.6345194 ,  0.01953206,\n",
      "         0.86306655,  0.00607267, -0.6345194 ,  0.01953206,  0.86306655]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.01485835, -0.43928373,  0.00817567,  0.56781924],\n",
      "       [ 0.01485835, -0.43928373,  0.00817567,  0.56781924]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.01485835, -0.43928373,  0.00817567,  0.56781924,  0.01485835,\n",
      "        -0.43928373,  0.00817567,  0.56781924,  0.01485835, -0.43928373,\n",
      "         0.00817567,  0.56781924,  0.01485835, -0.43928373,  0.00817567,\n",
      "         0.56781924,  0.01485835, -0.43928373,  0.00817567,  0.56781924],\n",
      "       [ 0.01485835, -0.43928373,  0.00817567,  0.56781924,  0.01485835,\n",
      "        -0.43928373,  0.00817567,  0.56781924,  0.01485835, -0.43928373,\n",
      "         0.00817567,  0.56781924,  0.01485835, -0.43928373,  0.00817567,\n",
      "         0.56781924,  0.01485835, -0.43928373,  0.00817567,  0.56781924]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.04641628,  0.4387124 , -0.02299397, -0.6283691 ],\n",
      "       [ 0.07406595, -0.55102164, -0.13917215,  0.30652317]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.05519053,  0.24391876, -0.03556136, -0.34301555],\n",
      "       [ 0.06304552, -0.7439144 , -0.1330417 ,  0.5522801 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.05519053,  0.24391876, -0.03556136, -0.34301555,  0.05519053,\n",
      "         0.24391876, -0.03556136, -0.34301555,  0.05519053,  0.24391876,\n",
      "        -0.03556136, -0.34301555,  0.05519053,  0.24391876, -0.03556136,\n",
      "        -0.34301555,  0.05519053,  0.24391876, -0.03556136, -0.34301555],\n",
      "       [ 0.06304552, -0.7439144 , -0.1330417 ,  0.5522801 ,  0.06304552,\n",
      "        -0.7439144 , -0.1330417 ,  0.5522801 ,  0.06304552, -0.7439144 ,\n",
      "        -0.1330417 ,  0.5522801 ,  0.06304552, -0.7439144 , -0.1330417 ,\n",
      "         0.5522801 ,  0.06304552, -0.7439144 , -0.1330417 ,  0.5522801 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.04641628,  0.4387124 , -0.02299397, -0.6283691 ],\n",
      "       [ 0.07406595, -0.55102164, -0.13917215,  0.30652317]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.04641628,  0.4387124 , -0.02299397, -0.6283691 ,  0.04641628,\n",
      "         0.4387124 , -0.02299397, -0.6283691 ,  0.04641628,  0.4387124 ,\n",
      "        -0.02299397, -0.6283691 ,  0.04641628,  0.4387124 , -0.02299397,\n",
      "        -0.6283691 ,  0.04641628,  0.4387124 , -0.02299397, -0.6283691 ],\n",
      "       [ 0.07406595, -0.55102164, -0.13917215,  0.30652317,  0.07406595,\n",
      "        -0.55102164, -0.13917215,  0.30652317,  0.07406595, -0.55102164,\n",
      "        -0.13917215,  0.30652317,  0.07406595, -0.55102164, -0.13917215,\n",
      "         0.30652317,  0.07406595, -0.55102164, -0.13917215,  0.30652317]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.01629397, -0.14766946, -0.05125821,  0.20930648],\n",
      "       [ 0.02463736, -0.04889527,  0.00460879, -0.02073713]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.01334058, -0.34202245, -0.04707208,  0.48538974],\n",
      "       [ 0.02365946,  0.14616029,  0.00419405, -0.31196237]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.01334058, -0.34202245, -0.04707208,  0.48538974,  0.01334058,\n",
      "        -0.34202245, -0.04707208,  0.48538974,  0.01334058, -0.34202245,\n",
      "        -0.04707208,  0.48538974,  0.01334058, -0.34202245, -0.04707208,\n",
      "         0.48538974,  0.01334058, -0.34202245, -0.04707208,  0.48538974],\n",
      "       [ 0.02365946,  0.14616029,  0.00419405, -0.31196237,  0.02365946,\n",
      "         0.14616029,  0.00419405, -0.31196237,  0.02365946,  0.14616029,\n",
      "         0.00419405, -0.31196237,  0.02365946,  0.14616029,  0.00419405,\n",
      "        -0.31196237,  0.02365946,  0.14616029,  0.00419405, -0.31196237]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.01629397, -0.14766946, -0.05125821,  0.20930648],\n",
      "       [ 0.02463736, -0.04889527,  0.00460879, -0.02073713]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.01629397, -0.14766946, -0.05125821,  0.20930648,  0.01629397,\n",
      "        -0.14766946, -0.05125821,  0.20930648,  0.01629397, -0.14766946,\n",
      "        -0.05125821,  0.20930648,  0.01629397, -0.14766946, -0.05125821,\n",
      "         0.20930648,  0.01629397, -0.14766946, -0.05125821,  0.20930648],\n",
      "       [ 0.02463736, -0.04889527,  0.00460879, -0.02073713,  0.02463736,\n",
      "        -0.04889527,  0.00460879, -0.02073713,  0.02463736, -0.04889527,\n",
      "         0.00460879, -0.02073713,  0.02463736, -0.04889527,  0.00460879,\n",
      "        -0.02073713,  0.02463736, -0.04889527,  0.00460879, -0.02073713]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.03818585, -0.1952429 ,  0.03412551,  0.34452286]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.03818585, -0.1952429 ,  0.03412551,  0.34452286, -0.03818585,\n",
      "        -0.1952429 ,  0.03412551,  0.34452286, -0.03818585, -0.1952429 ,\n",
      "         0.03412551,  0.34452286, -0.03818585, -0.1952429 ,  0.03412551,\n",
      "         0.34452286, -0.03818585, -0.1952429 ,  0.03412551,  0.34452286]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.07676628,  0.597084  , -0.08189166, -0.9562656 ],\n",
      "       [ 0.01053616,  0.24118753, -0.04299297, -0.3460417 ]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.08870797,  0.40315318, -0.10101697, -0.69039357],\n",
      "       [ 0.01535991,  0.04670266, -0.0499138 , -0.06722002]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.08870797,  0.40315318, -0.10101697, -0.69039357,  0.08870797,\n",
      "         0.40315318, -0.10101697, -0.69039357,  0.08870797,  0.40315318,\n",
      "        -0.10101697, -0.69039357,  0.08870797,  0.40315318, -0.10101697,\n",
      "        -0.69039357,  0.08870797,  0.40315318, -0.10101697, -0.69039357],\n",
      "       [ 0.01535991,  0.04670266, -0.0499138 , -0.06722002,  0.01535991,\n",
      "         0.04670266, -0.0499138 , -0.06722002,  0.01535991,  0.04670266,\n",
      "        -0.0499138 , -0.06722002,  0.01535991,  0.04670266, -0.0499138 ,\n",
      "        -0.06722002,  0.01535991,  0.04670266, -0.0499138 , -0.06722002]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.07676628,  0.597084  , -0.08189166, -0.9562656 ],\n",
      "       [ 0.01053616,  0.24118753, -0.04299297, -0.3460417 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.07676628,  0.597084  , -0.08189166, -0.9562656 ,  0.07676628,\n",
      "         0.597084  , -0.08189166, -0.9562656 ,  0.07676628,  0.597084  ,\n",
      "        -0.08189166, -0.9562656 ,  0.07676628,  0.597084  , -0.08189166,\n",
      "        -0.9562656 ,  0.07676628,  0.597084  , -0.08189166, -0.9562656 ],\n",
      "       [ 0.01053616,  0.24118753, -0.04299297, -0.3460417 ,  0.01053616,\n",
      "         0.24118753, -0.04299297, -0.3460417 ,  0.01053616,  0.24118753,\n",
      "        -0.04299297, -0.3460417 ,  0.01053616,  0.24118753, -0.04299297,\n",
      "        -0.3460417 ,  0.01053616,  0.24118753, -0.04299297, -0.3460417 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.09684921, -0.7844597 ,  0.12981439,  1.3163679 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.09684921, -0.7844597 ,  0.12981439,  1.3163679 , -0.09684921,\n",
      "        -0.7844597 ,  0.12981439,  1.3163679 , -0.09684921, -0.7844597 ,\n",
      "         0.12981439,  1.3163679 , -0.09684921, -0.7844597 ,  0.12981439,\n",
      "         1.3163679 , -0.09684921, -0.7844597 ,  0.12981439,  1.3163679 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03251052, -0.1404972 , -0.15123974, -0.7370315 ],\n",
      "       [-0.03406056, -0.44253477,  0.07286694,  0.6409595 ]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03532046, -0.33324277, -0.16598035, -0.49550858],\n",
      "       [-0.04291126, -0.63859284,  0.08568613,  0.9556702 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03532046, -0.33324277, -0.16598035, -0.49550858, -0.03532046,\n",
      "        -0.33324277, -0.16598035, -0.49550858, -0.03532046, -0.33324277,\n",
      "        -0.16598035, -0.49550858, -0.03532046, -0.33324277, -0.16598035,\n",
      "        -0.49550858, -0.03532046, -0.33324277, -0.16598035, -0.49550858],\n",
      "       [-0.04291126, -0.63859284,  0.08568613,  0.9556702 , -0.04291126,\n",
      "        -0.63859284,  0.08568613,  0.9556702 , -0.04291126, -0.63859284,\n",
      "         0.08568613,  0.9556702 , -0.04291126, -0.63859284,  0.08568613,\n",
      "         0.9556702 , -0.04291126, -0.63859284,  0.08568613,  0.9556702 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03251052, -0.1404972 , -0.15123974, -0.7370315 ],\n",
      "       [-0.03406056, -0.44253477,  0.07286694,  0.6409595 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03251052, -0.1404972 , -0.15123974, -0.7370315 , -0.03251052,\n",
      "        -0.1404972 , -0.15123974, -0.7370315 , -0.03251052, -0.1404972 ,\n",
      "        -0.15123974, -0.7370315 , -0.03251052, -0.1404972 , -0.15123974,\n",
      "        -0.7370315 , -0.03251052, -0.1404972 , -0.15123974, -0.7370315 ],\n",
      "       [-0.03406056, -0.44253477,  0.07286694,  0.6409595 , -0.03406056,\n",
      "        -0.44253477,  0.07286694,  0.6409595 , -0.03406056, -0.44253477,\n",
      "         0.07286694,  0.6409595 , -0.03406056, -0.44253477,  0.07286694,\n",
      "         0.6409595 , -0.03406056, -0.44253477,  0.07286694,  0.6409595 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.07406595, -0.55102164, -0.13917215,  0.30652317],\n",
      "       [ 0.10128722,  0.2128951 , -0.12698272, -0.5056859 ]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.06304552, -0.7439144 , -0.1330417 ,  0.5522801 ],\n",
      "       [ 0.10554513,  0.01976979, -0.13709643, -0.25556305]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.06304552, -0.7439144 , -0.1330417 ,  0.5522801 ,  0.06304552,\n",
      "        -0.7439144 , -0.1330417 ,  0.5522801 ,  0.06304552, -0.7439144 ,\n",
      "        -0.1330417 ,  0.5522801 ,  0.06304552, -0.7439144 , -0.1330417 ,\n",
      "         0.5522801 ,  0.06304552, -0.7439144 , -0.1330417 ,  0.5522801 ],\n",
      "       [ 0.10554513,  0.01976979, -0.13709643, -0.25556305,  0.10554513,\n",
      "         0.01976979, -0.13709643, -0.25556305,  0.10554513,  0.01976979,\n",
      "        -0.13709643, -0.25556305,  0.10554513,  0.01976979, -0.13709643,\n",
      "        -0.25556305,  0.10554513,  0.01976979, -0.13709643, -0.25556305]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.07406595, -0.55102164, -0.13917215,  0.30652317],\n",
      "       [ 0.10128722,  0.2128951 , -0.12698272, -0.5056859 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.07406595, -0.55102164, -0.13917215,  0.30652317,  0.07406595,\n",
      "        -0.55102164, -0.13917215,  0.30652317,  0.07406595, -0.55102164,\n",
      "        -0.13917215,  0.30652317,  0.07406595, -0.55102164, -0.13917215,\n",
      "         0.30652317,  0.07406595, -0.55102164, -0.13917215,  0.30652317],\n",
      "       [ 0.10128722,  0.2128951 , -0.12698272, -0.5056859 ,  0.10128722,\n",
      "         0.2128951 , -0.12698272, -0.5056859 ,  0.10128722,  0.2128951 ,\n",
      "        -0.12698272, -0.5056859 ,  0.10128722,  0.2128951 , -0.12698272,\n",
      "        -0.5056859 ,  0.10128722,  0.2128951 , -0.12698272, -0.5056859 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-1.1485017e-01, -8.4172434e-01,  1.9961546e-01,  1.4471519e+00],\n",
      "       [-3.8192656e-02,  3.4029011e-04,  3.3295039e-02,  4.1523747e-02]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.13168466, -1.0386618 ,  0.22855851,  1.7949903 ],\n",
      "       [-0.03818585, -0.1952429 ,  0.03412551,  0.34452286]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.13168466, -1.0386618 ,  0.22855851,  1.7949903 , -0.13168466,\n",
      "        -1.0386618 ,  0.22855851,  1.7949903 , -0.13168466, -1.0386618 ,\n",
      "         0.22855851,  1.7949903 , -0.13168466, -1.0386618 ,  0.22855851,\n",
      "         1.7949903 , -0.13168466, -1.0386618 ,  0.22855851,  1.7949903 ],\n",
      "       [-0.03818585, -0.1952429 ,  0.03412551,  0.34452286, -0.03818585,\n",
      "        -0.1952429 ,  0.03412551,  0.34452286, -0.03818585, -0.1952429 ,\n",
      "         0.03412551,  0.34452286, -0.03818585, -0.1952429 ,  0.03412551,\n",
      "         0.34452286, -0.03818585, -0.1952429 ,  0.03412551,  0.34452286]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-1.1485017e-01, -8.4172434e-01,  1.9961546e-01,  1.4471519e+00],\n",
      "       [-3.8192656e-02,  3.4029011e-04,  3.3295039e-02,  4.1523747e-02]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-1.1485017e-01, -8.4172434e-01,  1.9961546e-01,  1.4471519e+00,\n",
      "        -1.1485017e-01, -8.4172434e-01,  1.9961546e-01,  1.4471519e+00,\n",
      "        -1.1485017e-01, -8.4172434e-01,  1.9961546e-01,  1.4471519e+00,\n",
      "        -1.1485017e-01, -8.4172434e-01,  1.9961546e-01,  1.4471519e+00,\n",
      "        -1.1485017e-01, -8.4172434e-01,  1.9961546e-01,  1.4471519e+00],\n",
      "       [-3.8192656e-02,  3.4029011e-04,  3.3295039e-02,  4.1523747e-02,\n",
      "        -3.8192656e-02,  3.4029011e-04,  3.3295039e-02,  4.1523747e-02,\n",
      "        -3.8192656e-02,  3.4029011e-04,  3.3295039e-02,  4.1523747e-02,\n",
      "        -3.8192656e-02,  3.4029011e-04,  3.3295039e-02,  4.1523747e-02,\n",
      "        -3.8192656e-02,  3.4029011e-04,  3.3295039e-02,  4.1523747e-02]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05026639, -0.18041725,  0.056891  ,  0.33831754],\n",
      "       [ 0.04058871,  0.04801724, -0.01572341, -0.03296316]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05387473, -0.37630066,  0.06365735,  0.6483842 ],\n",
      "       [ 0.04154906,  0.2433611 , -0.01638267, -0.3305652 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.05387473, -0.37630066,  0.06365735,  0.6483842 , -0.05387473,\n",
      "        -0.37630066,  0.06365735,  0.6483842 , -0.05387473, -0.37630066,\n",
      "         0.06365735,  0.6483842 , -0.05387473, -0.37630066,  0.06365735,\n",
      "         0.6483842 , -0.05387473, -0.37630066,  0.06365735,  0.6483842 ],\n",
      "       [ 0.04154906,  0.2433611 , -0.01638267, -0.3305652 ,  0.04154906,\n",
      "         0.2433611 , -0.01638267, -0.3305652 ,  0.04154906,  0.2433611 ,\n",
      "        -0.01638267, -0.3305652 ,  0.04154906,  0.2433611 , -0.01638267,\n",
      "        -0.3305652 ,  0.04154906,  0.2433611 , -0.01638267, -0.3305652 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05026639, -0.18041725,  0.056891  ,  0.33831754],\n",
      "       [ 0.04058871,  0.04801724, -0.01572341, -0.03296316]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.05026639, -0.18041725,  0.056891  ,  0.33831754, -0.05026639,\n",
      "        -0.18041725,  0.056891  ,  0.33831754, -0.05026639, -0.18041725,\n",
      "         0.056891  ,  0.33831754, -0.05026639, -0.18041725,  0.056891  ,\n",
      "         0.33831754, -0.05026639, -0.18041725,  0.056891  ,  0.33831754],\n",
      "       [ 0.04058871,  0.04801724, -0.01572341, -0.03296316,  0.04058871,\n",
      "         0.04801724, -0.01572341, -0.03296316,  0.04058871,  0.04801724,\n",
      "        -0.01572341, -0.03296316,  0.04058871,  0.04801724, -0.01572341,\n",
      "        -0.03296316,  0.04058871,  0.04801724, -0.01572341, -0.03296316]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.01668782,  0.03163327,  0.01967609,  0.04981611]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.01668782,  0.03163327,  0.01967609,  0.04981611, -0.01668782,\n",
      "         0.03163327,  0.01967609,  0.04981611, -0.01668782,  0.03163327,\n",
      "         0.01967609,  0.04981611, -0.01668782,  0.03163327,  0.01967609,\n",
      "         0.04981611, -0.01668782,  0.03163327,  0.01967609,  0.04981611]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.02651395, -0.554671  ,  0.04060065,  0.9490294 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.02651395, -0.554671  ,  0.04060065,  0.9490294 , -0.02651395,\n",
      "        -0.554671  ,  0.04060065,  0.9490294 , -0.02651395, -0.554671  ,\n",
      "         0.04060065,  0.9490294 , -0.02651395, -0.554671  ,  0.04060065,\n",
      "         0.9490294 , -0.02651395, -0.554671  ,  0.04060065,  0.9490294 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.09724616, -0.9515575 ,  0.16672137,  1.6996827 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.09724616, -0.9515575 ,  0.16672137,  1.6996827 , -0.09724616,\n",
      "        -0.9515575 ,  0.16672137,  1.6996827 , -0.09724616, -0.9515575 ,\n",
      "         0.16672137,  1.6996827 , -0.09724616, -0.9515575 ,  0.16672137,\n",
      "         1.6996827 , -0.09724616, -0.9515575 ,  0.16672137,  1.6996827 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03722323, -0.7403858 , -0.11757976,  0.47266403],\n",
      "       [-0.07830913, -1.1219513 ,  0.08451675,  1.6470393 ]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.02241552, -0.54381657, -0.10812648,  0.14535719],\n",
      "       [-0.10074815, -1.317954  ,  0.11745754,  1.9648123 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.02241552, -0.54381657, -0.10812648,  0.14535719,  0.02241552,\n",
      "        -0.54381657, -0.10812648,  0.14535719,  0.02241552, -0.54381657,\n",
      "        -0.10812648,  0.14535719,  0.02241552, -0.54381657, -0.10812648,\n",
      "         0.14535719,  0.02241552, -0.54381657, -0.10812648,  0.14535719],\n",
      "       [-0.10074815, -1.317954  ,  0.11745754,  1.9648123 , -0.10074815,\n",
      "        -1.317954  ,  0.11745754,  1.9648123 , -0.10074815, -1.317954  ,\n",
      "         0.11745754,  1.9648123 , -0.10074815, -1.317954  ,  0.11745754,\n",
      "         1.9648123 , -0.10074815, -1.317954  ,  0.11745754,  1.9648123 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03722323, -0.7403858 , -0.11757976,  0.47266403],\n",
      "       [-0.07830913, -1.1219513 ,  0.08451675,  1.6470393 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.03722323, -0.7403858 , -0.11757976,  0.47266403,  0.03722323,\n",
      "        -0.7403858 , -0.11757976,  0.47266403,  0.03722323, -0.7403858 ,\n",
      "        -0.11757976,  0.47266403,  0.03722323, -0.7403858 , -0.11757976,\n",
      "         0.47266403,  0.03722323, -0.7403858 , -0.11757976,  0.47266403],\n",
      "       [-0.07830913, -1.1219513 ,  0.08451675,  1.6470393 , -0.07830913,\n",
      "        -1.1219513 ,  0.08451675,  1.6470393 , -0.07830913, -1.1219513 ,\n",
      "         0.08451675,  1.6470393 , -0.07830913, -1.1219513 ,  0.08451675,\n",
      "         1.6470393 , -0.07830913, -1.1219513 ,  0.08451675,  1.6470393 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.08041044, -0.18456493,  0.1096884 ,  0.43171245],\n",
      "       [ 0.01485835, -0.43928373,  0.00817567,  0.56781924]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.08410174,  0.00884684,  0.11832265,  0.17552212],\n",
      "       [ 0.00607267, -0.6345194 ,  0.01953206,  0.86306655]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.08410174,  0.00884684,  0.11832265,  0.17552212, -0.08410174,\n",
      "         0.00884684,  0.11832265,  0.17552212, -0.08410174,  0.00884684,\n",
      "         0.11832265,  0.17552212, -0.08410174,  0.00884684,  0.11832265,\n",
      "         0.17552212, -0.08410174,  0.00884684,  0.11832265,  0.17552212],\n",
      "       [ 0.00607267, -0.6345194 ,  0.01953206,  0.86306655,  0.00607267,\n",
      "        -0.6345194 ,  0.01953206,  0.86306655,  0.00607267, -0.6345194 ,\n",
      "         0.01953206,  0.86306655,  0.00607267, -0.6345194 ,  0.01953206,\n",
      "         0.86306655,  0.00607267, -0.6345194 ,  0.01953206,  0.86306655]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.08041044, -0.18456493,  0.1096884 ,  0.43171245],\n",
      "       [ 0.01485835, -0.43928373,  0.00817567,  0.56781924]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.08041044, -0.18456493,  0.1096884 ,  0.43171245, -0.08041044,\n",
      "        -0.18456493,  0.1096884 ,  0.43171245, -0.08041044, -0.18456493,\n",
      "         0.1096884 ,  0.43171245, -0.08041044, -0.18456493,  0.1096884 ,\n",
      "         0.43171245, -0.08041044, -0.18456493,  0.1096884 ,  0.43171245],\n",
      "       [ 0.01485835, -0.43928373,  0.00817567,  0.56781924,  0.01485835,\n",
      "        -0.43928373,  0.00817567,  0.56781924,  0.01485835, -0.43928373,\n",
      "         0.00817567,  0.56781924,  0.01485835, -0.43928373,  0.00817567,\n",
      "         0.56781924,  0.01485835, -0.43928373,  0.00817567,  0.56781924]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.11770944, -0.9670766 ,  0.1072313 ,  1.4646606 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.11770944, -0.9670766 ,  0.1072313 ,  1.4646606 , -0.11770944,\n",
      "        -0.9670766 ,  0.1072313 ,  1.4646606 , -0.11770944, -0.9670766 ,\n",
      "         0.1072313 ,  1.4646606 , -0.11770944, -0.9670766 ,  0.1072313 ,\n",
      "         1.4646606 , -0.11770944, -0.9670766 ,  0.1072313 ,  1.4646606 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.04640366, -0.22378856, -0.01511014,  0.30280432],\n",
      "       [-0.08360109,  0.3913968 ,  0.13697243, -0.24376503]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05087943, -0.41869193, -0.00905405,  0.5906838 ],\n",
      "       [-0.07577316,  0.5843236 ,  0.13209713, -0.49030063]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.05087943, -0.41869193, -0.00905405,  0.5906838 , -0.05087943,\n",
      "        -0.41869193, -0.00905405,  0.5906838 , -0.05087943, -0.41869193,\n",
      "        -0.00905405,  0.5906838 , -0.05087943, -0.41869193, -0.00905405,\n",
      "         0.5906838 , -0.05087943, -0.41869193, -0.00905405,  0.5906838 ],\n",
      "       [-0.07577316,  0.5843236 ,  0.13209713, -0.49030063, -0.07577316,\n",
      "         0.5843236 ,  0.13209713, -0.49030063, -0.07577316,  0.5843236 ,\n",
      "         0.13209713, -0.49030063, -0.07577316,  0.5843236 ,  0.13209713,\n",
      "        -0.49030063, -0.07577316,  0.5843236 ,  0.13209713, -0.49030063]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.04640366, -0.22378856, -0.01511014,  0.30280432],\n",
      "       [-0.08360109,  0.3913968 ,  0.13697243, -0.24376503]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.04640366, -0.22378856, -0.01511014,  0.30280432, -0.04640366,\n",
      "        -0.22378856, -0.01511014,  0.30280432, -0.04640366, -0.22378856,\n",
      "        -0.01511014,  0.30280432, -0.04640366, -0.22378856, -0.01511014,\n",
      "         0.30280432, -0.04640366, -0.22378856, -0.01511014,  0.30280432],\n",
      "       [-0.08360109,  0.3913968 ,  0.13697243, -0.24376503, -0.08360109,\n",
      "         0.3913968 ,  0.13697243, -0.24376503, -0.08360109,  0.3913968 ,\n",
      "         0.13697243, -0.24376503, -0.08360109,  0.3913968 ,  0.13697243,\n",
      "        -0.24376503, -0.08360109,  0.3913968 ,  0.13697243, -0.24376503]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.06600779,  0.36120397,  0.00929837, -0.5671217 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.06600779,  0.36120397,  0.00929837, -0.5671217 , -0.06600779,\n",
      "         0.36120397,  0.00929837, -0.5671217 , -0.06600779,  0.36120397,\n",
      "         0.00929837, -0.5671217 , -0.06600779,  0.36120397,  0.00929837,\n",
      "        -0.5671217 , -0.06600779,  0.36120397,  0.00929837, -0.5671217 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.01668782,  0.03163327,  0.01967609,  0.04981611],\n",
      "       [-0.03361785,  0.2186208 ,  0.03386042, -0.2680246 ]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.01605515, -0.1637652 ,  0.02067241,  0.34864148],\n",
      "       [-0.02924544,  0.41324356,  0.02849992, -0.5498383 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.01605515, -0.1637652 ,  0.02067241,  0.34864148, -0.01605515,\n",
      "        -0.1637652 ,  0.02067241,  0.34864148, -0.01605515, -0.1637652 ,\n",
      "         0.02067241,  0.34864148, -0.01605515, -0.1637652 ,  0.02067241,\n",
      "         0.34864148, -0.01605515, -0.1637652 ,  0.02067241,  0.34864148],\n",
      "       [-0.02924544,  0.41324356,  0.02849992, -0.5498383 , -0.02924544,\n",
      "         0.41324356,  0.02849992, -0.5498383 , -0.02924544,  0.41324356,\n",
      "         0.02849992, -0.5498383 , -0.02924544,  0.41324356,  0.02849992,\n",
      "        -0.5498383 , -0.02924544,  0.41324356,  0.02849992, -0.5498383 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.01668782,  0.03163327,  0.01967609,  0.04981611],\n",
      "       [-0.03361785,  0.2186208 ,  0.03386042, -0.2680246 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.01668782,  0.03163327,  0.01967609,  0.04981611, -0.01668782,\n",
      "         0.03163327,  0.01967609,  0.04981611, -0.01668782,  0.03163327,\n",
      "         0.01967609,  0.04981611, -0.01668782,  0.03163327,  0.01967609,\n",
      "         0.04981611, -0.01668782,  0.03163327,  0.01967609,  0.04981611],\n",
      "       [-0.03361785,  0.2186208 ,  0.03386042, -0.2680246 , -0.03361785,\n",
      "         0.2186208 ,  0.03386042, -0.2680246 , -0.03361785,  0.2186208 ,\n",
      "         0.03386042, -0.2680246 , -0.03361785,  0.2186208 ,  0.03386042,\n",
      "        -0.2680246 , -0.03361785,  0.2186208 ,  0.03386042, -0.2680246 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.06053054, -0.41917446, -0.00084769,  0.60127556]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.06053054, -0.41917446, -0.00084769,  0.60127556, -0.06053054,\n",
      "        -0.41917446, -0.00084769,  0.60127556, -0.06053054, -0.41917446,\n",
      "        -0.00084769,  0.60127556, -0.06053054, -0.41917446, -0.00084769,\n",
      "         0.60127556, -0.06053054, -0.41917446, -0.00084769,  0.60127556]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.10188267, -0.42029044,  0.05926717,  0.62676036]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.10188267, -0.42029044,  0.05926717,  0.62676036, -0.10188267,\n",
      "        -0.42029044,  0.05926717,  0.62676036, -0.10188267, -0.42029044,\n",
      "         0.05926717,  0.62676036, -0.10188267, -0.42029044,  0.05926717,\n",
      "         0.62676036, -0.10188267, -0.42029044,  0.05926717,  0.62676036]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.12261223, -0.42210323,  0.09055246,  0.66821986]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.12261223, -0.42210323,  0.09055246,  0.66821986, -0.12261223,\n",
      "        -0.42210323,  0.09055246,  0.66821986, -0.12261223, -0.42210323,\n",
      "         0.09055246,  0.66821986, -0.12261223, -0.42210323,  0.09055246,\n",
      "         0.66821986, -0.12261223, -0.42210323,  0.09055246,  0.66821986]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05976485, -0.7702667 ,  0.02142688,  1.1284889 ],\n",
      "       [-0.04555184,  0.20494656,  0.00318093, -0.32626176]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.07517019, -0.9656627 ,  0.04399667,  1.4278146 ],\n",
      "       [-0.04145291,  0.00977947, -0.00334431, -0.0325774 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.07517019, -0.9656627 ,  0.04399667,  1.4278146 , -0.07517019,\n",
      "        -0.9656627 ,  0.04399667,  1.4278146 , -0.07517019, -0.9656627 ,\n",
      "         0.04399667,  1.4278146 , -0.07517019, -0.9656627 ,  0.04399667,\n",
      "         1.4278146 , -0.07517019, -0.9656627 ,  0.04399667,  1.4278146 ],\n",
      "       [-0.04145291,  0.00977947, -0.00334431, -0.0325774 , -0.04145291,\n",
      "         0.00977947, -0.00334431, -0.0325774 , -0.04145291,  0.00977947,\n",
      "        -0.00334431, -0.0325774 , -0.04145291,  0.00977947, -0.00334431,\n",
      "        -0.0325774 , -0.04145291,  0.00977947, -0.00334431, -0.0325774 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05976485, -0.7702667 ,  0.02142688,  1.1284889 ],\n",
      "       [-0.04555184,  0.20494656,  0.00318093, -0.32626176]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.05976485, -0.7702667 ,  0.02142688,  1.1284889 , -0.05976485,\n",
      "        -0.7702667 ,  0.02142688,  1.1284889 , -0.05976485, -0.7702667 ,\n",
      "         0.02142688,  1.1284889 , -0.05976485, -0.7702667 ,  0.02142688,\n",
      "         1.1284889 , -0.05976485, -0.7702667 ,  0.02142688,  1.1284889 ],\n",
      "       [-0.04555184,  0.20494656,  0.00318093, -0.32626176, -0.04555184,\n",
      "         0.20494656,  0.00318093, -0.32626176, -0.04555184,  0.20494656,\n",
      "         0.00318093, -0.32626176, -0.04555184,  0.20494656,  0.00318093,\n",
      "        -0.32626176, -0.04555184,  0.20494656,  0.00318093, -0.32626176]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "Episode 10/2000, average last 10 rewards 18.1\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.02395593,  0.18445657,  0.00670073, -0.33770174]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.02395593,  0.18445657,  0.00670073, -0.33770174,  0.02395593,\n",
      "         0.18445657,  0.00670073, -0.33770174,  0.02395593,  0.18445657,\n",
      "         0.00670073, -0.33770174,  0.02395593,  0.18445657,  0.00670073,\n",
      "        -0.33770174,  0.02395593,  0.18445657,  0.00670073, -0.33770174]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.02309731,  0.18431684,  0.00321913, -0.3346093 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.02309731,  0.18431684,  0.00321913, -0.3346093 ,  0.02309731,\n",
      "         0.18431684,  0.00321913, -0.3346093 ,  0.02309731,  0.18431684,\n",
      "         0.00321913, -0.3346093 ,  0.02309731,  0.18431684,  0.00321913,\n",
      "        -0.3346093 ,  0.02309731,  0.18431684,  0.00321913, -0.3346093 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.03025305,  0.37950358, -0.01098511, -0.6287227 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.03025305,  0.37950358, -0.01098511, -0.6287227 ,  0.03025305,\n",
      "         0.37950358, -0.01098511, -0.6287227 ,  0.03025305,  0.37950358,\n",
      "        -0.01098511, -0.6287227 ,  0.03025305,  0.37950358, -0.01098511,\n",
      "        -0.6287227 ,  0.03025305,  0.37950358, -0.01098511, -0.6287227 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.00650013, -0.5364496 , -0.03736428,  0.76287335],\n",
      "       [-0.16031769, -0.9699851 ,  0.17230101,  1.5415105 ]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.00422887, -0.73103756, -0.02210682,  1.043569  ],\n",
      "       [-0.17971739, -1.166709  ,  0.20313121,  1.8826331 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.00422887, -0.73103756, -0.02210682,  1.043569  , -0.00422887,\n",
      "        -0.73103756, -0.02210682,  1.043569  , -0.00422887, -0.73103756,\n",
      "        -0.02210682,  1.043569  , -0.00422887, -0.73103756, -0.02210682,\n",
      "         1.043569  , -0.00422887, -0.73103756, -0.02210682,  1.043569  ],\n",
      "       [-0.17971739, -1.166709  ,  0.20313121,  1.8826331 , -0.17971739,\n",
      "        -1.166709  ,  0.20313121,  1.8826331 , -0.17971739, -1.166709  ,\n",
      "         0.20313121,  1.8826331 , -0.17971739, -1.166709  ,  0.20313121,\n",
      "         1.8826331 , -0.17971739, -1.166709  ,  0.20313121,  1.8826331 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.00650013, -0.5364496 , -0.03736428,  0.76287335],\n",
      "       [-0.16031769, -0.9699851 ,  0.17230101,  1.5415105 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.00650013, -0.5364496 , -0.03736428,  0.76287335,  0.00650013,\n",
      "        -0.5364496 , -0.03736428,  0.76287335,  0.00650013, -0.5364496 ,\n",
      "        -0.03736428,  0.76287335,  0.00650013, -0.5364496 , -0.03736428,\n",
      "         0.76287335,  0.00650013, -0.5364496 , -0.03736428,  0.76287335],\n",
      "       [-0.16031769, -0.9699851 ,  0.17230101,  1.5415105 , -0.16031769,\n",
      "        -0.9699851 ,  0.17230101,  1.5415105 , -0.16031769, -0.9699851 ,\n",
      "         0.17230101,  1.5415105 , -0.16031769, -0.9699851 ,  0.17230101,\n",
      "         1.5415105 , -0.16031769, -0.9699851 ,  0.17230101,  1.5415105 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.02309731,  0.18431684,  0.00321913, -0.3346093 ],\n",
      "       [-0.05604746, -0.22415386, -0.00706427,  0.31082883]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.02678365, -0.01085077, -0.00347305, -0.04091297],\n",
      "       [-0.06053054, -0.41917446, -0.00084769,  0.60127556]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.02678365, -0.01085077, -0.00347305, -0.04091297,  0.02678365,\n",
      "        -0.01085077, -0.00347305, -0.04091297,  0.02678365, -0.01085077,\n",
      "        -0.00347305, -0.04091297,  0.02678365, -0.01085077, -0.00347305,\n",
      "        -0.04091297,  0.02678365, -0.01085077, -0.00347305, -0.04091297],\n",
      "       [-0.06053054, -0.41917446, -0.00084769,  0.60127556, -0.06053054,\n",
      "        -0.41917446, -0.00084769,  0.60127556, -0.06053054, -0.41917446,\n",
      "        -0.00084769,  0.60127556, -0.06053054, -0.41917446, -0.00084769,\n",
      "         0.60127556, -0.06053054, -0.41917446, -0.00084769,  0.60127556]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.02309731,  0.18431684,  0.00321913, -0.3346093 ],\n",
      "       [-0.05604746, -0.22415386, -0.00706427,  0.31082883]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.02309731,  0.18431684,  0.00321913, -0.3346093 ,  0.02309731,\n",
      "         0.18431684,  0.00321913, -0.3346093 ,  0.02309731,  0.18431684,\n",
      "         0.00321913, -0.3346093 ,  0.02309731,  0.18431684,  0.00321913,\n",
      "        -0.3346093 ,  0.02309731,  0.18431684,  0.00321913, -0.3346093 ],\n",
      "       [-0.05604746, -0.22415386, -0.00706427,  0.31082883, -0.05604746,\n",
      "        -0.22415386, -0.00706427,  0.31082883, -0.05604746, -0.22415386,\n",
      "        -0.00706427,  0.31082883, -0.05604746, -0.22415386, -0.00706427,\n",
      "         0.31082883, -0.05604746, -0.22415386, -0.00706427,  0.31082883]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.15971565, -1.01116   ,  0.14990485,  1.6401103 ],\n",
      "       [-0.03951184,  0.01640485,  0.03816273,  0.00764899]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.17993885, -1.2076871 ,  0.18270706,  1.975502  ],\n",
      "       [-0.03918374, -0.17924304,  0.03831571,  0.31212425]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.17993885, -1.2076871 ,  0.18270706,  1.975502  , -0.17993885,\n",
      "        -1.2076871 ,  0.18270706,  1.975502  , -0.17993885, -1.2076871 ,\n",
      "         0.18270706,  1.975502  , -0.17993885, -1.2076871 ,  0.18270706,\n",
      "         1.975502  , -0.17993885, -1.2076871 ,  0.18270706,  1.975502  ],\n",
      "       [-0.03918374, -0.17924304,  0.03831571,  0.31212425, -0.03918374,\n",
      "        -0.17924304,  0.03831571,  0.31212425, -0.03918374, -0.17924304,\n",
      "         0.03831571,  0.31212425, -0.03918374, -0.17924304,  0.03831571,\n",
      "         0.31212425, -0.03918374, -0.17924304,  0.03831571,  0.31212425]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.15971565, -1.01116   ,  0.14990485,  1.6401103 ],\n",
      "       [-0.03951184,  0.01640485,  0.03816273,  0.00764899]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.15971565, -1.01116   ,  0.14990485,  1.6401103 , -0.15971565,\n",
      "        -1.01116   ,  0.14990485,  1.6401103 , -0.15971565, -1.01116   ,\n",
      "         0.14990485,  1.6401103 , -0.15971565, -1.01116   ,  0.14990485,\n",
      "         1.6401103 , -0.15971565, -1.01116   ,  0.14990485,  1.6401103 ],\n",
      "       [-0.03951184,  0.01640485,  0.03816273,  0.00764899, -0.03951184,\n",
      "         0.01640485,  0.03816273,  0.00764899, -0.03951184,  0.01640485,\n",
      "         0.03816273,  0.00764899, -0.03951184,  0.01640485,  0.03816273,\n",
      "         0.00764899, -0.03951184,  0.01640485,  0.03816273,  0.00764899]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.11770944, -0.9670766 ,  0.1072313 ,  1.4646606 ],\n",
      "       [ 0.10554513,  0.01976979, -0.13709643, -0.25556305]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.13705097, -1.1633362 ,  0.13652451,  1.7888248 ],\n",
      "       [ 0.10594052, -0.17315559, -0.1422077 , -0.00907054]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.13705097, -1.1633362 ,  0.13652451,  1.7888248 , -0.13705097,\n",
      "        -1.1633362 ,  0.13652451,  1.7888248 , -0.13705097, -1.1633362 ,\n",
      "         0.13652451,  1.7888248 , -0.13705097, -1.1633362 ,  0.13652451,\n",
      "         1.7888248 , -0.13705097, -1.1633362 ,  0.13652451,  1.7888248 ],\n",
      "       [ 0.10594052, -0.17315559, -0.1422077 , -0.00907054,  0.10594052,\n",
      "        -0.17315559, -0.1422077 , -0.00907054,  0.10594052, -0.17315559,\n",
      "        -0.1422077 , -0.00907054,  0.10594052, -0.17315559, -0.1422077 ,\n",
      "        -0.00907054,  0.10594052, -0.17315559, -0.1422077 , -0.00907054]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.11770944, -0.9670766 ,  0.1072313 ,  1.4646606 ],\n",
      "       [ 0.10554513,  0.01976979, -0.13709643, -0.25556305]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.11770944, -0.9670766 ,  0.1072313 ,  1.4646606 , -0.11770944,\n",
      "        -0.9670766 ,  0.1072313 ,  1.4646606 , -0.11770944, -0.9670766 ,\n",
      "         0.1072313 ,  1.4646606 , -0.11770944, -0.9670766 ,  0.1072313 ,\n",
      "         1.4646606 , -0.11770944, -0.9670766 ,  0.1072313 ,  1.4646606 ],\n",
      "       [ 0.10554513,  0.01976979, -0.13709643, -0.25556305,  0.10554513,\n",
      "         0.01976979, -0.13709643, -0.25556305,  0.10554513,  0.01976979,\n",
      "        -0.13709643, -0.25556305,  0.10554513,  0.01976979, -0.13709643,\n",
      "        -0.25556305,  0.10554513,  0.01976979, -0.13709643, -0.25556305]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.04735948,  0.20584352, -0.02179984, -0.2690961 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.04735948,  0.20584352, -0.02179984, -0.2690961 , -0.04735948,\n",
      "         0.20584352, -0.02179984, -0.2690961 , -0.04735948,  0.20584352,\n",
      "        -0.02179984, -0.2690961 , -0.04735948,  0.20584352, -0.02179984,\n",
      "        -0.2690961 , -0.04735948,  0.20584352, -0.02179984, -0.2690961 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03715833,  0.00988439, -0.01052213, -0.03489348],\n",
      "       [-0.04640366, -0.22378856, -0.01511014,  0.30280432]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03696064, -0.1850851 , -0.01122   ,  0.25445113],\n",
      "       [-0.05087943, -0.41869193, -0.00905405,  0.5906838 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03696064, -0.1850851 , -0.01122   ,  0.25445113, -0.03696064,\n",
      "        -0.1850851 , -0.01122   ,  0.25445113, -0.03696064, -0.1850851 ,\n",
      "        -0.01122   ,  0.25445113, -0.03696064, -0.1850851 , -0.01122   ,\n",
      "         0.25445113, -0.03696064, -0.1850851 , -0.01122   ,  0.25445113],\n",
      "       [-0.05087943, -0.41869193, -0.00905405,  0.5906838 , -0.05087943,\n",
      "        -0.41869193, -0.00905405,  0.5906838 , -0.05087943, -0.41869193,\n",
      "        -0.00905405,  0.5906838 , -0.05087943, -0.41869193, -0.00905405,\n",
      "         0.5906838 , -0.05087943, -0.41869193, -0.00905405,  0.5906838 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03715833,  0.00988439, -0.01052213, -0.03489348],\n",
      "       [-0.04640366, -0.22378856, -0.01511014,  0.30280432]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03715833,  0.00988439, -0.01052213, -0.03489348, -0.03715833,\n",
      "         0.00988439, -0.01052213, -0.03489348, -0.03715833,  0.00988439,\n",
      "        -0.01052213, -0.03489348, -0.03715833,  0.00988439, -0.01052213,\n",
      "        -0.03489348, -0.03715833,  0.00988439, -0.01052213, -0.03489348],\n",
      "       [-0.04640366, -0.22378856, -0.01511014,  0.30280432, -0.04640366,\n",
      "        -0.22378856, -0.01511014,  0.30280432, -0.04640366, -0.22378856,\n",
      "        -0.01511014,  0.30280432, -0.04640366, -0.22378856, -0.01511014,\n",
      "         0.30280432, -0.04640366, -0.22378856, -0.01511014,  0.30280432]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "states=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.04198532, -0.525683  , -0.17589054, -0.25938982],\n",
      "       [-0.15971565, -1.01116   ,  0.14990485,  1.6401103 ]],\n",
      "      dtype=float32)>\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05249898, -0.71791494, -0.18107833, -0.0269364 ],\n",
      "       [-0.17993885, -1.2076871 ,  0.18270706,  1.975502  ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.05249898, -0.71791494, -0.18107833, -0.0269364 , -0.05249898,\n",
      "        -0.71791494, -0.18107833, -0.0269364 , -0.05249898, -0.71791494,\n",
      "        -0.18107833, -0.0269364 , -0.05249898, -0.71791494, -0.18107833,\n",
      "        -0.0269364 , -0.05249898, -0.71791494, -0.18107833, -0.0269364 ],\n",
      "       [-0.17993885, -1.2076871 ,  0.18270706,  1.975502  , -0.17993885,\n",
      "        -1.2076871 ,  0.18270706,  1.975502  , -0.17993885, -1.2076871 ,\n",
      "         0.18270706,  1.975502  , -0.17993885, -1.2076871 ,  0.18270706,\n",
      "         1.975502  , -0.17993885, -1.2076871 ,  0.18270706,  1.975502  ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.04198532, -0.525683  , -0.17589054, -0.25938982],\n",
      "       [-0.15971565, -1.01116   ,  0.14990485,  1.6401103 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([0.9990014 , 1.0009993 , 1.0009961 , 0.9990004 , 0.99900234,\n",
      "       1.0009993 , 0.9990079 , 1.0009995 , 0.9990018 , 0.9990004 ,\n",
      "       0.9990014 , 1.0009999 , 0.9990022 , 0.9990003 , 0.9990014 ,\n",
      "       0.99900013, 0.99904734, 1.0009997 , 1.0009965 , 1.0009928 ],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.04198532, -0.525683  , -0.17589054, -0.25938982, -0.04198532,\n",
      "        -0.525683  , -0.17589054, -0.25938982, -0.04198532, -0.525683  ,\n",
      "        -0.17589054, -0.25938982, -0.04198532, -0.525683  , -0.17589054,\n",
      "        -0.25938982, -0.04198532, -0.525683  , -0.17589054, -0.25938982],\n",
      "       [-0.15971565, -1.01116   ,  0.14990485,  1.6401103 , -0.15971565,\n",
      "        -1.01116   ,  0.14990485,  1.6401103 , -0.15971565, -1.01116   ,\n",
      "         0.14990485,  1.6401103 , -0.15971565, -1.01116   ,  0.14990485,\n",
      "         1.6401103 , -0.15971565, -1.01116   ,  0.14990485,  1.6401103 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n"
     ]
    }
   ],
   "source": [
    "######### NEW TRAINING LOOP\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "    \n",
    "episode_reward_history = []\n",
    "step_count = 0\n",
    "for episode in range(n_episodes):\n",
    "    episode_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        # Interact with env\n",
    "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
    "        \n",
    "        # Store interaction in the replay memory\n",
    "        replay_memory.append(interaction)\n",
    "        \n",
    "        state = interaction['next_state']\n",
    "        episode_reward += interaction['reward']\n",
    "        step_count += 1\n",
    "        # print(f\"{state.shape=}\")\n",
    "        \n",
    "        # Update model\n",
    "        if step_count % steps_per_update == 0:\n",
    "            # Sample a batch of interactions and update Q_function\n",
    "            training_batch = np.random.choice(replay_memory, size=batch_size)\n",
    "            #\n",
    "            #\n",
    "            Q_learning_update(\n",
    "                states=np.asarray([x['state'] for x in training_batch]).squeeze(),\n",
    "                actions=np.asarray([x['action'] for x in training_batch]).squeeze(),\n",
    "                rewards=np.asarray([x['reward'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "                next_states=np.asarray([x['next_state'] for x in training_batch]).squeeze(),\n",
    "                done=np.asarray([x['done'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "                model=model,\n",
    "                model_target=model_target,\n",
    "                gamma=gamma,\n",
    "                n_actions=n_actions,\n",
    "                optimizer_w_tups=optimizer_w_tups,\n",
    "                )\n",
    "            #\n",
    "            #\n",
    "            #\n",
    "            #\n",
    "            # Q_learning_update(\n",
    "            #     states=np.asarray([x['state'] for x in training_batch]).squeeze(),\n",
    "            #     actions=np.asarray([x['action'] for x in training_batch]).squeeze(),\n",
    "            #     rewards=np.asarray([x['reward'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "            #     next_states=np.asarray([x['next_state'] for x in training_batch]).squeeze(),\n",
    "            #     done=np.asarray([x['done'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "            #     model=model,\n",
    "            #     model_target=model_target,\n",
    "            #     gamma=gamma,\n",
    "            #     n_actions=n_actions,\n",
    "            #     optimizer_w_tups=optimizer_w_tups,\n",
    "            # )\n",
    "            #\n",
    "            #\n",
    "            #\n",
    "            #\n",
    "            # Q_learning_update_new(\n",
    "            #     np.asarray([x['state'] for x in training_batch]).squeeze(),\n",
    "            #     np.asarray([x['action'] for x in training_batch]).squeeze(),\n",
    "            #     np.asarray([x['reward'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "            #     np.asarray([x['next_state'] for x in training_batch]).squeeze(),\n",
    "            #     np.asarray([x['done'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "            #     model,\n",
    "            #     gamma,\n",
    "            #     n_actions,\n",
    "            #     model_target,\n",
    "            #     )\n",
    "        \n",
    "        # Update target model\n",
    "        if step_count % steps_per_target_update == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "        \n",
    "        # Check if the episode is finished\n",
    "        if interaction['done']:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if (episode+1)%10 == 0:\n",
    "        avg_rewards = np.mean(episode_reward_history[-10:])\n",
    "        print(\"Episode {}/{}, average last 10 rewards {}\".format(\n",
    "            episode+1, n_episodes, avg_rewards))\n",
    "        if avg_rewards >= 500.0:\n",
    "            break\n",
    "        \n",
    "    if episode > 10: break\n",
    "######### NEW TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x14c8a2a00>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = zip([chr(ord('a')+i) for i in range(10)], [i for i in range(10)])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 0)\n",
      "('b', 1)\n",
      "('c', 2)\n",
      "('d', 3)\n",
      "('e', 4)\n",
      "('f', 5)\n",
      "('g', 6)\n",
      "('h', 7)\n",
      "('i', 8)\n",
      "('j', 9)\n"
     ]
    }
   ],
   "source": [
    "for a in x:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x14c8a2a00>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03445966,  0.1672034 , -0.01985493, -0.32845902],\n",
      "       [-0.03389587, -0.02818938, -0.01925969, -0.02976223]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03445966,  0.1672034 , -0.01985493, -0.32845902, -0.03445966,\n",
      "         0.1672034 , -0.01985493, -0.32845902, -0.03445966,  0.1672034 ,\n",
      "        -0.01985493, -0.32845902, -0.03445966,  0.1672034 , -0.01985493,\n",
      "        -0.32845902, -0.03445966,  0.1672034 , -0.01985493, -0.32845902],\n",
      "       [-0.03389587, -0.02818938, -0.01925969, -0.02976223, -0.03389587,\n",
      "        -0.02818938, -0.01925969, -0.02976223, -0.03389587, -0.02818938,\n",
      "        -0.01925969, -0.02976223, -0.03389587, -0.02818938, -0.01925969,\n",
      "        -0.02976223, -0.03389587, -0.02818938, -0.01925969, -0.02976223]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03389587, -0.02818938, -0.01925969, -0.02976223],\n",
      "       [-0.02942278, -0.22365454, -0.02467167,  0.27059904]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03389587, -0.02818938, -0.01925969, -0.02976223, -0.03389587,\n",
      "        -0.02818938, -0.01925969, -0.02976223, -0.03389587, -0.02818938,\n",
      "        -0.01925969, -0.02976223, -0.03389587, -0.02818938, -0.01925969,\n",
      "        -0.02976223, -0.03389587, -0.02818938, -0.01925969, -0.02976223],\n",
      "       [-0.02942278, -0.22365454, -0.02467167,  0.27059904, -0.02942278,\n",
      "        -0.22365454, -0.02467167,  0.27059904, -0.02942278, -0.22365454,\n",
      "        -0.02467167,  0.27059904, -0.02942278, -0.22365454, -0.02467167,\n",
      "         0.27059904, -0.02942278, -0.22365454, -0.02467167,  0.27059904]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03368935, -0.01268342, -0.16879675, -0.37685913],\n",
      "       [ 0.03009042,  0.17994651, -0.15646754, -0.61646026]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.03368935, -0.01268342, -0.16879675, -0.37685913,  0.03368935,\n",
      "        -0.01268342, -0.16879675, -0.37685913,  0.03368935, -0.01268342,\n",
      "        -0.16879675, -0.37685913,  0.03368935, -0.01268342, -0.16879675,\n",
      "        -0.37685913,  0.03368935, -0.01268342, -0.16879675, -0.37685913],\n",
      "       [ 0.03009042,  0.17994651, -0.15646754, -0.61646026,  0.03009042,\n",
      "         0.17994651, -0.15646754, -0.61646026,  0.03009042,  0.17994651,\n",
      "        -0.15646754, -0.61646026,  0.03009042,  0.17994651, -0.15646754,\n",
      "        -0.61646026,  0.03009042,  0.17994651, -0.15646754, -0.61646026]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.03009042,  0.17994651, -0.15646754, -0.61646026],\n",
      "       [ 0.02263189,  0.3729263 , -0.13922094, -0.86233026]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.03009042,  0.17994651, -0.15646754, -0.61646026,  0.03009042,\n",
      "         0.17994651, -0.15646754, -0.61646026,  0.03009042,  0.17994651,\n",
      "        -0.15646754, -0.61646026,  0.03009042,  0.17994651, -0.15646754,\n",
      "        -0.61646026,  0.03009042,  0.17994651, -0.15646754, -0.61646026],\n",
      "       [ 0.02263189,  0.3729263 , -0.13922094, -0.86233026,  0.02263189,\n",
      "         0.3729263 , -0.13922094, -0.86233026,  0.02263189,  0.3729263 ,\n",
      "        -0.13922094, -0.86233026,  0.02263189,  0.3729263 , -0.13922094,\n",
      "        -0.86233026,  0.02263189,  0.3729263 , -0.13922094, -0.86233026]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.01270189,  0.36350814, -0.0575353 , -0.6480369 ],\n",
      "       [-0.02884497, -0.02889065, -0.02438584, -0.01429122]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.01270189,  0.36350814, -0.0575353 , -0.6480369 , -0.01270189,\n",
      "         0.36350814, -0.0575353 , -0.6480369 , -0.01270189,  0.36350814,\n",
      "        -0.0575353 , -0.6480369 , -0.01270189,  0.36350814, -0.0575353 ,\n",
      "        -0.6480369 , -0.01270189,  0.36350814, -0.0575353 , -0.6480369 ],\n",
      "       [-0.02884497, -0.02889065, -0.02438584, -0.01429122, -0.02884497,\n",
      "        -0.02889065, -0.02438584, -0.01429122, -0.02884497, -0.02889065,\n",
      "        -0.02438584, -0.01429122, -0.02884497, -0.02889065, -0.02438584,\n",
      "        -0.01429122, -0.02884497, -0.02889065, -0.02438584, -0.01429122]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.02386354,  0.5580829 , -0.03897084, -0.9282229 ],\n",
      "       [-0.03216426,  0.16596481, -0.01836331, -0.30112663]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.02386354,  0.5580829 , -0.03897084, -0.9282229 , -0.02386354,\n",
      "         0.5580829 , -0.03897084, -0.9282229 , -0.02386354,  0.5580829 ,\n",
      "        -0.03897084, -0.9282229 , -0.02386354,  0.5580829 , -0.03897084,\n",
      "        -0.9282229 , -0.02386354,  0.5580829 , -0.03897084, -0.9282229 ],\n",
      "       [-0.03216426,  0.16596481, -0.01836331, -0.30112663, -0.03216426,\n",
      "         0.16596481, -0.01836331, -0.30112663, -0.03216426,  0.16596481,\n",
      "        -0.01836331, -0.30112663, -0.03216426,  0.16596481, -0.01836331,\n",
      "        -0.30112663, -0.03216426,  0.16596481, -0.01836331, -0.30112663]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.01270189,  0.36350814, -0.0575353 , -0.6480369 ],\n",
      "       [ 0.04822759, -0.0061276 , -0.01528314,  0.01249869]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.01270189,  0.36350814, -0.0575353 , -0.6480369 , -0.01270189,\n",
      "         0.36350814, -0.0575353 , -0.6480369 , -0.01270189,  0.36350814,\n",
      "        -0.0575353 , -0.6480369 , -0.01270189,  0.36350814, -0.0575353 ,\n",
      "        -0.6480369 , -0.01270189,  0.36350814, -0.0575353 , -0.6480369 ],\n",
      "       [ 0.04822759, -0.0061276 , -0.01528314,  0.01249869,  0.04822759,\n",
      "        -0.0061276 , -0.01528314,  0.01249869,  0.04822759, -0.0061276 ,\n",
      "        -0.01528314,  0.01249869,  0.04822759, -0.0061276 , -0.01528314,\n",
      "         0.01249869,  0.04822759, -0.0061276 , -0.01528314,  0.01249869]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.02386354,  0.5580829 , -0.03897084, -0.9282229 ],\n",
      "       [ 0.05225857, -0.20154944, -0.02152094,  0.31189024]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.02386354,  0.5580829 , -0.03897084, -0.9282229 , -0.02386354,\n",
      "         0.5580829 , -0.03897084, -0.9282229 , -0.02386354,  0.5580829 ,\n",
      "        -0.03897084, -0.9282229 , -0.02386354,  0.5580829 , -0.03897084,\n",
      "        -0.9282229 , -0.02386354,  0.5580829 , -0.03897084, -0.9282229 ],\n",
      "       [ 0.05225857, -0.20154944, -0.02152094,  0.31189024,  0.05225857,\n",
      "        -0.20154944, -0.02152094,  0.31189024,  0.05225857, -0.20154944,\n",
      "        -0.02152094,  0.31189024,  0.05225857, -0.20154944, -0.02152094,\n",
      "         0.31189024,  0.05225857, -0.20154944, -0.02152094,  0.31189024]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.186219  ,  0.39293292, -0.22123167, -0.785008  ],\n",
      "       [ 0.03215217, -0.39584526,  0.00859199,  0.5862572 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.186219  ,  0.39293292, -0.22123167, -0.785008  ,  0.186219  ,\n",
      "         0.39293292, -0.22123167, -0.785008  ,  0.186219  ,  0.39293292,\n",
      "        -0.22123167, -0.785008  ,  0.186219  ,  0.39293292, -0.22123167,\n",
      "        -0.785008  ,  0.186219  ,  0.39293292, -0.22123167, -0.785008  ],\n",
      "       [ 0.03215217, -0.39584526,  0.00859199,  0.5862572 ,  0.03215217,\n",
      "        -0.39584526,  0.00859199,  0.5862572 ,  0.03215217, -0.39584526,\n",
      "         0.00859199,  0.5862572 ,  0.03215217, -0.39584526,  0.00859199,\n",
      "         0.5862572 ,  0.03215217, -0.39584526,  0.00859199,  0.5862572 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.17452128,  0.5848864 , -0.20106322, -1.0084226 ],\n",
      "       [ 0.03616586, -0.20068437,  0.00273775,  0.29271212]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.17452128,  0.5848864 , -0.20106322, -1.0084226 ,  0.17452128,\n",
      "         0.5848864 , -0.20106322, -1.0084226 ,  0.17452128,  0.5848864 ,\n",
      "        -0.20106322, -1.0084226 ,  0.17452128,  0.5848864 , -0.20106322,\n",
      "        -1.0084226 ,  0.17452128,  0.5848864 , -0.20106322, -1.0084226 ],\n",
      "       [ 0.03616586, -0.20068437,  0.00273775,  0.29271212,  0.03616586,\n",
      "        -0.20068437,  0.00273775,  0.29271212,  0.03616586, -0.20068437,\n",
      "         0.00273775,  0.29271212,  0.03616586, -0.20068437,  0.00273775,\n",
      "         0.29271212,  0.03616586, -0.20068437,  0.00273775,  0.29271212]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.06501058,  0.03792677,  0.0077457 , -0.02325176],\n",
      "       [ 0.03009042,  0.17994651, -0.15646754, -0.61646026]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.06501058,  0.03792677,  0.0077457 , -0.02325176, -0.06501058,\n",
      "         0.03792677,  0.0077457 , -0.02325176, -0.06501058,  0.03792677,\n",
      "         0.0077457 , -0.02325176, -0.06501058,  0.03792677,  0.0077457 ,\n",
      "        -0.02325176, -0.06501058,  0.03792677,  0.0077457 , -0.02325176],\n",
      "       [ 0.03009042,  0.17994651, -0.15646754, -0.61646026,  0.03009042,\n",
      "         0.17994651, -0.15646754, -0.61646026,  0.03009042,  0.17994651,\n",
      "        -0.15646754, -0.61646026,  0.03009042,  0.17994651, -0.15646754,\n",
      "        -0.61646026,  0.03009042,  0.17994651, -0.15646754, -0.61646026]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.06186736, -0.15716124,  0.00237206,  0.26868206],\n",
      "       [ 0.02263189,  0.3729263 , -0.13922094, -0.86233026]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.06186736, -0.15716124,  0.00237206,  0.26868206, -0.06186736,\n",
      "        -0.15716124,  0.00237206,  0.26868206, -0.06186736, -0.15716124,\n",
      "         0.00237206,  0.26868206, -0.06186736, -0.15716124,  0.00237206,\n",
      "         0.26868206, -0.06186736, -0.15716124,  0.00237206,  0.26868206],\n",
      "       [ 0.02263189,  0.3729263 , -0.13922094, -0.86233026,  0.02263189,\n",
      "         0.3729263 , -0.13922094, -0.86233026,  0.02263189,  0.3729263 ,\n",
      "        -0.13922094, -0.86233026,  0.02263189,  0.3729263 , -0.13922094,\n",
      "        -0.86233026,  0.02263189,  0.3729263 , -0.13922094, -0.86233026]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03111559,  0.3626023 , -0.02642411, -0.62733656],\n",
      "       [ 0.04822759, -0.0061276 , -0.01528314,  0.01249869]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03111559,  0.3626023 , -0.02642411, -0.62733656, -0.03111559,\n",
      "         0.3626023 , -0.02642411, -0.62733656, -0.03111559,  0.3626023 ,\n",
      "        -0.02642411, -0.62733656, -0.03111559,  0.3626023 , -0.02642411,\n",
      "        -0.62733656, -0.03111559,  0.3626023 , -0.02642411, -0.62733656],\n",
      "       [ 0.04822759, -0.0061276 , -0.01528314,  0.01249869,  0.04822759,\n",
      "        -0.0061276 , -0.01528314,  0.01249869,  0.04822759, -0.0061276 ,\n",
      "        -0.01528314,  0.01249869,  0.04822759, -0.0061276 , -0.01528314,\n",
      "         0.01249869,  0.04822759, -0.0061276 , -0.01528314,  0.01249869]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03445966,  0.1672034 , -0.01985493, -0.32845902],\n",
      "       [ 0.05225857, -0.20154944, -0.02152094,  0.31189024]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03445966,  0.1672034 , -0.01985493, -0.32845902, -0.03445966,\n",
      "         0.1672034 , -0.01985493, -0.32845902, -0.03445966,  0.1672034 ,\n",
      "        -0.01985493, -0.32845902, -0.03445966,  0.1672034 , -0.01985493,\n",
      "        -0.32845902, -0.03445966,  0.1672034 , -0.01985493, -0.32845902],\n",
      "       [ 0.05225857, -0.20154944, -0.02152094,  0.31189024,  0.05225857,\n",
      "        -0.20154944, -0.02152094,  0.31189024,  0.05225857, -0.20154944,\n",
      "        -0.02152094,  0.31189024,  0.05225857, -0.20154944, -0.02152094,\n",
      "         0.31189024,  0.05225857, -0.20154944, -0.02152094,  0.31189024]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.04460993, -0.35297012, -0.02596862,  0.5768121 ],\n",
      "       [-0.05166934, -0.15749398, -0.01443238,  0.2760628 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.04460993, -0.35297012, -0.02596862,  0.5768121 , -0.04460993,\n",
      "        -0.35297012, -0.02596862,  0.5768121 , -0.04460993, -0.35297012,\n",
      "        -0.02596862,  0.5768121 , -0.04460993, -0.35297012, -0.02596862,\n",
      "         0.5768121 , -0.04460993, -0.35297012, -0.02596862,  0.5768121 ],\n",
      "       [-0.05166934, -0.15749398, -0.01443238,  0.2760628 , -0.05166934,\n",
      "        -0.15749398, -0.01443238,  0.2760628 , -0.05166934, -0.15749398,\n",
      "        -0.01443238,  0.2760628 , -0.05166934, -0.15749398, -0.01443238,\n",
      "         0.2760628 , -0.05166934, -0.15749398, -0.01443238,  0.2760628 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.0414436 , -0.15831648, -0.03185549,  0.29434365],\n",
      "       [-0.04460993, -0.35297012, -0.02596862,  0.5768121 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.0414436 , -0.15831648, -0.03185549,  0.29434365, -0.0414436 ,\n",
      "        -0.15831648, -0.03185549,  0.29434365, -0.0414436 , -0.15831648,\n",
      "        -0.03185549,  0.29434365, -0.0414436 , -0.15831648, -0.03185549,\n",
      "         0.29434365, -0.0414436 , -0.15831648, -0.03185549,  0.29434365],\n",
      "       [-0.04460993, -0.35297012, -0.02596862,  0.5768121 , -0.04460993,\n",
      "        -0.35297012, -0.02596862,  0.5768121 , -0.04460993, -0.35297012,\n",
      "        -0.02596862,  0.5768121 , -0.04460993, -0.35297012, -0.02596862,\n",
      "         0.5768121 , -0.04460993, -0.35297012, -0.02596862,  0.5768121 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05811842,  0.4239039 ,  0.02059673, -0.51466775],\n",
      "       [-0.05166934, -0.15749398, -0.01443238,  0.2760628 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.05811842,  0.4239039 ,  0.02059673, -0.51466775, -0.05811842,\n",
      "         0.4239039 ,  0.02059673, -0.51466775, -0.05811842,  0.4239039 ,\n",
      "         0.02059673, -0.51466775, -0.05811842,  0.4239039 ,  0.02059673,\n",
      "        -0.51466775, -0.05811842,  0.4239039 ,  0.02059673, -0.51466775],\n",
      "       [-0.05166934, -0.15749398, -0.01443238,  0.2760628 , -0.05166934,\n",
      "        -0.15749398, -0.01443238,  0.2760628 , -0.05166934, -0.15749398,\n",
      "        -0.01443238,  0.2760628 , -0.05166934, -0.15749398, -0.01443238,\n",
      "         0.2760628 , -0.05166934, -0.15749398, -0.01443238,  0.2760628 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.06270144,  0.22915092,  0.02519749, -0.23003826],\n",
      "       [-0.04460993, -0.35297012, -0.02596862,  0.5768121 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.06270144,  0.22915092,  0.02519749, -0.23003826, -0.06270144,\n",
      "         0.22915092,  0.02519749, -0.23003826, -0.06270144,  0.22915092,\n",
      "         0.02519749, -0.23003826, -0.06270144,  0.22915092,  0.02519749,\n",
      "        -0.23003826, -0.06270144,  0.22915092,  0.02519749, -0.23003826],\n",
      "       [-0.04460993, -0.35297012, -0.02596862,  0.5768121 , -0.04460993,\n",
      "        -0.35297012, -0.02596862,  0.5768121 , -0.04460993, -0.35297012,\n",
      "        -0.02596862,  0.5768121 , -0.04460993, -0.35297012, -0.02596862,\n",
      "         0.5768121 , -0.04460993, -0.35297012, -0.02596862,  0.5768121 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.0414436 , -0.15831648, -0.03185549,  0.29434365],\n",
      "       [ 0.04475235,  0.18929443, -0.21189171, -0.8329531 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.0414436 , -0.15831648, -0.03185549,  0.29434365, -0.0414436 ,\n",
      "        -0.15831648, -0.03185549,  0.29434365, -0.0414436 , -0.15831648,\n",
      "        -0.03185549,  0.29434365, -0.0414436 , -0.15831648, -0.03185549,\n",
      "         0.29434365, -0.0414436 , -0.15831648, -0.03185549,  0.29434365],\n",
      "       [ 0.04475235,  0.18929443, -0.21189171, -0.8329531 ,  0.04475235,\n",
      "         0.18929443, -0.21189171, -0.8329531 ,  0.04475235,  0.18929443,\n",
      "        -0.21189171, -0.8329531 ,  0.04475235,  0.18929443, -0.21189171,\n",
      "        -0.8329531 ,  0.04475235,  0.18929443, -0.21189171, -0.8329531 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.04217022,  0.03633085, -0.03209464,  0.01195724],\n",
      "       [ 0.03712334,  0.38145024, -0.19068691, -1.0602396 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.04217022,  0.03633085, -0.03209464,  0.01195724, -0.04217022,\n",
      "         0.03633085, -0.03209464,  0.01195724, -0.04217022,  0.03633085,\n",
      "        -0.03209464,  0.01195724, -0.04217022,  0.03633085, -0.03209464,\n",
      "         0.01195724, -0.04217022,  0.03633085, -0.03209464,  0.01195724],\n",
      "       [ 0.03712334,  0.38145024, -0.19068691, -1.0602396 ,  0.03712334,\n",
      "         0.38145024, -0.19068691, -1.0602396 ,  0.03712334,  0.38145024,\n",
      "        -0.19068691, -1.0602396 ,  0.03712334,  0.38145024, -0.19068691,\n",
      "        -1.0602396 ,  0.03712334,  0.38145024, -0.19068691, -1.0602396 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.0218556 ,  0.14738338,  0.01843474, -0.25141093]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.0218556 ,  0.14738338,  0.01843474, -0.25141093,  0.0218556 ,\n",
      "         0.14738338,  0.01843474, -0.25141093,  0.0218556 ,  0.14738338,\n",
      "         0.01843474, -0.25141093,  0.0218556 ,  0.14738338,  0.01843474,\n",
      "        -0.25141093,  0.0218556 ,  0.14738338,  0.01843474, -0.25141093]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.00543172,  0.16923296, -0.07049604, -0.37401238],\n",
      "       [-0.06798723,  0.22990696,  0.02903364, -0.24675255]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.00543172,  0.16923296, -0.07049604, -0.37401238, -0.00543172,\n",
      "         0.16923296, -0.07049604, -0.37401238, -0.00543172,  0.16923296,\n",
      "        -0.07049604, -0.37401238, -0.00543172,  0.16923296, -0.07049604,\n",
      "        -0.37401238, -0.00543172,  0.16923296, -0.07049604, -0.37401238],\n",
      "       [-0.06798723,  0.22990696,  0.02903364, -0.24675255, -0.06798723,\n",
      "         0.22990696,  0.02903364, -0.24675255, -0.06798723,  0.22990696,\n",
      "         0.02903364, -0.24675255, -0.06798723,  0.22990696,  0.02903364,\n",
      "        -0.24675255, -0.06798723,  0.22990696,  0.02903364, -0.24675255]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.01270189,  0.36350814, -0.0575353 , -0.6480369 ],\n",
      "       [-0.06869128,  0.03520197,  0.02829624,  0.03687005]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.01270189,  0.36350814, -0.0575353 , -0.6480369 , -0.01270189,\n",
      "         0.36350814, -0.0575353 , -0.6480369 , -0.01270189,  0.36350814,\n",
      "        -0.0575353 , -0.6480369 , -0.01270189,  0.36350814, -0.0575353 ,\n",
      "        -0.6480369 , -0.01270189,  0.36350814, -0.0575353 , -0.6480369 ],\n",
      "       [-0.06869128,  0.03520197,  0.02829624,  0.03687005, -0.06869128,\n",
      "         0.03520197,  0.02829624,  0.03687005, -0.06869128,  0.03520197,\n",
      "         0.02829624,  0.03687005, -0.06869128,  0.03520197,  0.02829624,\n",
      "         0.03687005, -0.06869128,  0.03520197,  0.02829624,  0.03687005]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.07403018,  0.03615837,  0.0333054 ,  0.01576674],\n",
      "       [ 0.06973638,  0.34252217, -0.05619989, -0.545111  ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.07403018,  0.03615837,  0.0333054 ,  0.01576674, -0.07403018,\n",
      "         0.03615837,  0.0333054 ,  0.01576674, -0.07403018,  0.03615837,\n",
      "         0.0333054 ,  0.01576674, -0.07403018,  0.03615837,  0.0333054 ,\n",
      "         0.01576674, -0.07403018,  0.03615837,  0.0333054 ,  0.01576674],\n",
      "       [ 0.06973638,  0.34252217, -0.05619989, -0.545111  ,  0.06973638,\n",
      "         0.34252217, -0.05619989, -0.545111  ,  0.06973638,  0.34252217,\n",
      "        -0.05619989, -0.545111  ,  0.06973638,  0.34252217, -0.05619989,\n",
      "        -0.545111  ,  0.06973638,  0.34252217, -0.05619989, -0.545111  ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.07085891, -0.15856387,  0.02731115,  0.2997127 ],\n",
      "       [ 0.0589948 ,  0.53707933, -0.03969892, -0.8250483 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.07085891, -0.15856387,  0.02731115,  0.2997127 , -0.07085891,\n",
      "        -0.15856387,  0.02731115,  0.2997127 , -0.07085891, -0.15856387,\n",
      "         0.02731115,  0.2997127 , -0.07085891, -0.15856387,  0.02731115,\n",
      "         0.2997127 , -0.07085891, -0.15856387,  0.02731115,  0.2997127 ],\n",
      "       [ 0.0589948 ,  0.53707933, -0.03969892, -0.8250483 ,  0.0589948 ,\n",
      "         0.53707933, -0.03969892, -0.8250483 ,  0.0589948 ,  0.53707933,\n",
      "        -0.03969892, -0.8250483 ,  0.0589948 ,  0.53707933, -0.03969892,\n",
      "        -0.8250483 ,  0.0589948 ,  0.53707933, -0.03969892, -0.8250483 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.02801701, -0.7981536 ,  0.04990847,  1.1890854 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.02801701, -0.7981536 ,  0.04990847,  1.1890854 , -0.02801701,\n",
      "        -0.7981536 ,  0.04990847,  1.1890854 , -0.02801701, -0.7981536 ,\n",
      "         0.04990847,  1.1890854 , -0.02801701, -0.7981536 ,  0.04990847,\n",
      "         1.1890854 , -0.02801701, -0.7981536 ,  0.04990847,  1.1890854 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.00088307, -0.02257435, -0.08847543, -0.15418608],\n",
      "       [ 0.03343568,  0.18438336, -0.17633393, -0.71764904]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.00088307, -0.02257435, -0.08847543, -0.15418608,  0.00088307,\n",
      "        -0.02257435, -0.08847543, -0.15418608,  0.00088307, -0.02257435,\n",
      "        -0.08847543, -0.15418608,  0.00088307, -0.02257435, -0.08847543,\n",
      "        -0.15418608,  0.00088307, -0.02257435, -0.08847543, -0.15418608],\n",
      "       [ 0.03343568,  0.18438336, -0.17633393, -0.71764904,  0.03343568,\n",
      "         0.18438336, -0.17633393, -0.71764904,  0.03343568,  0.18438336,\n",
      "        -0.17633393, -0.71764904,  0.03343568,  0.18438336, -0.17633393,\n",
      "        -0.71764904,  0.03343568,  0.18438336, -0.17633393, -0.71764904]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.00254347,  0.17132732, -0.08006357, -0.42059335],\n",
      "       [ 0.03368935, -0.01268342, -0.16879675, -0.37685913]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.00254347,  0.17132732, -0.08006357, -0.42059335, -0.00254347,\n",
      "         0.17132732, -0.08006357, -0.42059335, -0.00254347,  0.17132732,\n",
      "        -0.08006357, -0.42059335, -0.00254347,  0.17132732, -0.08006357,\n",
      "        -0.42059335, -0.00254347,  0.17132732, -0.08006357, -0.42059335],\n",
      "       [ 0.03368935, -0.01268342, -0.16879675, -0.37685913,  0.03368935,\n",
      "        -0.01268342, -0.16879675, -0.37685913,  0.03368935, -0.01268342,\n",
      "        -0.16879675, -0.37685913,  0.03368935, -0.01268342, -0.16879675,\n",
      "        -0.37685913,  0.03368935, -0.01268342, -0.16879675, -0.37685913]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.07985245, -0.99602437,  0.12819369,  1.5514617 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.07985245, -0.99602437,  0.12819369,  1.5514617 , -0.07985245,\n",
      "        -0.99602437,  0.12819369,  1.5514617 , -0.07985245, -0.99602437,\n",
      "         0.12819369,  1.5514617 , -0.07985245, -0.99602437,  0.12819369,\n",
      "         1.5514617 , -0.07985245, -0.99602437,  0.12819369,  1.5514617 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.12362153, -1.3888879 ,  0.19684769,  2.2188118 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.12362153, -1.3888879 ,  0.19684769,  2.2188118 , -0.12362153,\n",
      "        -1.3888879 ,  0.19684769,  2.2188118 , -0.12362153, -1.3888879 ,\n",
      "         0.19684769,  2.2188118 , -0.12362153, -1.3888879 ,  0.19684769,\n",
      "         2.2188118 , -0.12362153, -1.3888879 ,  0.19684769,  2.2188118 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00],\n",
      "       [ 4.4084489e-02, -3.9593157e-01, -9.0267509e-03,  5.8822483e-01]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00,\n",
      "         3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00,\n",
      "         3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00,\n",
      "         3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00,\n",
      "         3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00],\n",
      "       [ 4.4084489e-02, -3.9593157e-01, -9.0267509e-03,  5.8822483e-01,\n",
      "         4.4084489e-02, -3.9593157e-01, -9.0267509e-03,  5.8822483e-01,\n",
      "         4.4084489e-02, -3.9593157e-01, -9.0267509e-03,  5.8822483e-01,\n",
      "         4.4084489e-02, -3.9593157e-01, -9.0267509e-03,  5.8822483e-01,\n",
      "         4.4084489e-02, -3.9593157e-01, -9.0267509e-03,  5.8822483e-01]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.02733982,  0.57721627,  0.01733707, -0.8212275 ],\n",
      "       [ 0.04810503, -0.20102708, -0.01503316,  0.30032068]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.02733982,  0.57721627,  0.01733707, -0.8212275 ,  0.02733982,\n",
      "         0.57721627,  0.01733707, -0.8212275 ,  0.02733982,  0.57721627,\n",
      "         0.01733707, -0.8212275 ,  0.02733982,  0.57721627,  0.01733707,\n",
      "        -0.8212275 ,  0.02733982,  0.57721627,  0.01733707, -0.8212275 ],\n",
      "       [ 0.04810503, -0.20102708, -0.01503316,  0.30032068,  0.04810503,\n",
      "        -0.20102708, -0.01503316,  0.30032068,  0.04810503, -0.20102708,\n",
      "        -0.01503316,  0.30032068,  0.04810503, -0.20102708, -0.01503316,\n",
      "         0.30032068,  0.04810503, -0.20102708, -0.01503316,  0.30032068]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.0210405 ,  0.3851997 ,  0.04244129, -0.47744098]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.0210405 ,  0.3851997 ,  0.04244129, -0.47744098, -0.0210405 ,\n",
      "         0.3851997 ,  0.04244129, -0.47744098, -0.0210405 ,  0.3851997 ,\n",
      "         0.04244129, -0.47744098, -0.0210405 ,  0.3851997 ,  0.04244129,\n",
      "        -0.47744098, -0.0210405 ,  0.3851997 ,  0.04244129, -0.47744098]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.02585839, -0.59415203,  0.06811275,  1.0708566 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.02585839, -0.59415203,  0.06811275,  1.0708566 , -0.02585839,\n",
      "        -0.59415203,  0.06811275,  1.0708566 , -0.02585839, -0.59415203,\n",
      "         0.06811275,  1.0708566 , -0.02585839, -0.59415203,  0.06811275,\n",
      "         1.0708566 , -0.02585839, -0.59415203,  0.06811275,  1.0708566 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-4.3980081e-02, -9.9388564e-01,  7.3690176e-02,  1.4969850e+00],\n",
      "       [ 3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-4.3980081e-02, -9.9388564e-01,  7.3690176e-02,  1.4969850e+00,\n",
      "        -4.3980081e-02, -9.9388564e-01,  7.3690176e-02,  1.4969850e+00,\n",
      "        -4.3980081e-02, -9.9388564e-01,  7.3690176e-02,  1.4969850e+00,\n",
      "        -4.3980081e-02, -9.9388564e-01,  7.3690176e-02,  1.4969850e+00,\n",
      "        -4.3980081e-02, -9.9388564e-01,  7.3690176e-02,  1.4969850e+00],\n",
      "       [ 3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00,\n",
      "         3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00,\n",
      "         3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00,\n",
      "         3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00,\n",
      "         3.8884141e-02,  7.7209675e-01,  9.1252517e-04, -1.1084074e+00]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.02801701, -0.7981536 ,  0.04990847,  1.1890854 ],\n",
      "       [ 0.02733982,  0.57721627,  0.01733707, -0.8212275 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.02801701, -0.7981536 ,  0.04990847,  1.1890854 , -0.02801701,\n",
      "        -0.7981536 ,  0.04990847,  1.1890854 , -0.02801701, -0.7981536 ,\n",
      "         0.04990847,  1.1890854 , -0.02801701, -0.7981536 ,  0.04990847,\n",
      "         1.1890854 , -0.02801701, -0.7981536 ,  0.04990847,  1.1890854 ],\n",
      "       [ 0.02733982,  0.57721627,  0.01733707, -0.8212275 ,  0.02733982,\n",
      "         0.57721627,  0.01733707, -0.8212275 ,  0.02733982,  0.57721627,\n",
      "         0.01733707, -0.8212275 ,  0.02733982,  0.57721627,  0.01733707,\n",
      "        -0.8212275 ,  0.02733982,  0.57721627,  0.01733707, -0.8212275 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.06425205,  0.2329368 ,  0.00728066, -0.3134808 ],\n",
      "       [-0.03216426,  0.16596481, -0.01836331, -0.30112663]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.06425205,  0.2329368 ,  0.00728066, -0.3134808 , -0.06425205,\n",
      "         0.2329368 ,  0.00728066, -0.3134808 , -0.06425205,  0.2329368 ,\n",
      "         0.00728066, -0.3134808 , -0.06425205,  0.2329368 ,  0.00728066,\n",
      "        -0.3134808 , -0.06425205,  0.2329368 ,  0.00728066, -0.3134808 ],\n",
      "       [-0.03216426,  0.16596481, -0.01836331, -0.30112663, -0.03216426,\n",
      "         0.16596481, -0.01836331, -0.30112663, -0.03216426,  0.16596481,\n",
      "        -0.01836331, -0.30112663, -0.03216426,  0.16596481, -0.01836331,\n",
      "        -0.30112663, -0.03216426,  0.16596481, -0.01836331, -0.30112663]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.06501058,  0.03792677,  0.0077457 , -0.02325176],\n",
      "       [-0.03157597, -0.02941486, -0.01830884, -0.00272374]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.06501058,  0.03792677,  0.0077457 , -0.02325176, -0.06501058,\n",
      "         0.03792677,  0.0077457 , -0.02325176, -0.06501058,  0.03792677,\n",
      "         0.0077457 , -0.02325176, -0.06501058,  0.03792677,  0.0077457 ,\n",
      "        -0.02325176, -0.06501058,  0.03792677,  0.0077457 , -0.02325176],\n",
      "       [-0.03157597, -0.02941486, -0.01830884, -0.00272374, -0.03157597,\n",
      "        -0.02941486, -0.01830884, -0.00272374, -0.03157597, -0.02941486,\n",
      "        -0.01830884, -0.00272374, -0.03157597, -0.02941486, -0.01830884,\n",
      "        -0.00272374, -0.03157597, -0.02941486, -0.01830884, -0.00272374]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.07403018,  0.03615837,  0.0333054 ,  0.01576674],\n",
      "       [ 0.04417241,  0.62178695, -0.11634587, -0.8731383 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.07403018,  0.03615837,  0.0333054 ,  0.01576674, -0.07403018,\n",
      "         0.03615837,  0.0333054 ,  0.01576674, -0.07403018,  0.03615837,\n",
      "         0.0333054 ,  0.01576674, -0.07403018,  0.03615837,  0.0333054 ,\n",
      "         0.01576674, -0.07403018,  0.03615837,  0.0333054 ,  0.01576674],\n",
      "       [ 0.04417241,  0.62178695, -0.11634587, -0.8731383 ,  0.04417241,\n",
      "         0.62178695, -0.11634587, -0.8731383 ,  0.04417241,  0.62178695,\n",
      "        -0.11634587, -0.8731383 ,  0.04417241,  0.62178695, -0.11634587,\n",
      "        -0.8731383 ,  0.04417241,  0.62178695, -0.11634587, -0.8731383 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.07085891, -0.15856387,  0.02731115,  0.2997127 ],\n",
      "       [ 0.02786107,  0.81556726, -0.09364498, -1.1350439 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.07085891, -0.15856387,  0.02731115,  0.2997127 , -0.07085891,\n",
      "        -0.15856387,  0.02731115,  0.2997127 , -0.07085891, -0.15856387,\n",
      "         0.02731115,  0.2997127 , -0.07085891, -0.15856387,  0.02731115,\n",
      "         0.2997127 , -0.07085891, -0.15856387,  0.02731115,  0.2997127 ],\n",
      "       [ 0.02786107,  0.81556726, -0.09364498, -1.1350439 ,  0.02786107,\n",
      "         0.81556726, -0.09364498, -1.1350439 ,  0.02786107,  0.81556726,\n",
      "        -0.09364498, -1.1350439 ,  0.02786107,  0.81556726, -0.09364498,\n",
      "        -1.1350439 ,  0.02786107,  0.81556726, -0.09364498, -1.1350439 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.04354666, -0.21158068, -0.05604693,  0.21362802]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.04354666, -0.21158068, -0.05604693,  0.21362802,  0.04354666,\n",
      "        -0.21158068, -0.05604693,  0.21362802,  0.04354666, -0.21158068,\n",
      "        -0.05604693,  0.21362802,  0.04354666, -0.21158068, -0.05604693,\n",
      "         0.21362802,  0.04354666, -0.21158068, -0.05604693,  0.21362802]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05811842,  0.4239039 ,  0.02059673, -0.51466775],\n",
      "       [-0.05898096,  0.03753366,  0.00487794, -0.01457923]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.05811842,  0.4239039 ,  0.02059673, -0.51466775, -0.05811842,\n",
      "         0.4239039 ,  0.02059673, -0.51466775, -0.05811842,  0.4239039 ,\n",
      "         0.02059673, -0.51466775, -0.05811842,  0.4239039 ,  0.02059673,\n",
      "        -0.51466775, -0.05811842,  0.4239039 ,  0.02059673, -0.51466775],\n",
      "       [-0.05898096,  0.03753366,  0.00487794, -0.01457923, -0.05898096,\n",
      "         0.03753366,  0.00487794, -0.01457923, -0.05898096,  0.03753366,\n",
      "         0.00487794, -0.01457923, -0.05898096,  0.03753366,  0.00487794,\n",
      "        -0.01457923, -0.05898096,  0.03753366,  0.00487794, -0.01457923]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.06270144,  0.22915092,  0.02519749, -0.23003826],\n",
      "       [-0.055829  , -0.15759811, -0.00068847,  0.27832076]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.06270144,  0.22915092,  0.02519749, -0.23003826, -0.06270144,\n",
      "         0.22915092,  0.02519749, -0.23003826, -0.06270144,  0.22915092,\n",
      "         0.02519749, -0.23003826, -0.06270144,  0.22915092,  0.02519749,\n",
      "        -0.23003826, -0.06270144,  0.22915092,  0.02519749, -0.23003826],\n",
      "       [-0.055829  , -0.15759811, -0.00068847,  0.27832076, -0.055829  ,\n",
      "        -0.15759811, -0.00068847,  0.27832076, -0.055829  , -0.15759811,\n",
      "        -0.00068847,  0.27832076, -0.055829  , -0.15759811, -0.00068847,\n",
      "         0.27832076, -0.055829  , -0.15759811, -0.00068847,  0.27832076]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.04863103,  0.18812734, -0.01682411, -0.26108554],\n",
      "       [ 0.16105877,  0.9371759 , -0.20757791, -1.6577305 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.04863103,  0.18812734, -0.01682411, -0.26108554,  0.04863103,\n",
      "         0.18812734, -0.01682411, -0.26108554,  0.04863103,  0.18812734,\n",
      "        -0.01682411, -0.26108554,  0.04863103,  0.18812734, -0.01682411,\n",
      "        -0.26108554,  0.04863103,  0.18812734, -0.01682411, -0.26108554],\n",
      "       [ 0.16105877,  0.9371759 , -0.20757791, -1.6577305 ,  0.16105877,\n",
      "         0.9371759 , -0.20757791, -1.6577305 ,  0.16105877,  0.9371759 ,\n",
      "        -0.20757791, -1.6577305 ,  0.16105877,  0.9371759 , -0.20757791,\n",
      "        -1.6577305 ,  0.16105877,  0.9371759 , -0.20757791, -1.6577305 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.04877588, -0.00724205, -0.01756585,  0.03708741],\n",
      "       [ 0.14625311,  0.7402827 , -0.18129347, -1.3142221 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.04877588, -0.00724205, -0.01756585,  0.03708741,  0.04877588,\n",
      "        -0.00724205, -0.01756585,  0.03708741,  0.04877588, -0.00724205,\n",
      "        -0.01756585,  0.03708741,  0.04877588, -0.00724205, -0.01756585,\n",
      "         0.03708741,  0.04877588, -0.00724205, -0.01756585,  0.03708741],\n",
      "       [ 0.14625311,  0.7402827 , -0.18129347, -1.3142221 ,  0.14625311,\n",
      "         0.7402827 , -0.18129347, -1.3142221 ,  0.14625311,  0.7402827 ,\n",
      "        -0.18129347, -1.3142221 ,  0.14625311,  0.7402827 , -0.18129347,\n",
      "        -1.3142221 ,  0.14625311,  0.7402827 , -0.18129347, -1.3142221 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.0039055 ,  0.36998335, -0.1010275 , -0.7934965 ],\n",
      "       [ 0.03343568,  0.18438336, -0.17633393, -0.71764904]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.0039055 ,  0.36998335, -0.1010275 , -0.7934965 ,  0.0039055 ,\n",
      "         0.36998335, -0.1010275 , -0.7934965 ,  0.0039055 ,  0.36998335,\n",
      "        -0.1010275 , -0.7934965 ,  0.0039055 ,  0.36998335, -0.1010275 ,\n",
      "        -0.7934965 ,  0.0039055 ,  0.36998335, -0.1010275 , -0.7934965 ],\n",
      "       [ 0.03343568,  0.18438336, -0.17633393, -0.71764904,  0.03343568,\n",
      "         0.18438336, -0.17633393, -0.71764904,  0.03343568,  0.18438336,\n",
      "        -0.17633393, -0.71764904,  0.03343568,  0.18438336, -0.17633393,\n",
      "        -0.71764904,  0.03343568,  0.18438336, -0.17633393, -0.71764904]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 4.3158460e-04,  1.7369568e-01, -9.1559149e-02, -4.7341746e-01],\n",
      "       [ 3.3689346e-02, -1.2683421e-02, -1.6879675e-01, -3.7685913e-01]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 4.3158460e-04,  1.7369568e-01, -9.1559149e-02, -4.7341746e-01,\n",
      "         4.3158460e-04,  1.7369568e-01, -9.1559149e-02, -4.7341746e-01,\n",
      "         4.3158460e-04,  1.7369568e-01, -9.1559149e-02, -4.7341746e-01,\n",
      "         4.3158460e-04,  1.7369568e-01, -9.1559149e-02, -4.7341746e-01,\n",
      "         4.3158460e-04,  1.7369568e-01, -9.1559149e-02, -4.7341746e-01],\n",
      "       [ 3.3689346e-02, -1.2683421e-02, -1.6879675e-01, -3.7685913e-01,\n",
      "         3.3689346e-02, -1.2683421e-02, -1.6879675e-01, -3.7685913e-01,\n",
      "         3.3689346e-02, -1.2683421e-02, -1.6879675e-01, -3.7685913e-01,\n",
      "         3.3689346e-02, -1.2683421e-02, -1.6879675e-01, -3.7685913e-01,\n",
      "         3.3689346e-02, -1.2683421e-02, -1.6879675e-01, -3.7685913e-01]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.10224986,  1.3503833 , -0.18235257, -2.1783924 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.10224986,  1.3503833 , -0.18235257, -2.1783924 ,  0.10224986,\n",
      "         1.3503833 , -0.18235257, -2.1783924 ,  0.10224986,  1.3503833 ,\n",
      "        -0.18235257, -2.1783924 ,  0.10224986,  1.3503833 , -0.18235257,\n",
      "        -2.1783924 ,  0.10224986,  1.3503833 , -0.18235257, -2.1783924 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.07569954, -0.80857265,  0.05998891,  1.1283528 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.07569954, -0.80857265,  0.05998891,  1.1283528 , -0.07569954,\n",
      "        -0.80857265,  0.05998891,  1.1283528 , -0.07569954, -0.80857265,\n",
      "         0.05998891,  1.1283528 , -0.07569954, -0.80857265,  0.05998891,\n",
      "         1.1283528 , -0.07569954, -0.80857265,  0.05998891,  1.1283528 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.05959331,  0.03771189,  0.00101105, -0.01851071],\n",
      "       [-0.02721264, -0.6130166 , -0.00703261,  0.824838  ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.05959331,  0.03771189,  0.00101105, -0.01851071, -0.05959331,\n",
      "         0.03771189,  0.00101105, -0.01851071, -0.05959331,  0.03771189,\n",
      "         0.00101105, -0.01851071, -0.05959331,  0.03771189,  0.00101105,\n",
      "        -0.01851071, -0.05959331,  0.03771189,  0.00101105, -0.01851071],\n",
      "       [-0.02721264, -0.6130166 , -0.00703261,  0.824838  , -0.02721264,\n",
      "        -0.6130166 , -0.00703261,  0.824838  , -0.02721264, -0.6130166 ,\n",
      "        -0.00703261,  0.824838  , -0.02721264, -0.6130166 , -0.00703261,\n",
      "         0.824838  , -0.02721264, -0.6130166 , -0.00703261,  0.824838  ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.06425205,  0.2329368 ,  0.00728066, -0.3134808 ],\n",
      "       [-0.01884966, -0.41814923, -0.01778887,  0.53781277]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.06425205,  0.2329368 ,  0.00728066, -0.3134808 , -0.06425205,\n",
      "         0.2329368 ,  0.00728066, -0.3134808 , -0.06425205,  0.2329368 ,\n",
      "         0.00728066, -0.3134808 , -0.06425205,  0.2329368 ,  0.00728066,\n",
      "        -0.3134808 , -0.06425205,  0.2329368 ,  0.00728066, -0.3134808 ],\n",
      "       [-0.01884966, -0.41814923, -0.01778887,  0.53781277, -0.01884966,\n",
      "        -0.41814923, -0.01778887,  0.53781277, -0.01884966, -0.41814923,\n",
      "        -0.01778887,  0.53781277, -0.01884966, -0.41814923, -0.01778887,\n",
      "         0.53781277, -0.01884966, -0.41814923, -0.01778887,  0.53781277]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.02292344,  0.14755008, -0.01165922, -0.24777997]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.02292344,  0.14755008, -0.01165922, -0.24777997,  0.02292344,\n",
      "         0.14755008, -0.01165922, -0.24777997,  0.02292344,  0.14755008,\n",
      "        -0.01165922, -0.24777997,  0.02292344,  0.14755008, -0.01165922,\n",
      "        -0.24777997,  0.02292344,  0.14755008, -0.01165922, -0.24777997]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.02485594,  0.19077161,  0.04643651, -0.19976065],\n",
      "       [-0.07199263, -0.15480448,  0.07671287,  0.3943404 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.02485594,  0.19077161,  0.04643651, -0.19976065, -0.02485594,\n",
      "         0.19077161,  0.04643651, -0.19976065, -0.02485594,  0.19077161,\n",
      "         0.04643651, -0.19976065, -0.02485594,  0.19077161,  0.04643651,\n",
      "        -0.19976065, -0.02485594,  0.19077161,  0.04643651, -0.19976065],\n",
      "       [-0.07199263, -0.15480448,  0.07671287,  0.3943404 , -0.07199263,\n",
      "        -0.15480448,  0.07671287,  0.3943404 , -0.07199263, -0.15480448,\n",
      "         0.07671287,  0.3943404 , -0.07199263, -0.15480448,  0.07671287,\n",
      "         0.3943404 , -0.07199263, -0.15480448,  0.07671287,  0.3943404 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.02478235, -0.00367939,  0.04486779,  0.07843559],\n",
      "       [-0.07281882,  0.04130964,  0.07513426,  0.07893045]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.02478235, -0.00367939,  0.04486779,  0.07843559, -0.02478235,\n",
      "        -0.00367939,  0.04486779,  0.07843559, -0.02478235, -0.00367939,\n",
      "         0.04486779,  0.07843559, -0.02478235, -0.00367939,  0.04486779,\n",
      "         0.07843559, -0.02478235, -0.00367939,  0.04486779,  0.07843559],\n",
      "       [-0.07281882,  0.04130964,  0.07513426,  0.07893045, -0.07281882,\n",
      "         0.04130964,  0.07513426,  0.07893045, -0.07281882,  0.04130964,\n",
      "         0.07513426,  0.07893045, -0.07281882,  0.04130964,  0.07513426,\n",
      "         0.07893045, -0.07281882,  0.04130964,  0.07513426,  0.07893045]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.02433106, -0.04727072,  0.04750233,  0.03830589]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.02433106, -0.04727072,  0.04750233,  0.03830589, -0.02433106,\n",
      "        -0.04727072,  0.04750233,  0.03830589, -0.02433106, -0.04727072,\n",
      "         0.04750233,  0.03830589, -0.02433106, -0.04727072,  0.04750233,\n",
      "         0.03830589, -0.02433106, -0.04727072,  0.04750233,  0.03830589]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.02217653, -0.15107757, -0.01676968,  0.31114623],\n",
      "       [-0.02081743, -0.19824588,  0.03768778,  0.3590005 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.02217653, -0.15107757, -0.01676968,  0.31114623, -0.02217653,\n",
      "        -0.15107757, -0.01676968,  0.31114623, -0.02217653, -0.15107757,\n",
      "        -0.01676968,  0.31114623, -0.02217653, -0.15107757, -0.01676968,\n",
      "         0.31114623, -0.02217653, -0.15107757, -0.01676968,  0.31114623],\n",
      "       [-0.02081743, -0.19824588,  0.03768778,  0.3590005 , -0.02081743,\n",
      "        -0.19824588,  0.03768778,  0.3590005 , -0.02081743, -0.19824588,\n",
      "         0.03768778,  0.3590005 , -0.02081743, -0.19824588,  0.03768778,\n",
      "         0.3590005 , -0.02081743, -0.19824588,  0.03768778,  0.3590005 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.02305239,  0.04379283, -0.01724878,  0.02395502],\n",
      "       [-0.01295716, -0.3930134 ,  0.02481243,  0.6437677 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.02305239,  0.04379283, -0.01724878,  0.02395502, -0.02305239,\n",
      "         0.04379283, -0.01724878,  0.02395502, -0.02305239,  0.04379283,\n",
      "        -0.01724878,  0.02395502, -0.02305239,  0.04379283, -0.01724878,\n",
      "         0.02395502, -0.02305239,  0.04379283, -0.01724878,  0.02395502],\n",
      "       [-0.01295716, -0.3930134 ,  0.02481243,  0.6437677 , -0.01295716,\n",
      "        -0.3930134 ,  0.02481243,  0.6437677 , -0.01295716, -0.3930134 ,\n",
      "         0.02481243,  0.6437677 , -0.01295716, -0.3930134 ,  0.02481243,\n",
      "         0.6437677 , -0.01295716, -0.3930134 ,  0.02481243,  0.6437677 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.03013729, -0.04863724,  0.05518025,  0.06850991]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.03013729, -0.04863724,  0.05518025,  0.06850991, -0.03013729,\n",
      "        -0.04863724,  0.05518025,  0.06850991, -0.03013729, -0.04863724,\n",
      "         0.05518025,  0.06850991, -0.03013729, -0.04863724,  0.05518025,\n",
      "         0.06850991, -0.03013729, -0.04863724,  0.05518025,  0.06850991]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.00817974, -0.2167256 ,  0.00089201,  0.326749  ],\n",
      "       [ 0.03455397,  0.3688434 , -0.03640854, -0.55604845]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.00817974, -0.2167256 ,  0.00089201,  0.326749  ,  0.00817974,\n",
      "        -0.2167256 ,  0.00089201,  0.326749  ,  0.00817974, -0.2167256 ,\n",
      "         0.00089201,  0.326749  ,  0.00817974, -0.2167256 ,  0.00089201,\n",
      "         0.326749  ,  0.00817974, -0.2167256 ,  0.00089201,  0.326749  ],\n",
      "       [ 0.03455397,  0.3688434 , -0.03640854, -0.55604845,  0.03455397,\n",
      "         0.3688434 , -0.03640854, -0.55604845,  0.03455397,  0.3688434 ,\n",
      "        -0.03640854, -0.55604845,  0.03455397,  0.3688434 , -0.03640854,\n",
      "        -0.55604845,  0.03455397,  0.3688434 , -0.03640854, -0.55604845]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 8.6117489e-03, -2.1600617e-02,  2.1202528e-04,  3.3999179e-02],\n",
      "       [ 3.1088205e-02,  1.7328835e-01, -3.1335562e-02, -2.5364873e-01]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 8.6117489e-03, -2.1600617e-02,  2.1202528e-04,  3.3999179e-02,\n",
      "         8.6117489e-03, -2.1600617e-02,  2.1202528e-04,  3.3999179e-02,\n",
      "         8.6117489e-03, -2.1600617e-02,  2.1202528e-04,  3.3999179e-02,\n",
      "         8.6117489e-03, -2.1600617e-02,  2.1202528e-04,  3.3999179e-02,\n",
      "         8.6117489e-03, -2.1600617e-02,  2.1202528e-04,  3.3999179e-02],\n",
      "       [ 3.1088205e-02,  1.7328835e-01, -3.1335562e-02, -2.5364873e-01,\n",
      "         3.1088205e-02,  1.7328835e-01, -3.1335562e-02, -2.5364873e-01,\n",
      "         3.1088205e-02,  1.7328835e-01, -3.1335562e-02, -2.5364873e-01,\n",
      "         3.1088205e-02,  1.7328835e-01, -3.1335562e-02, -2.5364873e-01,\n",
      "         3.1088205e-02,  1.7328835e-01, -3.1335562e-02, -2.5364873e-01]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "Episode 10/2000, average last 10 rewards 24.6\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.02217653, -0.15107757, -0.01676968,  0.31114623],\n",
      "       [ 0.00088307, -0.02257435, -0.08847543, -0.15418608]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.02217653, -0.15107757, -0.01676968,  0.31114623, -0.02217653,\n",
      "        -0.15107757, -0.01676968,  0.31114623, -0.02217653, -0.15107757,\n",
      "        -0.01676968,  0.31114623, -0.02217653, -0.15107757, -0.01676968,\n",
      "         0.31114623, -0.02217653, -0.15107757, -0.01676968,  0.31114623],\n",
      "       [ 0.00088307, -0.02257435, -0.08847543, -0.15418608,  0.00088307,\n",
      "        -0.02257435, -0.08847543, -0.15418608,  0.00088307, -0.02257435,\n",
      "        -0.08847543, -0.15418608,  0.00088307, -0.02257435, -0.08847543,\n",
      "        -0.15418608,  0.00088307, -0.02257435, -0.08847543, -0.15418608]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.02305239,  0.04379283, -0.01724878,  0.02395502],\n",
      "       [-0.00254347,  0.17132732, -0.08006357, -0.42059335]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.02305239,  0.04379283, -0.01724878,  0.02395502, -0.02305239,\n",
      "         0.04379283, -0.01724878,  0.02395502, -0.02305239,  0.04379283,\n",
      "        -0.01724878,  0.02395502, -0.02305239,  0.04379283, -0.01724878,\n",
      "         0.02395502, -0.02305239,  0.04379283, -0.01724878,  0.02395502],\n",
      "       [-0.00254347,  0.17132732, -0.08006357, -0.42059335, -0.00254347,\n",
      "         0.17132732, -0.08006357, -0.42059335, -0.00254347,  0.17132732,\n",
      "        -0.08006357, -0.42059335, -0.00254347,  0.17132732, -0.08006357,\n",
      "        -0.42059335, -0.00254347,  0.17132732, -0.08006357, -0.42059335]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03513099, -0.34583092,  0.00747327,  0.5956388 ],\n",
      "       [ 0.02263189,  0.3729263 , -0.13922094, -0.86233026]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03513099, -0.34583092,  0.00747327,  0.5956388 , -0.03513099,\n",
      "        -0.34583092,  0.00747327,  0.5956388 , -0.03513099, -0.34583092,\n",
      "         0.00747327,  0.5956388 , -0.03513099, -0.34583092,  0.00747327,\n",
      "         0.5956388 , -0.03513099, -0.34583092,  0.00747327,  0.5956388 ],\n",
      "       [ 0.02263189,  0.3729263 , -0.13922094, -0.86233026,  0.02263189,\n",
      "         0.3729263 , -0.13922094, -0.86233026,  0.02263189,  0.3729263 ,\n",
      "        -0.13922094, -0.86233026,  0.02263189,  0.3729263 , -0.13922094,\n",
      "        -0.86233026,  0.02263189,  0.3729263 , -0.13922094, -0.86233026]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.03211722, -0.15068872,  0.00142312,  0.3025074 ],\n",
      "       [ 0.01130516,  0.5663362 , -0.11689743, -1.1161757 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.03211722, -0.15068872,  0.00142312,  0.3025074 , -0.03211722,\n",
      "        -0.15068872,  0.00142312,  0.3025074 , -0.03211722, -0.15068872,\n",
      "         0.00142312,  0.3025074 , -0.03211722, -0.15068872,  0.00142312,\n",
      "         0.3025074 , -0.03211722, -0.15068872,  0.00142312,  0.3025074 ],\n",
      "       [ 0.01130516,  0.5663362 , -0.11689743, -1.1161757 ,  0.01130516,\n",
      "         0.5663362 , -0.11689743, -1.1161757 ,  0.01130516,  0.5663362 ,\n",
      "        -0.11689743, -1.1161757 ,  0.01130516,  0.5663362 , -0.11689743,\n",
      "        -1.1161757 ,  0.01130516,  0.5663362 , -0.11689743, -1.1161757 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.06616765, -0.05091693,  0.09168074,  0.08504879]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.06616765, -0.05091693,  0.09168074,  0.08504879, -0.06616765,\n",
      "        -0.05091693,  0.09168074,  0.08504879, -0.06616765, -0.05091693,\n",
      "         0.09168074,  0.08504879, -0.06616765, -0.05091693,  0.09168074,\n",
      "         0.08504879, -0.06616765, -0.05091693,  0.09168074,  0.08504879]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.07320137, -0.249961  ,  0.10435249,  0.46624437]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[-0.07320137, -0.249961  ,  0.10435249,  0.46624437, -0.07320137,\n",
      "        -0.249961  ,  0.10435249,  0.46624437, -0.07320137, -0.249961  ,\n",
      "         0.10435249,  0.46624437, -0.07320137, -0.249961  ,  0.10435249,\n",
      "         0.46624437, -0.07320137, -0.249961  ,  0.10435249,  0.46624437]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.00501361, -0.19725566,  0.01021323,  0.3369555 ],\n",
      "       [ 0.05225857, -0.20154944, -0.02152094,  0.31189024]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.00501361, -0.19725566,  0.01021323,  0.3369555 , -0.00501361,\n",
      "        -0.19725566,  0.01021323,  0.3369555 , -0.00501361, -0.19725566,\n",
      "         0.01021323,  0.3369555 , -0.00501361, -0.19725566,  0.01021323,\n",
      "         0.3369555 , -0.00501361, -0.19725566,  0.01021323,  0.3369555 ],\n",
      "       [ 0.05225857, -0.20154944, -0.02152094,  0.31189024,  0.05225857,\n",
      "        -0.20154944, -0.02152094,  0.31189024,  0.05225857, -0.20154944,\n",
      "        -0.02152094,  0.31189024,  0.05225857, -0.20154944, -0.02152094,\n",
      "         0.31189024,  0.05225857, -0.20154944, -0.02152094,  0.31189024]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.0049736 , -0.00200037,  0.00938672,  0.04132582],\n",
      "       [ 0.05239358, -0.00675047, -0.02204582,  0.02624372]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[-0.0049736 , -0.00200037,  0.00938672,  0.04132582, -0.0049736 ,\n",
      "        -0.00200037,  0.00938672,  0.04132582, -0.0049736 , -0.00200037,\n",
      "         0.00938672,  0.04132582, -0.0049736 , -0.00200037,  0.00938672,\n",
      "         0.04132582, -0.0049736 , -0.00200037,  0.00938672,  0.04132582],\n",
      "       [ 0.05239358, -0.00675047, -0.02204582,  0.02624372,  0.05239358,\n",
      "        -0.00675047, -0.02204582,  0.02624372,  0.05239358, -0.00675047,\n",
      "        -0.02204582,  0.02624372,  0.05239358, -0.00675047, -0.02204582,\n",
      "         0.02624372,  0.05239358, -0.00675047, -0.02204582,  0.02624372]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.02868467,  0.42252254, -0.00545799, -0.5371402 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.02868467,  0.42252254, -0.00545799, -0.5371402 ,  0.02868467,\n",
      "         0.42252254, -0.00545799, -0.5371402 ,  0.02868467,  0.42252254,\n",
      "        -0.00545799, -0.5371402 ,  0.02868467,  0.42252254, -0.00545799,\n",
      "        -0.5371402 ,  0.02868467,  0.42252254, -0.00545799, -0.5371402 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.02263189,  0.3729263 , -0.13922094, -0.86233026],\n",
      "       [ 0.01237108, -0.40729544, -0.00894005,  0.5893477 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.02263189,  0.3729263 , -0.13922094, -0.86233026,  0.02263189,\n",
      "         0.3729263 , -0.13922094, -0.86233026,  0.02263189,  0.3729263 ,\n",
      "        -0.13922094, -0.86233026,  0.02263189,  0.3729263 , -0.13922094,\n",
      "        -0.86233026,  0.02263189,  0.3729263 , -0.13922094, -0.86233026],\n",
      "       [ 0.01237108, -0.40729544, -0.00894005,  0.5893477 ,  0.01237108,\n",
      "        -0.40729544, -0.00894005,  0.5893477 ,  0.01237108, -0.40729544,\n",
      "        -0.00894005,  0.5893477 ,  0.01237108, -0.40729544, -0.00894005,\n",
      "         0.5893477 ,  0.01237108, -0.40729544, -0.00894005,  0.5893477 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.01130516,  0.5663362 , -0.11689743, -1.1161757 ],\n",
      "       [ 0.01661888, -0.21238998, -0.01496851,  0.30142286]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.01130516,  0.5663362 , -0.11689743, -1.1161757 ,  0.01130516,\n",
      "         0.5663362 , -0.11689743, -1.1161757 ,  0.01130516,  0.5663362 ,\n",
      "        -0.11689743, -1.1161757 ,  0.01130516,  0.5663362 , -0.11689743,\n",
      "        -1.1161757 ,  0.01130516,  0.5663362 , -0.11689743, -1.1161757 ],\n",
      "       [ 0.01661888, -0.21238998, -0.01496851,  0.30142286,  0.01661888,\n",
      "        -0.21238998, -0.01496851,  0.30142286,  0.01661888, -0.21238998,\n",
      "        -0.01496851,  0.30142286,  0.01661888, -0.21238998, -0.01496851,\n",
      "         0.30142286,  0.01661888, -0.21238998, -0.01496851,  0.30142286]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([2, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.14878428,  0.4324819 , -0.17663608, -0.7677405 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.14878428,  0.4324819 , -0.17663608, -0.7677405 ,  0.14878428,\n",
      "         0.4324819 , -0.17663608, -0.7677405 ,  0.14878428,  0.4324819 ,\n",
      "        -0.17663608, -0.7677405 ,  0.14878428,  0.4324819 , -0.17663608,\n",
      "        -0.7677405 ,  0.14878428,  0.4324819 , -0.17663608, -0.7677405 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n",
      "inputs=[<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ 0.15743393,  0.6295385 , -0.1919909 , -1.1103876 ]],\n",
      "      dtype=float32)>]\n",
      "self.lmbd=<tf.Variable 'lambdas:0' shape=(20,) dtype=float32, numpy=\n",
      "array([1.000996  , 1.0009992 , 1.0009987 , 0.9990013 , 0.9990435 ,\n",
      "       1.0009998 , 1.0009992 , 0.99900055, 0.99900806, 1.0009947 ,\n",
      "       1.0009991 , 0.99900126, 1.000999  , 1.0009998 , 0.999005  ,\n",
      "       1.0009997 , 1.0000217 , 1.0000224 , 0.999001  , 0.99900055],\n",
      "      dtype=float32)>\n",
      "tiled_up_inputs=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 0.15743393,  0.6295385 , -0.1919909 , -1.1103876 ,  0.15743393,\n",
      "         0.6295385 , -0.1919909 , -1.1103876 ,  0.15743393,  0.6295385 ,\n",
      "        -0.1919909 , -1.1103876 ,  0.15743393,  0.6295385 , -0.1919909 ,\n",
      "        -1.1103876 ,  0.15743393,  0.6295385 , -0.1919909 , -1.1103876 ]],\n",
      "      dtype=float32)>\n",
      "scaled_inputs.shape=TensorShape([1, 20])\n"
     ]
    }
   ],
   "source": [
    "### Main training loop\n",
    "\n",
    "# with tf.device(\"/gpu:0\"):\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "episode_reward_history = []\n",
    "step_count = 0\n",
    "for episode in range(n_episodes):\n",
    "    episode_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        # Interact with environment.\n",
    "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
    "        \n",
    "        # Preserve interaction in the replay memory.\n",
    "        replay_memory.append(interaction)\n",
    "        \n",
    "        state = interaction['next_state']\n",
    "        episode_reward += interaction['reward']\n",
    "        step_count += 1\n",
    "        \n",
    "        if step_count % steps_per_update == 0:\n",
    "            training_batch = np.random.choice(replay_memory, size=batch_size) # Randomly select interactions from replay memory and train on them.\n",
    "            \n",
    "            Q_learning_update(\n",
    "                states=np.asarray([x['state'] for x in training_batch]).squeeze(),\n",
    "                actions=np.asarray([x['action'] for x in training_batch]).squeeze(),\n",
    "                rewards=np.asarray([x['reward'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "                next_states=np.asarray([x['next_state'] for x in training_batch]).squeeze(),\n",
    "                done=np.asarray([x['done'] for x in training_batch], dtype=np.float32).squeeze(),\n",
    "                model=model,\n",
    "                model_target=model_target,\n",
    "                gamma=gamma,\n",
    "                n_actions=n_actions,\n",
    "                optimizer_w_tups=optimizer_w_tups,\n",
    "            )\n",
    "        \n",
    "        if step_count % steps_per_target_update == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "        \n",
    "        if interaction['done']:\n",
    "            break\n",
    "        \n",
    "    # Decay epsilon.\n",
    "    epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    \n",
    "    if (episode+1) % 10 == 0:\n",
    "        avg_rewards = np.mean(episode_reward_history[-10:])\n",
    "        print(\"Episode {}/{}, average last 10 rewards {}\".format(episode+1, n_episodes, avg_rewards))\n",
    "        if avg_rewards >= 500.0:\n",
    "            break\n",
    "    \n",
    "    if episode > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01039738,  0.01608276,  0.00298963,  0.03868127],\n",
       "       [-0.01039721,  0.01605068,  0.00298371,  0.03860574],\n",
       "       [-0.0104175 ,  0.01601867,  0.00298969,  0.03860421],\n",
       "       [-0.01039686,  0.01605086,  0.00298385,  0.03868117],\n",
       "       [-0.01039675,  0.01601875,  0.00298394,  0.03860412]])"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2, 4)\n",
    "inputs = np.array([[-0.01039682,  0.01605072,  0.0029838 ,  0.03860417], [ 0.01260861,  0.21097969, -0.0169554 , -0.2498462 ]])\n",
    "\n",
    "# (5, 4, 1)\n",
    "weights_enc = np.array([[[1.000054  ],\n",
    "        [1.0019959 ],\n",
    "        [1.0019536 ],\n",
    "        [1.0019972 ]],\n",
    "\n",
    "       [[1.0000376 ],\n",
    "        [0.9999973 ],\n",
    "        [0.99996847],\n",
    "        [1.0000407 ]],\n",
    "\n",
    "       [[1.001989  ],\n",
    "        [0.9980035 ],\n",
    "        [1.0019749 ],\n",
    "        [1.000001  ]],\n",
    "\n",
    "       [[1.0000042 ],\n",
    "        [1.0000086 ],\n",
    "        [1.0000173 ],\n",
    "        [1.0019947 ]],\n",
    "\n",
    "       [[0.99999315],\n",
    "        [0.99800795],\n",
    "        [1.0000461 ],\n",
    "        [0.9999987 ]]], dtype='float32')\n",
    "weights_enc = weights_enc.squeeze()\n",
    "\n",
    "\n",
    "(inputs[0] * weights_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01260929,  0.21140079, -0.01698852, -0.2503452 ],\n",
       "       [ 0.01260908,  0.21097912, -0.01695487, -0.24985636],\n",
       "       [ 0.01263369,  0.21055847, -0.01698889, -0.24984644],\n",
       "       [ 0.01260866,  0.2109815 , -0.01695569, -0.25034458],\n",
       "       [ 0.01260852,  0.21055941, -0.01695618, -0.24984587]])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs[1] * weights_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum(\"lqf,bq->blqf\", weights_enc[..., None], inputs).squeeze()[0] == (inputs[0] * weights_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====\n",
    "# inputs=<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
    "# array([[-0.01039682,  0.01605072,  0.0029838 ,  0.03860417],\n",
    "#        [ 0.01260861,  0.21097969, -0.0169554 , -0.2498462 ]],\n",
    "#       dtype=float32)>\n",
    "# before: weights_enc=<tf.Tensor: shape=(5, 4, 1), dtype=float32, numpy=\n",
    "# array([[[1.000054  ],\n",
    "#         [1.0019959 ],\n",
    "#         [1.0019536 ],\n",
    "#         [1.0019972 ]],\n",
    "\n",
    "#        [[1.0000376 ],\n",
    "#         [0.9999973 ],\n",
    "#         [0.99996847],\n",
    "#         [1.0000407 ]],\n",
    "\n",
    "#        [[1.001989  ],\n",
    "#         [0.9980035 ],\n",
    "#         [1.0019749 ],\n",
    "#         [1.000001  ]],\n",
    "\n",
    "#        [[1.0000042 ],\n",
    "#         [1.0000086 ],\n",
    "#         [1.0000173 ],\n",
    "#         [1.0019947 ]],\n",
    "\n",
    "#        [[0.99999315],\n",
    "#         [0.99800795],\n",
    "#         [1.0000461 ],\n",
    "#         [0.9999987 ]]], dtype=float32)>\n",
    "# after: weights_enc=<tf.Tensor: shape=(2, 5, 4, 1), dtype=float32, numpy=\n",
    "# array([[[[-0.01039738],\n",
    "#          [ 0.01608276],\n",
    "#          [ 0.00298963],\n",
    "#          [ 0.03868127]],\n",
    "\n",
    "#         [[-0.01039721],\n",
    "#          [ 0.01605068],\n",
    "#          [ 0.00298371],\n",
    "#          [ 0.03860573]],\n",
    "\n",
    "#         [[-0.0104175 ],\n",
    "#          [ 0.01601868],\n",
    "#          [ 0.0029897 ],\n",
    "#          [ 0.0386042 ]],\n",
    "\n",
    "#         [[-0.01039687],\n",
    "#          [ 0.01605086],\n",
    "#          [ 0.00298386],\n",
    "#          [ 0.03868117]],\n",
    "\n",
    "#         [[-0.01039675],\n",
    "#          [ 0.01601875],\n",
    "#          [ 0.00298394],\n",
    "#          [ 0.03860411]]],\n",
    "\n",
    "\n",
    "#        [[[ 0.0126093 ],\n",
    "#          [ 0.21140078],\n",
    "#          [-0.01698852],\n",
    "#          [-0.2503452 ]],\n",
    "\n",
    "#         [[ 0.01260909],\n",
    "#          [ 0.21097912],\n",
    "#          [-0.01695487],\n",
    "#          [-0.24985637]],\n",
    "\n",
    "#         [[ 0.01263369],\n",
    "#          [ 0.21055846],\n",
    "#          [-0.01698889],\n",
    "#          [-0.24984644]],\n",
    "\n",
    "#         [[ 0.01260867],\n",
    "#          [ 0.2109815 ],\n",
    "#          [-0.01695569],\n",
    "#          [-0.25034457]],\n",
    "\n",
    "#         [[ 0.01260853],\n",
    "#          [ 0.2105594 ],\n",
    "#          [-0.01695618],\n",
    "#          [-0.24984588]]]], dtype=float32)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'weights_var:0' shape=(6, 4, 3) dtype=float32, numpy=\n",
       " array([[[0.9805262 , 2.5970278 , 2.1508203 ],\n",
       "         [0.02007758, 2.472989  , 1.2282661 ],\n",
       "         [0.9203254 , 3.1179914 , 3.0089736 ],\n",
       "         [1.7464586 , 0.51630473, 0.42340934]],\n",
       " \n",
       "        [[0.41462833, 1.6811546 , 1.7945408 ],\n",
       "         [1.6022853 , 1.5148928 , 0.48847726],\n",
       "         [1.1644031 , 1.5469987 , 1.7803334 ],\n",
       "         [0.6535356 , 0.5715202 , 3.1369405 ]],\n",
       " \n",
       "        [[1.1618369 , 1.1905388 , 2.425289  ],\n",
       "         [2.1439352 , 1.255527  , 2.4622328 ],\n",
       "         [2.133518  , 2.3068001 , 1.7332988 ],\n",
       "         [0.3449569 , 2.0372598 , 3.1062958 ]],\n",
       " \n",
       "        [[2.5762544 , 2.212892  , 3.0082142 ],\n",
       "         [0.07117579, 2.9414892 , 2.0472023 ],\n",
       "         [0.99574083, 0.0045045 , 2.8930953 ],\n",
       "         [1.19997   , 2.425756  , 2.8760033 ]],\n",
       " \n",
       "        [[1.8057718 , 2.4933574 , 1.3486687 ],\n",
       "         [0.60163903, 2.3402739 , 1.3109922 ],\n",
       "         [2.5687842 , 0.6329979 , 2.0295756 ],\n",
       "         [0.5188675 , 1.4077289 , 1.9021418 ]],\n",
       " \n",
       "        [[2.5629013 , 1.2963688 , 0.82717633],\n",
       "         [0.5986544 , 2.39862   , 2.6201646 ],\n",
       "         [1.5524462 , 2.1561182 , 0.9432915 ],\n",
       "         [0.8407294 , 2.6207483 , 1.9788439 ]]], dtype=float32)>,\n",
       " <tf.Variable 'weights_enc:0' shape=(5, 4, 1) dtype=float32, numpy=\n",
       " array([[[1.0009956 ],\n",
       "         [1.0009959 ],\n",
       "         [0.9990008 ],\n",
       "         [0.9990003 ]],\n",
       " \n",
       "        [[0.9990063 ],\n",
       "         [1.0009991 ],\n",
       "         [0.9990005 ],\n",
       "         [1.0009586 ]],\n",
       " \n",
       "        [[1.0009702 ],\n",
       "         [0.9990002 ],\n",
       "         [0.99900144],\n",
       "         [1.0009998 ]],\n",
       " \n",
       "        [[1.0009961 ],\n",
       "         [1.0009995 ],\n",
       "         [0.99900097],\n",
       "         [0.99900013]],\n",
       " \n",
       "        [[1.0009578 ],\n",
       "         [0.99900025],\n",
       "         [1.0009985 ],\n",
       "         [0.99900013]]], dtype=float32)>,\n",
       " <tf.Variable 'obs-weights:0' shape=(1, 2) dtype=float32, numpy=array([[1.0999972, 1.0999984]], dtype=float32)>]"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weights_var:0' shape=(6, 4, 3) dtype=float32, numpy=\n",
      "array([[[0.9805262 , 2.5970278 , 2.1508203 ],\n",
      "        [0.02007758, 2.472989  , 1.2282661 ],\n",
      "        [0.9203254 , 3.1179914 , 3.0089736 ],\n",
      "        [1.7464586 , 0.51630473, 0.42340934]],\n",
      "\n",
      "       [[0.41462833, 1.6811546 , 1.7945408 ],\n",
      "        [1.6022853 , 1.5148928 , 0.48847726],\n",
      "        [1.1644031 , 1.5469987 , 1.7803334 ],\n",
      "        [0.6535356 , 0.5715202 , 3.1369405 ]],\n",
      "\n",
      "       [[1.1618369 , 1.1905388 , 2.425289  ],\n",
      "        [2.1439352 , 1.255527  , 2.4622328 ],\n",
      "        [2.133518  , 2.3068001 , 1.7332988 ],\n",
      "        [0.3449569 , 2.0372598 , 3.1062958 ]],\n",
      "\n",
      "       [[2.5762544 , 2.212892  , 3.0082142 ],\n",
      "        [0.07117579, 2.9414892 , 2.0472023 ],\n",
      "        [0.99574083, 0.0045045 , 2.8930953 ],\n",
      "        [1.19997   , 2.425756  , 2.8760033 ]],\n",
      "\n",
      "       [[1.8057718 , 2.4933574 , 1.3486687 ],\n",
      "        [0.60163903, 2.3402739 , 1.3109922 ],\n",
      "        [2.5687842 , 0.6329979 , 2.0295756 ],\n",
      "        [0.5188675 , 1.4077289 , 1.9021418 ]],\n",
      "\n",
      "       [[2.5629013 , 1.2963688 , 0.82717633],\n",
      "        [0.5986544 , 2.39862   , 2.6201646 ],\n",
      "        [1.5524462 , 2.1561182 , 0.9432915 ],\n",
      "        [0.8407294 , 2.6207483 , 1.9788439 ]]], dtype=float32)>\n",
      "<tf.Variable 'weights_enc:0' shape=(5, 4, 1) dtype=float32, numpy=\n",
      "array([[[1.0009956 ],\n",
      "        [1.0009959 ],\n",
      "        [0.9990008 ],\n",
      "        [0.9990003 ]],\n",
      "\n",
      "       [[0.9990063 ],\n",
      "        [1.0009991 ],\n",
      "        [0.9990005 ],\n",
      "        [1.0009586 ]],\n",
      "\n",
      "       [[1.0009702 ],\n",
      "        [0.9990002 ],\n",
      "        [0.99900144],\n",
      "        [1.0009998 ]],\n",
      "\n",
      "       [[1.0009961 ],\n",
      "        [1.0009995 ],\n",
      "        [0.99900097],\n",
      "        [0.99900013]],\n",
      "\n",
      "       [[1.0009578 ],\n",
      "        [0.99900025],\n",
      "        [1.0009985 ],\n",
      "        [0.99900013]]], dtype=float32)>\n",
      "<tf.Variable 'obs-weights:0' shape=(1, 2) dtype=float32, numpy=array([[1.0999972, 1.0999984]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "updated_trainable_variables = model.trainable_variables # Preserve\n",
    "for v in updated_trainable_variables: print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x156529190>]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_reward_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/qml/lib/python3.11/site-packages/pennylane/math/utils.py:227: UserWarning: Contains tensors of types {'autograd', 'tensorflow'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'state': array([[0.00602067, 0.01091631, 0.00542063, 0.0035057 ]], dtype=float32),\n",
       "  'action': 0,\n",
       "  'reward': 1.0,\n",
       "  'next_state': array([[ 0.00623899, -0.18428296,  0.00549074,  0.29789397]],\n",
       "        dtype=float32),\n",
       "  'done': False},\n",
       " {'state': array([[0.00602067, 0.01091631, 0.00542063, 0.0035057 ]], dtype=float32),\n",
       "  'action': 0,\n",
       "  'reward': 1.0,\n",
       "  'next_state': array([[ 0.00255333, -0.37948275,  0.01144862,  0.5923035 ]],\n",
       "        dtype=float32),\n",
       "  'done': False},\n",
       " {'state': array([[0.00602067, 0.01091631, 0.00542063, 0.0035057 ]], dtype=float32),\n",
       "  'action': 0,\n",
       "  'reward': 1.0,\n",
       "  'next_state': array([[-0.00503632, -0.57476306,  0.02329469,  0.8885706 ]],\n",
       "        dtype=float32),\n",
       "  'done': False},\n",
       " {'state': array([[0.00602067, 0.01091631, 0.00542063, 0.0035057 ]], dtype=float32),\n",
       "  'action': 0,\n",
       "  'reward': 1.0,\n",
       "  'next_state': array([[-0.01653158, -0.7701933 ,  0.0410661 ,  1.1884844 ]],\n",
       "        dtype=float32),\n",
       "  'done': False},\n",
       " {'state': array([[0.00602067, 0.01091631, 0.00542063, 0.0035057 ]], dtype=float32),\n",
       "  'action': 0,\n",
       "  'reward': 1.0,\n",
       "  'next_state': array([[-0.03193545, -0.9658228 ,  0.06483579,  1.4937514 ]],\n",
       "        dtype=float32),\n",
       "  'done': False},\n",
       " {'state': array([[0.00602067, 0.01091631, 0.00542063, 0.0035057 ]], dtype=float32),\n",
       "  'action': 0,\n",
       "  'reward': 1.0,\n",
       "  'next_state': array([[-0.0512519 , -1.1616708 ,  0.09471082,  1.8059545 ]],\n",
       "        dtype=float32),\n",
       "  'done': False},\n",
       " {'state': array([[0.00602067, 0.01091631, 0.00542063, 0.0035057 ]], dtype=float32),\n",
       "  'action': 0,\n",
       "  'reward': 1.0,\n",
       "  'next_state': array([[-0.07448532, -1.3577137 ,  0.1308299 ,  2.1265044 ]],\n",
       "        dtype=float32),\n",
       "  'done': False},\n",
       " {'state': array([[0.00602067, 0.01091631, 0.00542063, 0.0035057 ]], dtype=float32),\n",
       "  'action': 0,\n",
       "  'reward': 1.0,\n",
       "  'next_state': array([[-0.1016396 , -1.5538708 ,  0.17335999,  2.4565797 ]],\n",
       "        dtype=float32),\n",
       "  'done': False},\n",
       " {'state': array([[0.00602067, 0.01091631, 0.00542063, 0.0035057 ]], dtype=float32),\n",
       "  'action': 0,\n",
       "  'reward': 1.0,\n",
       "  'next_state': array([[-0.13271701, -1.749987  ,  0.22249159,  2.7970574 ]],\n",
       "        dtype=float32),\n",
       "  'done': True}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "epsilon = 0\n",
    "\n",
    "state, _ = env.reset()\n",
    "trajectory = []\n",
    "for t in range(100):\n",
    "    traj = interact_env(state, model, epsilon, n_actions, env)\n",
    "    trajectory.append(traj)\n",
    "    if traj['done']:\n",
    "        break\n",
    "\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_episode_as_gif_frames(env_name, actor, max_steps):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "    screen = env.render()\n",
    "    images = [Image.fromarray(screen)]\n",
    "    \n",
    "    for i in range(1, max_steps+1):\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        action_probs = actor(state)\n",
    "        action = np.argmax(np.squeeze(action_probs))\n",
    "        \n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        state = tf.constant(state, dtype=tf.float32)\n",
    "        \n",
    "        screen = env.render()\n",
    "        images.append(Image.fromarray(screen))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return images\n",
    "\n",
    "def save_frames_as_gif(frames: list[Image.Image], path: Path, **kwargs):\n",
    "    frames[1].save(\n",
    "        path,\n",
    "        save_all=True,\n",
    "        append_images=frames[2:],\n",
    "        optimize=False,\n",
    "        duration=40,\n",
    "        loop=0,\n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "import IPython.display\n",
    "def display_gif(path: Path) -> IPython.display.DisplayHandle:\n",
    "    return display(\n",
    "        IPython.display.Image(\n",
    "            data=open(path,'rb').read(), format='png'),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/qml/lib/python3.11/site-packages/pennylane/math/utils.py:227: UserWarning: Contains tensors of types {'autograd', 'tensorflow'}; dispatch will prioritize TensorFlow, PyTorch, and  Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 10 frames to GIF ./images/gym_CartPole.gif\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gif_path = './images/gym_CartPole.gif'\n",
    "frames = render_episode_as_gif_frames('CartPole-v1', model, 100)\n",
    "print(f\"rendering {len(frames)} frames to GIF {gif_path}\")\n",
    "save_frames_as_gif(frames, gif_path)\n",
    "display_gif(gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
